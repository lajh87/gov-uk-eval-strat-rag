# Standards

Food Agency

food.gov.uk

Evaluation Action Plan

June 2022

Author: Anna Cordes

Analytics Unit | Science, Evidence and Research Directorate

# Evaluation Action Plan

Foreword

Executive Summary

# 1. Introduction

1.1 Monitoring and evaluation in the policy cycle

1.2 This document: the FSA’s Evaluation Action Plan

1.3 Scope

# 2. Our Vision for the Evaluation Action Plan

2.1 Anticipated outcomes of the Evaluation Action Plan

# 3. Supporting the Evaluation Action Plan

3.1 Our Evaluation Approach

3.2 Quality assurance

3.3 Building evaluation skills and capability across the FSA

3.4 Further raising the profile of evaluation

Next Steps

# Annex A: Guidance for when and how to evaluate

Evaluation criteria

Evaluation scale

Evaluation type

# Annex B: Table of recommendations

# Annex C: Current and Forthcoming Evaluation Activity

# Annex D: How the FSA uses Evidence

# Foreword

This is the Food Standards Agency’s (FSA) first Evaluation Action Plan, which sets out the broad principles by which we will monitor and evaluate our work.

The UK food system is rapidly changing; technology is transforming how food is produced, bought and sold, whilst economic, social and environmental pressures make accessing a healthy and sustainable diet challenging for many.

The FSA’s ability to deliver on its mission – food you can trust - relies on the generation and use of high quality evidence, including evaluation evidence. Evaluation highlights good practice, improves our performance and identifies approaches that work, why and for whom. Conducting thorough evaluations ensures the effective use of public funds and helps create regulation that is proportionate and fit for purpose.

This Action Plan builds on the good practices that are already in place within the FSA and aligns with our existing commitment to be an evidence-led organisation. It also makes a number of recommendations that will strengthen the organisation’s ability to generate and act on evaluation evidence, helping us navigate the complex challenges of the food system now and into the future.

Robin May

Chief Scientific Adviser

# Executive Summary

The Food Standards Agency (FSA) is launching a new Evaluation Action Plan, which will guide the operation and development of its monitoring and evaluation activities.

Our vision and overarching goal for this Action Plan is that it facilitates proportionate, good quality evaluation evidence generation and application across the FSA. This is rooted in our belief, as articulated in the FSA Strategy, that effective policy and regulation needs to be evidence led. Monitoring and evaluation is one source of the evidence needed to guide our work. It allows for systematic learning from past and current activities, facilitates understanding of what works, why and for whom, and supports the effective use of public funds and proportionate regulation.

This Action Plan builds on existing monitoring and evaluation activities conducted by the FSA and is intended to complement benefits measurement activities already undertaken within the organisation. It sets out a framework to support FSA colleagues in identifying and prioritising areas for evaluation, guides evaluation approaches and outlines actions that will support high quality evaluations across the FSA.

The success of this Action Plan relies on further strengthening evaluation capability and skills within the FSA and the fermentation of an evaluation mindset among staff. This is already facilitated by the FSA’s existing activities, including our established performance reporting systems, commitments to measuring progress and track record of conducting evaluations. These activities will be further supported by the implementation of the actions recommended in this plan.

This is a living document and will be periodically reviewed to ensure it is fit for purpose and having the intended impact. We will work to deliver the recommended actions listed in this document to ensure the FSA becomes known across government for the high quality of its evaluation activities.

# Introduction

The FSA is the independent government department responsible for protecting public health and consumers’ wider interests in relation to food in England, Wales and Northern Ireland. Our core mission is to ensure that people can trust food. We undertake a range of activities to ensure food is safe, what it says it is, and is healthier and more sustainable.

We invest significant resources in delivering against our mission. While providing value for money has always been an organizational priority, it is critical that our investments and interventions deliver as intended, are appropriately targeted, and provide the greatest possible economic and social return. This is even more important in times of constrained resources, and with many of our partner organizations and stakeholders (for example, Local Authorities, Food Business Operators) working to recover from the impact of the Covid Pandemic.

Generating good quality monitoring and evaluation evidence is vital to the FSA’s ability to do this. This evidence ensures we are continually learning and accountable to our funders and those we serve. It also allows us to design and deliver policies, programs, communications, and regulations effectively. We can also use information and data gathered through monitoring and evaluation activities to avoid unnecessary burdens being placed on businesses while ensuring consumers can have confidence in the food system.

# Monitoring and evaluation in the policy cycle

Monitoring and evaluation form a key part of the policy development and delivery lifecycle at the FSA and reflects best practice policy development guidance from both HM Treasury and the National Audit Office (NAO). Examples of current and forthcoming evaluations and how we have used evaluation evidence to inform actions are included in Annexes C and D respectively.

HM Treasury’s Green Book states that each level of policy development, be that setting policies, portfolios, programs, or projects, follows broadly the same policy development and review pattern: the ROAMEF cycle (see Figure 1).

# Figure 1 The ROAMEF Cycle

|Rationale|Feedback|Objectives|
||||
|Evaluation|Monitoring|Appraisal|

ROAMEF – standing for Rationale, Objectives, Appraisal, Monitoring, Evaluation and Feedback – places the generation and application of evidence throughout the policy development lifecycle. Evidence can be generated through evaluation and monitoring as well as other activities, such as primary research. It should support options appraisal and inform changes to (or continuations of) organisational activities. Evidence demonstrates whether activities are having the intended impact, shows progress on delivery and identifies barriers and facilitators to implementation and impact.

The NAO similarly highlights that evidence generated through evaluation and monitoring, and through other organisational activities, are critical for effective regulation. The collection and analysis of data and information and the monitoring of compliance with regulations can help identify problems that need intervention and can enable the prioritising and targeting of activities and resources. Likewise, developing theories of change and evaluating the impact and outcomes of regulation on an ongoing basis help evidence value for money, provide insight into unintended outcomes and refine regulatory interventions to improve outcomes.

Appraisal, monitoring and evaluation are core ways evidence is generated during policy making. While deployed at different points, they typically use similar approaches (for example, quantitative and qualitative research) and tools (for example, Logic.

Models/Theories of Change). The Magenta Book provides comprehensive advice and guidance on how to design evaluations and appropriate approaches.

For brevity, in this document the term ‘evaluation’ will be used as shorthand to refer to the FSA’s use of appraisal, monitoring and evaluations. The HM Treasury Green Book provides the following definitions of these terms:

- Appraisal: the process of assessing the costs, benefits and risks of alternative ways to meet government objectives. It helps decision makers to understand the potential effects, trade-offs and overall impact of options by providing an objective evidence base for decision making.
- Monitoring: the collection of data, both during and after implementation. This can form a baseline against which any changes to implementation can be measured.
- Evaluation: the systematic assessment of an intervention’s design, implementation and outcomes. It tests how far an intervention is working or has worked against expected, if the costs and benefits were as anticipated, whether there were significant unexpected consequences, how it was implemented and, if changes were made, why.

1.2 This document: the FSA’s Evaluation Action Plan

This Action Plan is an opportunity to strengthen the work the FSA already does to evaluate its work. Chapter 2 describes our vision for evaluation. Chapter 3 focuses on the practical steps we will take to deliver this vision.

This is a living document. We plan to periodically review the Evaluation Action Plan, and the FSA’s implementation of it, to ensure the Plan is having the intended impact. We anticipate the first review of the Evaluation Action Plan will occur in 2025, in line with the development of a Benefits Management Action Plan (BMAP). The BMAP will focus on how the FSA can systematically measure and realise the benefit of its work.

1.3 Scope

The FSA’s remit is broad with the FSA carrying out different roles in the food system to help ensure food is safe, what it says it is, and healthier and more sustainable. The FSA shares responsibility for food policy in the United Kingdom: Food Standards Scotland is

7

responsible for food policy and implementation in Scotland; the FSA has different responsibilities within England, Wales and Northern Ireland, and partners with different bodies to deliver its work accordingly; and, responsibility for food policy is itself split across different government departments.

This Action Plan applies to work directly undertaken by the FSA to deliver its Strategy. As outlined in Annex C, this includes FSA programmes (for example, Achieving Business Compliance (ABC) and the Operational Transformation Programme (OTP)), activities undertaken as part of our business as usual (for example, measuring the effectiveness of our official control interventions and the difference our support and guidance make to our stakeholders) and activities commissioned by the FSA to facilitate delivery (for example, non-routine surveillance activities, special projects). Policies, projects and initiatives outside of the direct control of the Agency (for example, decisions made by other government departments) or where we do not contribute data are outside this Action Plan’s scope.

The cross-government, multi-agency nature of much of the FSA’s work means the FSA will need to collaborate with other government departments and delivery partners to conduct effective evaluations. It will also need to reflect variations in activities or implementation approaches in different nations in its evaluations and provide proportionate support to other government departments when requested to do so.

# 8

# Our Vision for the Evaluation Action Plan

The FSA’s vision for this Action Plan is that it creates a climate in which proportionate, good quality evaluation evidence is consistently generated and used. This vision is grounded in our belief that effective policy and regulation needs to be evidence led, something outlined in our guiding principles.

Strengthening our existing evaluation capability and skills and ensuring consistent practice across the FSA is key to achieving this. This section sets out our vision for what fortifying evaluation capability and skills will deliver. The next chapter provides detail on how this can be delivered.

# Anticipated outcomes of the Evaluation Action Plan

The FSA’s Evaluation Action Plan is centred on strengthening evaluation capability and skills across the FSA. This will deliver the following outcomes:

Increased understanding and awareness of pe value and need for evaluation evidence.
Increased capacity to design and deliver robust evaluations internally.
Improved ability to commission evaluations wip mepodologies pat are fit for purpose and in a timely manner.
Improved ability to apply evaluation and monitoring evidence in policy development and operational delivery.
Strengpening of an evaluation mindset among FSA.

This last outcome – a stronger evaluation mindset – is perhaps the most important outcome of this Action Plan, as it will drive the realization of the other benefits. Having an evaluation mindset means being focused on the organization’s ultimate mission, rather than the success (or failure) of individual activities. This mindset means that evaluations, the findings of which can sometimes be uncomfortable, are seen as valuable learning tools which support mission delivery and organizational outcomes, even when they demonstrate that activities are not working optimally.

# Supporting the Evaluation Action Plan

Delivering on the FSA’s vision for this Action Plan depends on colleagues knowing when and how to evaluate. This in turn relies on there being appropriate governance structures and colleagues having the skills necessary to design and commission evaluations.

This section sets out questions and considerations that should guide when, how and at what scale to evaluate, the quality assurance measures that are in place to ensure evaluations are robust and fit for purpose, how FSA staff’s evaluation capability and skills can be supported and how the profile of evaluation can be raised across the FSA.

# Our Evaluation Approach

Evaluation is important. The FSA already conducts a range of evaluation activities pre- and post-policy implementation, in line with guidance provided in the Green Book; all of the FSA’s discretionary spending in Northern Ireland is required to undergo proportionate post-project evaluation. Examples of approaches take across the FSA include our benefits measurement approach within our business case process and our use of establishing project impact (EPI) forms pre and post award to capture the intended and realised impact of our work.

It is critical, however, that evaluation activities are proportionate and meet the needs of decision makers and those scrutinising our activities. While good quality evaluation evidence supports the delivery of our mission, it is not an end in itself: evaluation should facilitate the FSA’s work, integrate into our existing processes and be timely to support effective organisational delivery.

When deciding what form of evaluation to conduct, FSA colleagues should consider the potential value of the evidence generated from evaluation (i.e., the filling of knowledge gaps), alongside reporting requirements (i.e., the need to demonstrate accountability and transparency, scale of investment / use of public funds) and the practicalities of delivering an evaluation (i.e., whether it is feasible to evaluate in a timely manner). This should reflect how best to evidence anticipated benefits described in business cases. Areas that make a greater contribution to the evidence base, where it is a requirement, where it is

# feasible and where activities sit within the FSA’s priority programmes should be prioritised.

These considerations also have a bearing on evaluation scale and approach. Activities can be evaluated in different ways with the most appropriate form of evaluation depending on the questions being addressed, the profile and cost of the activities being evaluated, and the risk/uncertainty surrounding what can be learnt through evaluation.

There is no one-size-fits-all approach for choosing an evaluation approach and robust evaluation methods can take many forms. While commissioning third-parties to conduct evaluations may be desirable, it is not always necessary, feasible or proportionate; internally designed and delivered evaluation activities can provide the required insight in a timely way and more efficiently but can sometimes be perceived as less independent. Likewise, although experimental methods allow the impact of activities to be clearly demonstrated, and are considered by default at the FSA, they cannot always be operationalised. That said, gathering quantitative insights through our evaluations is generally desirable.

Colleagues leading FSA activities should engage broadly and consult with Science, Evidence and Research Division (SERD) colleagues early in the policy formation and business case development process to decide what type of evaluation is required, whether evaluation activities can be conducted in-house and whether independently conducted evaluation is appropriate. SERD colleagues will also be able to advise on evaluation approach, timing and any practical or ethical issues that may support or prevent a particular approach.

Further detail on how we identify and prioritise areas for evaluation and factors which inform our choice of evaluation type and approach are detailed in Annex A.

# 3.2 Quality assurance

Evaluations can be resource intensive; doing them well requires active and early engagement with subject matter specialists and understanding of the value evaluation evidence can offer.

Supporting evaluation capability and skills, and thereby supporting an evaluation mindset, will support the early consideration of evaluation and ensure research questions

can be addressed with appropriate research methods. This will help ensure colleagues are able to identify when evaluation is needed, what the implications of evaluation are for policy implementation and or rollout, and that appropriate colleagues with the FSA (for example, SERD, operations, policy and so forth) and in delivery partners (for example, local authorities) are engaged to support data collection and evaluation delivery.

In conjunction with ensuring colleagues have the right capabilities and conducting evaluation is appropriately incentivised, use of our existing quality assurance mechanisms will ensure FSA evaluations are robust and their findings credible. Within the FSA, we have the following mechanisms to support quality assurance and the delivery of robust evaluations, which are drawn on where applicable:

- Our existing business case process which captures anticipated benefits of business activities and prompts consideration of how these benefits will be evidenced and in time realised.
- Reference of the Analytical Quality Assurance (Aqua) Book, the Magenta Book and Annexes to ensure processes align with best practice guidance.
- Use of the Advisory Committee for Social Science (ACSS) to act as a critical friend and provide input to our evaluation plans and approach, and to support with peer review.
- Use of external peer reviewers to advise on evaluation approaches and to quality assure evaluation outputs.
- Involvement of a suitably experienced project officer and / or project manager to ensure that evaluations run to time and budget, that risks are managed and key milestones hit.

We will also explore the feasibility of introducing the following additional quality assurance measures:

- Publication of evaluation plans, publication plans, trial protocols, results and datasets as appropriate to evaluation methods before/at the start of evaluations

# where possible and where doing so will not compromise the efficacy of the evaluation or policy development process.

- Publication of supporting documentation (for example, Logical Models/Theories of Change, Project Plans) alongside final outputs.
- Use of the Assurance working group to support the impartial commissioning and delivery of evaluations, including a review of the types of data collected and research questions, through provision of critique of evaluation method and Logic Models etc.

# Building evaluation skills and capability across the FSA

Ensuring FSA colleagues have the skills they need to commission, design, deliver and quality assure evaluations is critical to the delivery of this Action Plan; it also underpins an evaluation mindset.

Delivering the following activities will support the development of evaluation skills across the FSA and the design and delivery of timely, robust evaluation:

- Measure existing levels of awareness and understanding of evaluation and evaluation skills at the FSA to identify key gaps, create a baseline to measure changes in awareness and understanding and to appropriately target learning activities. While this would be a cross FSA activity, conducting focused work within SERD to baseline existing evaluation experience and expertise would support targeted learning and development activities to fill any unmet needs.
- Development of bespoke training programmes for FSA colleagues to increase evaluation skills and awareness. This could include the provision of introductory training for policymakers on the value of evaluation, how it can be integrated into the policy development process, and the strengths, limitations and requirements of different types of evaluation, and the provision of specialist training on the

Note, the FSA already pre-registers protocols for its behavioural trials and makes use of the trial advice panel.

design and delivery of evaluation to increase the capacity for conducting evaluation, with SERD colleagues prioritised.

- Review of existing FSA resources and tools for evaluation to ensure consistency and sharing of good practice. Where gaps are identified, the development of toolkits to support conduct of discrete stages of evaluation (for example, scoping, development of Theory of Change).
- The creation of evaluation drop-in surgeries whereby colleagues can engage with an evaluation expert, discuss options for evaluation and troubleshoot potential evaluation challenges.

In addition to the above, the following tools could be developed:

- A checklist of key evaluation considerations for colleagues to use during the business case process in order to identify appropriate evaluation approaches and the implications of these choices for implementation/rollout of business activities.

# Further raising the profile of evaluation

The FSA already supports the evaluation of its work and incorporates monitoring and benefits measurement into its business practices (see Annex C). Our Strategy includes the guiding principles of being ‘science and evidence led’ and ‘open and transparent’. These align with key precepts of evaluation - that it supports learning, informs action and supports accountability.

To support wider awareness of our evaluation activities, we already publish and promote our evaluation findings, conduct lessons learned sessions and share best practice across the FSA. We propose doing the following to raise the profile of evaluation and support an evaluation mindset across the FSA:

- Seek an evaluation champion(s) at senior level to support the use of evaluations, showcase evaluation activities and the benefits they have delivered, and advocate for training for staff on the benefits evaluation evidence delivers. This could include promotion of the FSA’s evaluation criteria (Annex A), wider government resources designed to promote evaluation (for example, the Magenta Book) and the importance of benefits measurement to our activities.

# Create an evaluation community of practice

who could supporting effective evaluation and the sharing of best practice across the FSA.

# Include a prompt for colleagues

to confirm they have considered how projects are to be evaluated when producing a business case. Tools used by the FSA in Northern Ireland, where proportionate post-project evaluation is a requirement for all discretionary spend, could be used as a template for activities across the FSA. Changes could include a prompt to engage with SERD on the development of an evaluation plan and a requirement to describe which evaluation approach is recommended, which have been considered and which have been discarded. Changes to the business case process will need to align with the BMAP.

# Ensure alignment between benefit measurement process and evaluation activities

to ensure evaluation is proportionate and efficient.

# Create an annual ‘Evaluation Week'

to promote understanding and awareness of the value and benefits of evaluation.

Collectively, the above actions will remind colleagues to plan evaluation, demonstrate the value of evaluation and support the use of evaluation evidence across the FSA.

Next Steps

This Action Plan has set out the FSA’s vision for evaluation and actions that will support the generation and application of evaluation evidence across the agency. We recognise that it will take time and resources to deliver this plan and to realise its benefits. To make this a success a concerted effort is needed by FSA colleagues as well as effective collaboration with our delivery partners.

We intend to review this Evaluation Action Plan periodically to gauge the effect of its implementation and to ensure it remains relevant and fit for purpose. Crucially, we will need to ensure that its recommendations align with forthcoming Benefits Management Action Plan, which is intended to support the measurement and realisation of benefits provided by FSA activities.

In the interim we will work to build on our existing evaluation activities to ensure the FSA continues to be an evidence-led organisation.

16

# Annex A: Guidance for when and how to evaluate

This annex contains more detail on the criteria FSA colleagues should consider when deciding whether to evaluate their work, how to decide the scale of an evaluation and how to decide what type of evaluation is required.

# Evaluation criteria

The FSA has developed a set of criteria based on best practice guidance that should be considered when deciding whether and how to evaluate FSA activities. These are:

Wheper pere is a knowledge gap to be filled by evaluation, including when pe policy or strategy being implemented is novel or untested or where evaluation evidence would inform future practice.
Wheper evaluation is required to demonstrate accountability and transparency over use of public funds and pe fitness for purpose of Agency interventions.
What pe scale of investment associated wip pe policy, programme or project being evaluated has been.
Wheper conducting an evaluation would be feasible and deliver evidence in a timely manner.

These criteria are intended to be used alongside and to complement existing tools, most notably the FSA’s business case process. This process requires colleagues to articulate the anticipated benefit of their proposed activity, current performance in the area (for example, baseline data) and anticipated measures.

# Evaluation scale

Activities can be evaluated in many ways. The nature and range of work done by the FSA, which commonly involves a range of delivery partners as well as cross-organisation and cross-nation working, means a tailored approach to evaluation is required.

# Although final decisions on the scale of evaluation required will have to be taken in the context of wider business needs, available resources, organisational priorities, and what has been specified within business cases, Figure 2 provides a rule-of-thumb guide which FSA colleagues can follow to help choose an appropriate scale of an evaluation. This framework is based on that developed by the UK Space Agency.

Choosing Proportionate Evaluation

Risk and uncertainty
Low - Straightforward low-risk programme wip low uncertainty around pe outcomes
Medium - Programme not especially complex or risky, but some uncertainty around outcomes
High - Complex programme design, and/or significant risk and uncertainty around programme outcomes

Budget and profile:
High - Large programme wip significant budget, and/or high profile wip public interest, and potentially high impact
Medium - Medium-sized programme wip moderate budget, and/or some public interest, expected to have a sizeable impact
Low - Small budget and/or limited public interest

|Category|Risk and uncertainty: Low|Risk and uncertainty: Medium|Risk and uncertainty: High|
|||||
|Budget and Profile:|Level 2|Level 3|Level 3|
|Budget and Profile:|Level 2|Level 2|Level 3|

# Budget and Profile: Low

|Level 1|Level 2|Level 2|
||||
|light-touch evaluation recommended, including before/after monitoring|consider commissioning externally, with appropriate budget allocation|detailed, externally commissioned evaluation with budget of 1-5% of total programme recommended|

Budget thresholds: &lt; £150,000 Low; £150,001-£500,000 Medium; £500,001+ High

The framework recommends that FSA colleagues consider the budget and profile of the activities being evaluated alongside the risk associated with the work and uncertainty over what outcomes and evaluation would deliver when deciding what level of evaluation is needed.

Activities which are lower risk and where there is low uncertainty around the outcomes of an evaluation and where budgets are small would likely suit a ‘light-touch’ evaluation that could include before / after monitoring (Level 1).

Activities where there is some uncertainty around outcomes, but which are not especially complex, may require a larger-scale evaluation, with appropriate budget allocation (Level 2). These activities may require a Level 3 evaluation – a detailed, externally commissioned evaluation with appropriate budget (1-5% of total programme recommended) - when the activity being evaluated has a significant budget, and/or there is high interest in the outcome, and potentially high impact in conducting an evaluation for the future of the policy or programme.

Activities that have complex programme designs, and/or significant risk and uncertainty around programme outcomes typically require a Level 3 evaluation, although in situations where the activities have small budgets or where there is limited media/public interest a Level 2 evaluation may be suitable.

Levels do not need to be distinct: there could be scope for including elements from two levels, for example. Likewise, not all Level 2 or Level 3 evaluations will require externally

Here ‘budget’ includes allocation of funds and internal resources such as staff time.

# commissioned evaluations

In some instances, internally led evaluations may be more appropriate and represent a more effective use of resources. Similarly, while benefits measurement may be delivered through evidence generated in-house, externally commissioned evaluation may provide evidence of unanticipated consequences of an intervention and a more nuanced understanding of the effect of activities.

We anticipate that evaluation of the FSA’s priority programmes and corporate priorities will require either Level 2 or Level 3 evaluations, but that constituent parts of these programmes may require Level 1 or Level 2 evaluations.

# Evaluation type

There are three common types of evaluation: process, impact and value-for-money.

|Process evaluations|consider whether an intervention is being implemented / delivered as intended, whether the design is working and what is working more or less effectively, for whom and why.|
|||
|Impact evaluations|involve an objective test of what changes have occurred, the scale of those changes in an assessment of the extent to which they can be attributed to the intervention.|
|Value-for-money evaluation|involve comparing the benefits and costs of the intervention.|

Which type of evaluation is appropriate depends on the questions being addressed by the evaluation. While further details are available in the Magenta Book, process evaluations typically seek to understand what can be learned from how the intervention was delivered, impact evaluations try to understand what difference the intervention made, while value-for money evaluations seek to address whether the intervention is a good use of resources.

The Magenta Book recommends conducting scoping work prior to deciding the type of evaluation required. For this to be done effectively, a broad range of internal, and sometimes external, stakeholders need to be engaged. In line with the ROAMEF Policy Development Cycle, this scoping work should take place alongside policy development and prior to implementation. It is essential that evaluation be considered at the start of

the policy development and implementation process; suitable benefit measures and plans for how these can be realised must be included in business cases. This is because how the policy is implemented and what data is collected during implementation holds implications for what types of evaluation are feasible. This is particularly the case for impact evaluations, where control/comparison groups are often necessary to demonstrate impact.

Process, impact and value for money evaluations require different approaches and resources (see Figure 3). Specific guidance on this is available in the Magenta Book Annex A. This details the analytical methods for use within an evaluation, including generic research methods for use in process and impact evaluations, methods for experimental and quasi experimental methods for impact evaluation, theory based methods, methods for value for money evaluation and methods for the synthesis of existing evidence. Regardless of evaluation type, most evaluations benefit from quantitative methods being included in some capacity.

|Scoping, Designing and Conducting an Evaluation from the Magenta Book|
||
|Define the rationale of the intervention, what it aims to achieve|
|Identify the purpose of the evaluation; determine the type(s) of evaluation required|
|Process|Impact|Value-for-money|
|What can be learned from how the intervention was delivered?|How and why did the impact occur?|What difference did the policy make?|
|Is it the best use of resources?|
|Approach based on collecting primary data to|Experimental (counterfactual approach)|Quasi-experimental (counterfactual approach)|
|Approach weighs up costs and benefits using monetary|
|data|
|Choose the Evaluation Method(s) (depends on many factors; data from the scoping stage for further negotiation)|
|Collect; review & synthesise data|
|Primary data collection includes performance monitoring, consultative/deliberative methods, observational study; surveys|
|Synthesis evaluation includes meta-analysis or meta-evaluation, realist synthesis, Bayesian synthesis. Reviews include rapid evidence review; systematic review|
|Many evaluations can also be participatory (e.g. using developmental action research methods). These are particularly useful in complex settings (see Complexity Guide)|

The type of evaluation approach and methods used should always be informed by the research questions being addressed and considerations of what is feasible in a given context. While best practice guidance may recommend experimental approaches that can demonstrate causal relationships between control and comparison groups (for example, Level 3 in The Nesta Standards of Evidence), and such experimental methods

are considered by default by the FSA, this may not always be practicable within an organisation’s structure or its regulatory responsibilities. Likewise, certain evaluation types can only be conducted if appropriate baseline data is collected. It is therefore essential that evaluation is not an afterthought but instead integrated into the policy development process.

# 22

# Annex B: Table of recommendations

|Recommendation / Action|Anticipated Benefit|Priority (high / medium / low)|
||||
|Creation of an evaluation group within SERD who could lead on supporting effective evaluation across agency.|Support evaluation mindset; raise profile of evaluation; position evaluation as organizational norm.|High|
|Showcasing completed evaluations and lessons learned sessions at both a programme and FSA level to highlight the value of evaluations among colleagues.|Support evaluation mindset; raise profile of evaluation; position evaluation as organizational norm.|High|
|Measure existing levels of awareness and understanding of evaluation at the FSA to identify key gaps, create a baseline to measure changes in awareness and understanding and to appropriately target learning activities.|Support delivery of robust evaluation; support development of tailored training programme within FSA.|High|
|Alignment between benefit measurement and realisation and wider evaluation activities.|Ensure evaluation is proportionate, efficient and to avoid duplication of effort.|High|

|Recommendation / Action|Anticipated Benefit|Priority (high / medium / low)|
||||
|Inclusion of a prompt for colleagues to confirm they have considered how projects are to be evaluated when producing a business case.|Support delivery of robust evaluation; quality assurance; position evaluation as organisational norm.|High|
|Explore feasibility of publishing evaluation plans, publication plans, and trial protocols before/at the start of evaluations where possible and where doing so will not compromise the efficacy of the evaluation or policy development process.|Support delivery of robust evaluation; quality assurance|High|
|Publication of evaluation results and datasets as soon as possible following completion of the evaluation and where doing so will not compromise the policy development process.|Support delivery of robust evaluation; quality assurance|High|
|Publication of supporting documentation (for example, Logical Models/Theories of Change, Project Plans) alongside final outputs.|Support delivery of robust evaluation; quality assurance|High|

|Recommendation / Action|Anticipated Benefit|Priority (high / medium / low)|
||||
|A checklist of key evaluation considerations for colleagues to use during the business case process in order to identify appropriate evaluation approaches and the implications of these choices for implementation/rollout of business activities.|Support delivery of robust evaluation; Ensure evaluation is proportionate, efficient and to avoid duplication of effort.|High|
|Conduct a skills audit to baseline existing evaluation experience and expertise in delivering specific types of evaluation and using particular methods.|Support delivery of robust evaluation; support development of tailored training programme within FSA.|Medium|
|Seek an evaluation champion(s) at senior level to support the use of evaluations, showcase evaluation activities and the benefits they have delivered, and advocate for training for staff on the benefits evaluation evidence delivers.|Support evaluation mindset; raise profile of evaluation; position evaluation as organizational norm.|Medium|
|Creation of an annual ‘Evaluation Week’ in through which to promote understanding and awareness of the value and benefits of evaluation.|Support evaluation mindset; raise profile of evaluation; position evaluation as organizational norm.|Medium|

|Recommendation / Action|Anticipated Benefit|Priority (high / medium / low)|
||||
|Use of the Assurance working group to support the impartial commissioning and delivery of evaluations, including a review of the types of data collected and research questions, through provision of critique of evaluation method and Logic Models etc|Support delivery of robust evaluation; Ensure evaluation is proportionate; quality assurance|Medium|
|Development of bespoke training programmes for policy professionals and SERD colleagues to increase evaluation skills.|Support delivery of robust evaluation; quality assurance.|Medium|
|Review of existing FSA resources and tools for evaluation to ensure consistency and sharing of good practice across the organisation.|Support delivery of robust evaluation; quality assurance.|Medium|
|The creation of evaluation drop-in surgeries whereby colleagues could engage with an evaluation expert, discuss options for evaluation and troubleshoot potential evaluation challenges.|Support delivery of robust evaluation; quality assurance.|Low|

# Annex C: Current and Forthcoming Evaluation Activity

|Evaluation Activity|Description|Schedule|
||||
|Evaluation of implementation of Prepacked for Direct Sale (PPDS) Legislation|Mixed method evaluation|Autumn 2022/Spring 2023|
|Evaluation of Remote Assessments for FHRS Requested Re-inspections|Qualitative evaluation of use of remote reassessment|Autumn/Winter 2022|
|Evaluation of the pilots for the new Food Standards Delivery Model|Mixed method quasi-experimental evaluation|Ongoing, scheduled to complete Autumn/Winter 2022|
|Evaluation of Achieving Business Compliance (ABC) programme|Modernizing the way food businesses are regulated|Ongoing, scheduled to be reviewed at the end of financial year 24/25|
|Evaluation of Operational Transformation programme (OTP)|Modernizing delivery of Official Controls for meat, dairy, and wine|Planned, covering various elements|

Ongoing reporting activities:

[Annual] Mandatory Local Aupority Returns, Collected as part of pe Management Information System [Annual]
[Annual] The Annual Report and Accounts
[Quarterly] The Quarterly Performance and Resource Report
[Rolling] Benefits Measurement and Tracking of Realisation
[Rolling] Setting and Monitoring of Key Performance Indicators (KPIs)

# Annex D: How the FSA uses Evidence

The FSA is an evidence-led organisation. Below are some case studies illustrating how we have used evaluation evidence to inform our decisions.

# Case study 1: Evaluating the use of remote assessments by local authorities for regulating food businesses (2021)

As part of its response to the coronavirus pandemic the FSA advised local authorities that they may use remote assessments in some circumstances to determine areas to target during a subsequent onsite visit and in other circumstances to inform the need for an onsite visit to assess and address potential public health risks.

These remote assessments can take a variety of forms, for example, a phone call, a video call, or exchange of information online.

The FSA commissioned ICF to undertake a short evaluation of local authorities and food business operators experience of using remote assessments. This was to help inform the FSA’s thinking about future regulatory practice and to ensure the most efficient and effective use of local authority resources.

ICF took a qualitative approach to the evaluation. They first conducted a scoping and desk research phase followed by 20 interviews with local authorities.

The evaluation found that remote assessments were perceived as a helpful tool in the context of the pandemic and for use with low-risk businesses and for Food Standards Controls (for example, assessing menus, labelling). It also highlighted that there were concerns about a perceived potential for food businesses to conceal information and falsify/mask problems with their businesses.

Evidence from this evaluation provided the FSA with valuable insight on the viability of using remote assessments with food businesses. It subsequently informed the decision to conduct further research on the use of remote reassessment in the scenario in which a business requests to be re-rated if they did not achieve the top food hygiene rating and if they have made the required improvements.

More information on this evaluation is available on the FSA’s website.

# Case study 2: Evaluations of recalls and withdrawals

Between 2016 and 2017, the FSA and Food Standards Scotland (FSS) undertook a review of the withdrawal and recall system in the UK food retail sector, to identify if improvements were needed to enhance the current system.

This system redesign aimed to increase consumer awareness of the recall process, outline clear recall roles and responsibilities (for Food Business Operators, local authority enforcement officers, consumers and the FSA) and increase legislative compliance among food business operators (FBOs). The system redesign resulted in the creation of a package of tools, including UK guidance on Traceability, Withdrawals and Recalls, best practice guidance on communicating food recalls to consumers, a template point of sale notice and a Root Cause Analysis (RCA) package.

RSM UK Consulting LLP (RSM) was commissioned jointly by FSA/FSS in 2021 to conduct a process evaluation to explore the programme processes and the partnership approach used as part of these processes, and the success (or otherwise) of achieving: clear and distinct roles/ responsibilities in the new system; consistent and accessible information provided to consumers, and cross industry sharing of approaches and impact; increased public awareness of food recalls and actions they need to take; and, commitment to continuous system improvement.

The evaluation involved a desk review of existing programme documents and data to understand the original evidence base and rationale for change, interviews with external stakeholder reference group members, case studies with FBOs and enforcement agencies involved in recent recalls, exploration of hypothetical scenarios to glean learning on the ability of the redesigned recalls system, focus groups with consumers to explore consumer awareness of product recalls, and secondary data analysis to establish baseline and explore implementation for the post system redesign.

The final report of findings and recommendations is due to be delivered to the agency in August 2022. This evaluation is expected to inform provision of new and tailored guidance for FBOs, communications work to educate and raise awareness of the recalls procedures and resources amongst businesses and consumers, and, an improved system for sharing Root Cause Analysis findings.

Learnings from the evaluation will also be shared across programmes within the FSA to support learning across workstreams. The evaluation report will also recommend that the Agency use the successful Recalls system redesign approach, with clearly designed workstreams and regular engagement with key stakeholders, for any future projects that require partnership working.

# Case study 3: Evaluation of the Food Hygiene Rating Scheme and the Food Hygiene Information Scheme

In 2010/11 the FSA introduced two schemes intended to provide consumers with information about the hygiene standards of food premises so that they can make informed decisions about where to buy food and eat away from home: The Food Hygiene Rating Scheme (FHRS) in England, Wales and Northern Ireland and the Food Hygiene Information Scheme (FHIS) in Scotland (which now comes under the responsibility of FSS). The schemes aim to improve food hygiene standards among food businesses, which are expected to respond to public demand for higher standards, and their overarching goal is to reduce the incidence of food-borne illnesses in the UK population. The schemes are FSA/local authority partnership initiatives which provide consumers with information about hygiene standards in food premises at the time they are inspected to check compliance with legal requirements. The FHRS rating or FHIS result given to the business reflects the inspection findings. Under the FHRS, businesses are given one of six ratings on a numerical scale from ‘5’ (very good hygiene standards) at the top to ‘0’ (urgent improvement required) at the bottom. Under the FHIS, businesses are given either a ‘Pass’ result or an ‘Improvement required’ result.

In 2011, the FSA and FSS commissioned the Policy Studies Institute (PSI) to evaluate these programmes. The overall aim of the evaluation was to assess whether the FHRS and FHIS were operating as intended; whether the schemes improved food hygiene standards at food premises and ultimately contributed to a reduction in food-borne illnesses.

It had two main strands:

- A process evaluation, which took place between autumn 2011 and summer 2013 and collected data from the perspectives of local authority food safety team staff, food business operators and consumers. The process evaluation took a case study approach in which fieldwork took place within a sample of UK local

authorities. Data collection included interviews with local authority officers and food business operators, a survey of food business operators and focus groups with consumers.

An impact evaluation which focused on those local authorities that launched the FHRS or FHIS during the 2010/11 financial year and tested the causal effect of the FHRS/FHIS on two sets of outcomes: i) compliance with food hygiene standards and ii) food-borne illnesses. The study used a difference-in-differences (DID) methodology: outcomes for local authorities that introduced the FHRS/FHIS (in financial year 2010/11) were compared to outcomes for local authorities that did not. The difference between the outcomes observed for the two groups of local authorities provided an estimate of the causal effect of the FHRS/FHIS. Impacts were observed in the first and second years after local authorities launched a national scheme (early adopters), financial years 2011/12 and 2012/13.

To support the evaluations, theories of change were developed to set out the scheme logic and assumptions underpinning behaviour changes for each stakeholder group (local authorities, food business operators and consumers). These theories of change served as the conceptual framework for the evaluation.

The process evaluation validated the theory of change, in the most part, with the UK-wide implementation of the schemes on target at the point the evaluation reported. All but one LA was operating or had committed to run the schemes by the end of the evaluation. That said, there were variations in how the schemes were implemented across the nations with the marketing of the schemes varying a great deal in Northern Ireland and Wales.

The impact evaluation found that the FHRS had a statistically significant positive impact on food hygiene standards; the FHIS scheme was not shown to have had a statistically significant impact on compliance but that the trends were broadly the same as in the rest of the United Kingdom. Business compliance had also improved and there had been a significant decrease in the volume of poorly performing premises due to FHRS. However, due to serious data limitations it was not possible to derive reliable impact estimates testing the effect of the FHRS/FHIS on the incidence of food-borne illnesses.

The evaluation provided valuable evidence for FSA to inform and support the take up of FHRS by local authorities, becoming mandatory in Wales in 2013 (and extended to

business to business in 2014) and in Northern Ireland in 2016. It has been used to build an evidence case in support of England introducing a mandatory scheme. The evaluation also enabled the FSA to take targeted actions to support Local Authorities operating the schemes, encourage businesses to comply and support consumer awareness and engagement. Learnings were also circulated internally to ensure colleagues could reflect these in their workstreams.

33

#
# Standards Food Agency

© Crown copyright 2022

This publication (not including logos) is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. Where we have identified any third party copyright information you will need to obtain permission from the copyright holders concerned.

For more information and to view this licence:

- visit the National Archives website
- email psi@nationalarchives.gov.uk
- write to: Information Policy Team, The National Archives, Kew, London, TW9 4DU

For enquiries about this publication, contact the Food Standards Agency Social Science Team.

Follow us on Twitter @foodgov

Find us on Facebook facebook.com/FoodStandardsAgency