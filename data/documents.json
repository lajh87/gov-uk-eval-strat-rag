[
    "BEIS Monitoring and Evaluation Framework\n\nBEIS Research Paper Number 2020/016\n\nDecember 2020\n\n\u00a9 Crown copyright 2020",
    "December 2020\n\n\u00a9 Crown copyright 2020\n\n\nThis publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated.\nTo view this licence, visit nationalarchives.gov.uk/doc/open-government-licence/version/3 or write to the\nInformation Policy Team, The National Archives, Kew, London TW9 4DU, or email:\npsi@nationalarchives.gsi.gov.uk.",
    "Where we have identified any third-party copyright information you will need to obtain permission from the\ncopyright holders concerned.\n\n\nAny enquiries regarding this publication should be sent to us at:\nenquiries@beis.gov.uk\n\n# Contents\n\nForeword from the Permanent Secretary - 4\n\nExecutive Summary - 5\n\nIntroduction - 7\n\n- 1.1 BEIS\u2019 vision for monitoring and evaluation - 7\n\nMonitoring and evaluation in BEIS - 9",
    "Monitoring and evaluation in BEIS - 9\n\n- 2.1 Why does BEIS monitor policies? - 9\n- 2.2 Why does BEIS evaluate policies? - 10\n- 2.2.1 Types of evaluation used in BEIS - 11\n\nStandards for monitoring and evaluation in BEIS - 12",
    "- 3.1 Where do monitoring and evaluation fit into policy design? - 13\n- 3.2 Monitoring and evaluation planning - 14\n- 3.2.1 Monitoring and evaluation planning for BEIS investments - 14\n- 3.2.2 Monitoring and evaluation planning for statutory commitments to review regulations - 15\n- 3.2.3 Checklist for policy makers: required monitoring and evaluation planning - 15",
    "Achieving the BEIS monitoring and evaluation vision - 31",
    "- 4.1 Establish comprehensive monitoring and evaluation coverage - 31\n- 4.1.1 Requiring monitoring and evaluation plans - 31\n- 4.1.2 Encouraging best practice in our partner organisations - 31\n- 4.2 Embed monitoring and evaluation into governance processes - 32\n- 4.2.1 Evaluation governance - 32\n- 4.2.2 Investment governance - 33",
    "- 4.2.2 Investment governance - 33\n- 4.2.3 Regulatory governance - 34\n- 4.3 Build policy and analytical capacity and capability - 35\n- 4.4 Facilitate a positive learning culture - 36\n- 4.5 Maintain independent, transparent quality assurance of findings - 36",
    "Next steps - 37\n\nAppendices - 38\n\n- A1. Glossary of terms - 39\n- A2. BEIS evaluation case studies - 43\n- A3. Theory of Change template - 49\n- A4. Logical framework (logframe) template - 51\n- A5. Further information on additional data requirements and linking - 52\n- A6. Regulatory post implementation review plan template - 54",
    "# BEIS Monitoring and Evaluation Framework\n\nForeword from the Permanent Secretary\n\nI am pleased to introduce the Department for Business, Energy & Industrial Strategy (BEIS) Monitoring and Evaluation Framework.",
    "BEIS is at the heart of the government\u2019s commitment to build an economy that works for everyone, with great places across the UK for people to work and for businesses to invest, innovate and grow. The Department promotes investment in science, research and innovation to ensure the UK is the most innovative economy, builds long-term strategic partnerships with business to drive increased productivity across all the sectors, works to improve awareness of and access to finance for small and medium enterprises (SMEs) and high growth,",
    "awareness of and access to finance for small and medium enterprises (SMEs) and high growth, innovative-based businesses, promotes competitive markets and responsible business practices, aims to ensure the UK has a reliable, low cost and clean energy system, and promotes global action to tackle climate change. BEIS encourages the utilisation of the best technology to achieve these ambitions.",
    "The development of effective programmes to progress departmental policies depends on timely and accurate monitoring and evaluation to understand and assess progress against objectives, understand what works for whom, how and why, and whether it is value for money. Without continuous monitoring and evaluation, it is not possible to understand how far policies are achieving their goals, nor generate the evidence needed to develop the most effective policies in the future.",
    "This Framework reflects our strong commitment to maintaining and developing a robust evidence base across BEIS and its partner organisations, providing a clear pathway to embedding monitoring and evaluation throughout the policy cycle.",
    "We will continue a coherent and comprehensive programme of work to ensure full monitoring and evaluation (M&E) coverage of our key policies and programmes, M&E is firmly embedded in our governance processes, we continue to build policy, project delivery and analytical capacity and capability, facilitate a positive learning culture and maintain an independent external expert peer review system to quality assure our evaluations.\n\nMy warm thanks to everyone in the department who has contributed to the development of this important Framework.",
    "My warm thanks to everyone in the department who has contributed to the development of this important Framework.\n\n# BEIS Monitoring and Evaluation Framework\n\n# Executive Summary\n\nBEIS currently spends around \u00a314bn a year on a range of policy areas including science and innovation, business and enterprise, energy, and clean growth. It also regulates several areas including labour markets, consumer markets and business law.",
    "The department is accountable for how it spends taxpayers\u2019 money and the financial burden its regulation places on organisations across the UK. To meet its priorities BEIS is strongly committed to using robust evidence to support development, implementation, and improvement of our policies.\n\nThis Framework outlines BEIS\u2019 vision for comprehensive, proportionate, good quality monitoring and evaluation across the department and its partner organisations.",
    "BEIS has benefitted from learning through process evaluations, which assess how the policy is being or has been delivered and whether improvements can be made; impact evaluations, which assess what difference the policy has made and why; and value for money evaluations, which assess the policy by comparing the (monetised) benefits of the policy with its costs (cost-benefit analysis) or compare the relative costs and outcomes (effects) of different courses of action (cost-effectiveness analysis). To fully understand a",
    "and outcomes (effects) of different courses of action (cost-effectiveness analysis). To fully understand a policy\u2019s delivery, effect, and value for money all types of evaluation are typically employed.",
    "BEIS expects policy teams to:",
    "1. demonstrate during policy design that learning from previous monitoring and evaluation has been addressed;\n2. clarify the policy objectives and anticipated effects in a Theory of Change;\n3. assess what level of monitoring and evaluation is proportionate;\n4. assess what evaluation evidence is needed, who will use it, and when it will be required;\n5. identify evaluation objectives and questions;\n6. identify the evaluation approach and methodologies required;",
    "5. identify evaluation objectives and questions;\n6. identify the evaluation approach and methodologies required;\n7. identify monitoring and evaluation data requirements;\n8. secure the internal resources and budget required;\n9. conduct or commission the evaluation;\n10. use and publish the findings.",
    "To achieve the BEIS monitoring and evaluation vision central analysis will seek to:",
    "- Establish comprehensive, appropriate and proportionate monitoring and evaluation coverage across all policies and programmes in BEIS and its partner organisations.\n- Firmly embed monitoring and evaluation into structures and governance processes to ensure that proportionate monitoring and evaluation is delivered, even in challenging circumstances.\n- Build capacity and capability in the policy, project delivery and analytical professions to conduct and commission good quality monitoring and evaluation.",
    "- Facilitate a positive learning culture across BEIS where lessons from monitoring and evaluation inform policy decisions and delivery, and future monitoring and evaluation design.",
    "1 BEIS annual report and accounts 2019 to 2020: BEIS Annual Report and Accounts 2019 to 2020\n\nBEIS Monitoring and Evaluation Framework\n\n\u2022   Maintain independent and transparent quality assurance of evaluation findings, so pat stakeholders can have confidence in pe findings generated from monitoring and evaluation of BEIS policies. This Framework outlines how pis will be achieved.\n\n# BEIS Monitoring and Evaluation Framework\n\nIntroduction",
    "Introduction\n\nBEIS was created in July 2016 and is at the heart of the government\u2019s commitment to build an economy that works for everyone, with great places across the UK for people to work and for businesses to invest, innovate and grow. The department currently spends around \u00a314bn a year on a range of policy areas including science and innovation, business and enterprise, and energy and clean growth. It also regulates several areas including labour markets, consumer markets and business law.",
    "The department needs to be accountable for how it spends taxpayers\u2019 money and the financial burden its regulation places on organisations across the UK. In a recent report on business support schemes the National Audit Office lists monitoring and evaluation plans amongst recommendations for implementation from March 2020. Additionally, BEIS put in place the Government\u2019s response to the COVID-19 crisis, and the government has continued to support the economy approving over \u00a365bn in support, for which a proportionate assurance regime will be implemented.",
    "To meet its priorities, BEIS is strongly committed to using robust evidence to support the development, implementation, and improvement of BEIS policies. Proportionate monitoring and evaluation facilitate the development of the most effective interventions by helping to understand how BEIS policies, projects and regulations are being implemented, what effects they have, for whom, how, why, and in what circumstances. This Framework builds on the 2010 and 2014 evaluation strategies published by the Department for Business, Innovation & Skills",
    "the 2010 and 2014 evaluation strategies published by the Department for Business, Innovation & Skills (BIS) and the building of an evaluation function with a strong emphasis on learning in the Department of Energy and Climate Change (DECC). It outlines BEIS\u2019 vision for proportionate, good quality monitoring and evaluation of interventions across the department and its partner organisations that informs policy design, development and implementation, and legislative options.",
    "HM Treasury has published two books that provide guidance on how to undertake monitoring and evaluation, which this Framework builds on. The Green Book provides guidance on how to appraise and evaluate policies, while the Magenta Book provides guidance on what to consider when designing an evaluation.\n\n# 1.1 BEIS\u2019 vision for monitoring and evaluation",
    "# 1.1 BEIS\u2019 vision for monitoring and evaluation\n\nBEIS' vision is to create the conditions which will allow proportionate, good quality monitoring and evaluation to be consistently used across the department. To achieve departmental\n\nBEIS annual report and accounts 2019 to 2020: Link\n\nnao.org.uk/report/business-support-schemes/\n\nLink\n\nLink\n\nLink\n\nLink\n\nLink\n\n# BEIS Monitoring and Evaluation Framework",
    "Link\n\nLink\n\nLink\n\nLink\n\nLink\n\n# BEIS Monitoring and Evaluation Framework\n\nObjectives, monitoring, and evaluation must also be used to inform and influence key policy decisions and delivery.\n\nTo achieve our vision, BEIS aim to:",
    "Establish comprehensive, appropriate and proportionate monitoring and evaluation coverage across all policies and programmes in BEIS and its partner organisations.\nFirmly embed monitoring and evaluation into governance processes to ensure pat proportionate monitoring and evaluation is delivered, even in challenging circumstances.\nBuild capacity and capability in policy, project delivery and analytical professions to conduct and commission good quality monitoring and evaluation.",
    "Facilitate a positive learning culture across BEIS where lessons from monitoring and evaluation inform policy decisions and delivery, and future monitoring and evaluation design.\nMaintain independent and transparent quality assurance of evaluation findings, so pat stakeholders can have confidence in pe findings generated from monitoring and evaluation of BEIS policies.",
    "Figure 1: Achieving the BEIS monitoring and evaluation vision\n\n|Build policy and analytical capacity and capability|Build policy and analytical capacity and capability|\n||\n|Embed M&E into governance processes|Facilitate a positive learning culture|\n|Establish comprehensive monitoring and evaluation coverage|Maintain an independent, transparent quality assurance of findings|\n\nThis Framework sets out how we will achieve this vision.\n\n# BEIS Monitoring and Evaluation Framework\n\n# 2. Monitoring and evaluation in BEIS",
    "# BEIS Monitoring and Evaluation Framework\n\n# 2. Monitoring and evaluation in BEIS\n\nMonitoring and evaluation provide evidence to improve policies and inform decisions. This chapter summarises why BEIS monitors and evaluates policies and the approaches the department uses.\n\n# 2.1 Why does BEIS monitor policies?",
    "# 2.1 Why does BEIS monitor policies?\n\nAs it says in the Magenta Book, to be done well, both monitoring and evaluation should begin during the policy development with skilled expertise to ensure real-time evidence is available during implementation to aid decision-making. This can result in speedy changes to the policy design and objectives.",
    "Monitoring data are collected throughout an intervention to provide answers to a number of policy, research and performance questions. This data typically covers all aspects of an intervention\u2019s operation and is generally used to help track progress of an intervention\u2019s delivery.\n\nBEIS uses monitoring to:",
    "- assess whether the policy has delivered the target outputs (such as numbers of units installed, or businesses signed up);\n- demonstrate whether a policy is reaching its target population (by assessing the number and characteristics of people, organisations and businesses accessing or using a policy);\n- identify data required to measure inputs, outputs, outcomes and impacts;\n- understand stakeholders\u2019 perceptions/attitudes towards the intervention/behaviour change;",
    "- understand stakeholders\u2019 perceptions/attitudes towards the intervention/behaviour change;\n- establish whether the intended outcomes have been achieved, and identify any unintended effects so senior managers can make informed decisions;\n- link to public administrative, private and academic data (such as tax records, business databases or energy efficiency databases) to create richer data sets by collecting good quality identifiers;",
    "- enable further research and evaluation by collecting contact details and characteristics of those affected by a policy and a similar comparison group who are not affected, before the intervention and after the anticipated impact, to assess that impact;\n- inform cost-benefit analysis and determine whether assumptions about policy implementation, such as cost and time, were correct.",
    "Further information on monitoring and evaluation design can be found in the Magenta Book (HM Treasury guidance on what to consider when designing an evaluation) and the Green Book (HM Treasury guidance on how to appraise and evaluate policies, programmes and projects).\n\nhttps://www.gov.uk/government/publications/the-magenta-book\n\n# BEIS Monitoring and Evaluation Framework",
    "# BEIS Monitoring and Evaluation Framework\n\nAll BEIS policies and projects should develop proportionate good quality monitoring to assess and improve performance and inform learning, ahead of and throughout implementation. Further information on monitoring can be found in section \u20183.2.3.6 identify the appropriate monitoring approach\u2019.",
    "Frequent monitoring and evaluation outputs allow an assessment and explanation of progress towards realizing the intended benefits - benefits management. This enables corrective action to be taken where necessary. Benefits management is mandatory for all BEIS policies, it should be proportionate to the size and complexity of the policy.",
    "Benefits management is a program management approach that aims to make sure the desired business change or policy outcomes have been clearly defined, are measurable and provide a compelling case for investment. Good benefits management, with input from key stakeholders and customers, will help:",
    "- identify what you are aiming to achieve with the intervention;\n- establish end goals \u2013 the desired positive outcomes and benefits from the intervention;\n- set out a process to help monitor and track progress towards the end goals, so you know when you\u2019ve achieved what you set out to deliver, as well as putting measures in place to mitigate risks and increase benefits;\n- identify both the positive and negative effects from change.",
    "As such benefits management provides valuable evidence and data to help in evaluating policies including whether they have delivered what was intended. Further guidance on effective benefits management is available from the Infrastructure and Projects Authority and other sources.\n\n# Why does BEIS evaluate policies?",
    "# Why does BEIS evaluate policies?\n\nAs it says in the Magenta Book \u201cevaluation is the systematic assessment of the design, implementation and outcomes of an intervention\u201d. Well designed and implemented evaluation provides an understanding of the actual economic, financial, social and environmental impacts of a policy; and/or provides an assessment of how it is/was implemented, why it did or did not deliver as expected, and whether it represents value for money.",
    "This information can inform ongoing implementation decisions to maximize the impact of an intervention and can be fed into future development decisions.",
    "Evaluation can inform thinking before, during and after intervention implementation. It can answer questions such as: What can we learn from previous monitoring and evaluation? How is the intervention being delivered? Are there any unintended consequences? What could be improved? What are the emerging impacts? What difference has it made for different groups? How has the context influenced delivery and intended outcomes? How much of the",
    "Infrastructure and Projects Authority. (2017). Guide for Effective Benefits Management in Major Projects [pdf]. Crown Copyright. Available at: https://www.gov.uk/government/publications/guide-for-effective-benefits-management-in-major-projects [Accessed 5th November 2019]\n\nJenner, S. (2014) Managing Benefits. 2nd Edition. The Stationery Office.",
    "The Magenta Book provides guidance on what to consider when designing an evaluation: https://www.gov.uk/government/publications/the-magenta-book\n\n# BEIS Monitoring and Evaluation Framework\n\nImpact can be attributed to the intervention? Can the intervention be expected to make a difference in other contexts? Is the intervention value for money?",
    "There are two primary drivers of policy evaluation: learning and accountability. Learning helps manage risk and uncertainty of the policy and its implementation; provides an understanding of what works for whom, how, why, and in what circumstances; and informs policy development and ultimately decisions. Accountability relates to BEIS being transparent with its stakeholders, for example about how public money is spent (such as informing Spending Reviews, National Audit Office Reviews, and the requirements of the Better Regulation Executive and the Regulatory Policy Committee), how well",
    "Audit Office Reviews, and the requirements of the Better Regulation Executive and the Regulatory Policy Committee), how well an intervention is targeted, and whether a regulation has an appropriate balance between burden and protections.",
    "# Types of evaluation used in BEIS\n\nBEIS achieves learning and accountability through a combination of:",
    "|Process evaluations|Impact evaluations|Value for Money evaluations|\n||||",
    "|Assess how the policy is being or has been delivered and whether improvements can be made. This is often through collecting and analysing stakeholder perceptions and administrative data.|Assess what changes have occurred, what difference the intervention has made and why. They answer questions such as: did it achieve its stated objectives? Who did the intervention affect? How did the effects vary across individuals, groups, sectors, geography, and time? What were the intended and unintended outcomes of the intervention? Can the",
    "sectors, geography, and time? What were the intended and unintended outcomes of the intervention? Can the change be attributed to the intervention? This is investigated through theory-based, experimental, and/or quasi-experimental approaches.|Assess the policy by comparing the (monetised) benefits of the policy with its costs (cost-benefit analysis) or compare the relative costs and outcomes (effects) of different courses of action (cost-effectiveness analysis). These approaches build on evidence gathered from impact evaluations and costs",
    "courses of action (cost-effectiveness analysis). These approaches build on evidence gathered from impact evaluations and costs of the intervention. They answer the question: is the project or programme an economic, efficient, and effective use of resources?|",
    "To fully understand an intervention\u2019s implementation, effect, and value for money all types of evaluation are typically used in combination.\n\nThe exact research questions for each evaluation will be agreed at the start of the evaluation and reflect the needs of the intervention stakeholders, and the context in which it is implemented.\n\nFurther information on evaluation approaches used in BEIS can be found in appendix A2, including case studies.",
    "See https://www.nao.org.uk/about-us/ for more information on the role of the National Audit Office.\n\n# BEIS Monitoring and Evaluation Framework\n\nStandards for monitoring and evaluation in BEIS\n\nTo maximise the potential for robust, useable findings that can inform policy implementation and decision-making, good quality monitoring and evaluation requires planning alongside policy design, linking monitoring and evaluation to policy objectives and evidence needs.\n\nThe checklist below summarises the standards required for policy makers.",
    "|Figure 2: Monitoring and evaluation plan checklist for policy makers|\n||\n|1. Learning|Demonstrate learning from previous monitoring and evaluation has been addressed in the policy design|\n|2. Theory of Change|Clarify the policy objectives and anticipated effects in a Theory of Change|\n|3. Proportionality|Assess what level of monitoring and evaluation is proportionate|",
    "|3. Proportionality|Assess what level of monitoring and evaluation is proportionate|\n|4. Evidence required|Assess what evaluation evidence is needed, who will use it, and when it will be required|\n|5. Objectives and questions|Identify evaluation objectives and questions|\n|6. Approach|Identify the evaluation approach required|\n|7. Data requirements for monitoring and evaluation|Identify the data requirements|",
    "|7. Data requirements for monitoring and evaluation|Identify the data requirements|\n|8. Resources|Secure the resources|\n|9. Evaluation|Conduct / commission the evaluation|\n|10. Use and publish findings|Use and publish the evaluation findings|",
    "# BEIS Monitoring and Evaluation Framework\n\nThis chapter starts by outlining where monitoring and evaluation fit in the policy cycle. The checklist items are discussed in more detail in '3.2 Monitoring and evaluation planning' after a summary of what BEIS expects in monitoring and evaluation plans for BEIS investments and regulations.\n\n# 3.1 Where do monitoring and evaluation fit into policy design?",
    "# 3.1 Where do monitoring and evaluation fit into policy design?\n\nGood monitoring and evaluation are built into the design of a policy and thought about throughout its development and implementation. This allows both the programme and the evaluation to be tailored to maximise the potential for robust, useable findings that can help future decision-making. The design of a policy will affect how rigorously it can be evaluated.",
    "Failure to think about monitoring limits the ability to track progress, while failure to think about evaluation and build this into the policy design can preclude a reliable understanding of the impact of the policy being achieved; appropriate baseline data is often not collected and a comparison or control group is not available to help understand what would happen in the absence of the policy.",
    "The Green Book presents a framework of how monitoring and evaluation fits into the policy development process. As it says in the Magenta Book monitoring and evaluation have a role at each stage in the policy cycle.\n\nFigure 3: The policy cycle",
    "|Rationale|Rationale: Why is government intervening? What is the problem that government is trying to solve? What does the evidence say about this problem?|\n|||\n|Objective|Feedback: What have we learnt? How will we use these results in future? Objective: What would success from the intervention look like? What metrics can we use to measure success?|",
    "|Appraisal|Evaluation: Research and analysis answer the questions: Did the intervention work as expected? What was the impact on who and why? Was it cost-effective?|\n|Monitoring|Appraisal: What are the options for intervening? What is the likely evidence on the effectiveness and cost-effectiveness of these options?|\n|Feedback|Monitoring: Data collection to answer the questions: Did we do what we said we would do? How are our success metrics changing over time?|",
    "Measurement of conditions prior to implementation of a project against which subsequent progress can be assessed, such as behaviors, attitudes, employment, turnover, emissions\n\nHM Treasury guidance on how to appraise and evaluate policies, programs, and projects: The Green Book\n\nHM Treasury guidance on what to consider when designing an evaluation: The Magenta Book\n\n# BEIS Monitoring and Evaluation Framework\n\nThe outputs and learning from earlier evaluations feed into the rationale, objectives, and appraisal stages of the cycle.",
    "# 3.2 Monitoring and evaluation planning\n\nMonitoring and evaluation are most suitable for informing policy implementation and decisions when they have been planned alongside the development of the policy. This section discusses requirements for the monitoring and evaluation of BEIS investments and regulations, then discusses each item in the \u2018monitoring and evaluation plan checklist for policy makers\u2019 in turn.\n\n# 3.2.1 Monitoring and evaluation planning for BEIS investments",
    "# 3.2.1 Monitoring and evaluation planning for BEIS investments\n\nThe BEIS Projects and Investments Committee (PIC) scrutinises and approves significant, risky or contentious investments of taxpayer funds undertaken by the department.",
    "PIC considers strategic, outline and full business cases in terms of: their strategic fit with ministerial priorities; the economic rationale for investing taxpayer funds according to the recommended option; assurance that expenditure is regular and proper and offers value for money; confirmation that the proposed expenditure is affordable; and confidence that the project is deliverable, considering timescale and the capability and capacity of those responsible. This aligns with the Treasury\u2019s five-case methodology as set out in the Green Book, which includes a strategic,",
    "with the Treasury\u2019s five-case methodology as set out in the Green Book, which includes a strategic, economic, commercial, financial, and management case.",
    "A monitoring and evaluation plan is part of the management case. Figure 4 summarises what is required at each stage of a BEIS Project Investment Committee business case.",
    "|Investment monitoring and evaluation planning requirements within BEIS|How previous M&E learning has informed project or policy development|\n|||\n|Strategic Business Case|A high-level outline of the purpose of the evaluation|\n| |An articulation of the expected impacts and outcomes of the intervention|\n| |An indication of the scale of the evaluation, and justification based on proportionality|\n| |An initial estimate of budget and resources for M&E|",
    "| |An initial estimate of budget and resources for M&E|\n| |A discussion of the scope for piloting and testing prior to implementation|\n|Outline Business Case|Above box plus: Theory of Change (a logic model plus a narrative of what is expected to happen)|\n| |Reviewed by key stakeholders and signed off by the SRO/programme board|\n| |The main monitoring and evaluation objectives|",
    "| |The main monitoring and evaluation objectives|\n| |The likely uses and users of the M&E evidence to assess the appropriateness of the M&E approach and timescales|\n| |High level timetable setting out main stages of evaluation planning and delivery against the programme timetable, including BEIS Peer Review Group review(s)|\n|Full Business Case|Above boxes plus: An initial set of evaluation questions|",
    "|Full Business Case|Above boxes plus: An initial set of evaluation questions|\n| |Detail of monitoring and evaluation arrangements including delivery and reporting framework; including a log frame of key arrangements and partners|\n| |Indication of any research required (type, scope, method of engagement)|\n| |An initial data sources; milestones, targets and reporting plan|\n| |The budget and resources allocated for M&E and evaluation expert|",
    "With a whole life cost of \u00a320m or above either to BEIS or the economy as a whole.\n\nThe Green Book - Appraisal and Evaluation in Central Government\n\n# BEIS Monitoring and Evaluation Framework",
    "# BEIS Monitoring and Evaluation Framework\n\nThe Performance and Risk Committee (P&R) provides assurance for the BEIS Executive\nCommittee and the Permanent Secretary on the performance and delivery of BEIS\u2019\nprogramme and policy commitments, as well as its Partner Organisations. It assesses\nperformance against agreed programme outcomes and milestones and provides assurance on\nthe operation of the BEIS monitoring and evaluation framework, including monitoring and\nevaluation plans submitted to PIC as part of a business case.",
    "Monitoring and evaluation planning for statutory commitments to review regulations\n\nDepartments and partner bodies are required to produce impact assessments (IAs) assessing\nthe costs and benefits of regulatory changes prior to consultation, enactment and\nimplementation. The evidence and analysis used within IAs are scrutinised by the independent\nRegulatory Policy Committee (RPC).",
    "Post implementation reviews (PIRs) of these regulatory changes are a key element of the\npolicy-making cycle and provide an evidence-based evaluation of the effectiveness of a\nmeasure after it has been implemented and operational (after an appropriate period of time). A\nPIR will review: the original policy objectives; the extent to which the measure is achieving its\nintended effects/meeting its objectives; whether there have been any unintended",
    "intended effects/meeting its objectives; whether there have been any unintended\nconsequences; how well it is working; and the reasons why. It will also assess whether the\nobjectives could be achieved with a system that imposes less regulation.",
    "Evidence from PIRs will support decisions about the next steps for a measure, which are:\n\n- Renewal - measure continues without change\n- Amendment - measure remains but changes are made to improve it\n- Removal - measure is removed without replacement\n- Replacement - measure is replaced or redesigned substantially",
    "A summary of the monitoring and evaluation plan for the PIR is part of the IA template and\nBEIS requires a more detailed PIR review plan to be completed for every high impact\nstatutory commitment to review. A PIR Plan template can be found in appendix A6. Analytical\nguidance on conducting a PIR is provided as supplementary guidance to the Magenta Book.\n\nChecklist for policy makers: required monitoring and evaluation planning",
    "Checklist for policy makers: required monitoring and evaluation planning\n\nThe remainder of this chapter takes the key aspects of monitoring and evaluation planning\nfrom the checklist at Figure 2 and outlines what is expected in BEIS.\n\n# BEIS Monitoring and Evaluation Framework\n\n3.2.3.1 Demonstrate learning from previous monitoring and evaluation has been addressed in the policy design",
    "As outlined in the \u2018feedback\u2019 stage of the policy cycle, policy development should not be conducted in isolation; learning from previous monitoring and evaluation and wider evidence should be built on. There should be some discussion of previous evidence, and how it has informed development of the intervention, and proposed monitoring and evaluation plans (in terms of key evidence gaps, for example) - or clarity that such evidence does not exist if it's a novel area.",
    "3.2.3.2 Clarify the policy objectives and anticipated effects in a Theory of Change",
    "As outlined in the Magenta Book (2.2.1), good policy making necessitates a thorough understanding of the intervention and how it is expected to achieve the expected outcomes, this also informs the monitoring and evaluation. The first thing to do is identify the objectives and anticipated outcomes and impacts of the policy. It is important to set this out clearly, to provide common understanding and framework for the evaluation plan and help identify exactly what the evaluation is assessing. The recommended way to do this is to",
    "evaluation plan and help identify exactly what the evaluation is assessing. The recommended way to do this is to set out the logic in a Theory of Change. For complex policies, it may be necessary to develop a series of linked Theories of Change (for more information on evaluating complexity see the Magenta Book supplementary guidance).",
    "At an early stage of policy development, Theories of Change can be undertaken as a desk exercise, based on a review of policy documentation. However, developing and testing the theory with key stakeholders \u2013 for example organisations involved in the delivery and representatives of different groups affected by the intervention, will ensure all important issues are identified and assumptions are reality checked. The Theory of Change should be signed off by the Senior Responsible Officer and/or the project/programme board.",
    "The benefits of a well-developed Theory of Change include:\n\n- Improved policy development\n- A shared vision of the intervention and end goals\n- Enabling early thinking on how intervention success will be measured and understood\n- A clear one page \u2018story\u2019 of the intervention\n- Improving intervention delivery and management\n\nReference:\n\n24 https://www.gov.uk/government/publications/the-magenta-book",
    "Reference:\n\n24 https://www.gov.uk/government/publications/the-magenta-book\n\n25 \u2018Handling Complexity in policy evaluation\u2019: https://www.gov.uk/government/publications/the-magenta-book\n\n# BEIS Monitoring and Evaluation Framework\n\nFigure 5: Theory of Change\n\nContext and rationale\n\nHow the policy links to BEIS objectives. The issue to be addressed, the context in which it is located, why the government should intervene\n\nPolicy objectives",
    "Policy objectives\n\nHow the policy is supposed to effect its various target outcomes\n\nOutcomes\n\nOutputs\n\nThe ultimate objectives of the policy, long term\n\nImpacts\n\nSpecific short- and medium-term changes BEIS wants to achieve, related to the policy objectives, such as take up of energy efficiency measures, skills increases; new carbon technologies developed",
    "The wider economic, financial; social or environmental effects and results, such as behavior change, jobs created, energy and greenhouse gas emission reductions, or increased productivity\n\nInputs\n\nWhat resource is provided e.g.: grant funding; information\n\nAssumptions, risks, mitigations\n\nWhat factors and processes need to be in place to enable this progression?\n\nEvaluation objectives\n\nThe focus of the systematic assessment\n\nA well-developed Theory of Change should include:",
    "- A summary of the context and rationale for intervention - how the intervention links to BEIS objectives and priorities, the issues being addressed, and the context within which intervention takes place.\n- It should include the problem, its causes and its consequences, how the intervention will address the problem, and why the government should intervene (based on the existing knowledge base).",
    "- It should summarize how the intervention aligns or overlaps with other similar interventions that may also affect the expected outcomes, and use these insights to help establish the boundaries around the intervention.\n- SMART intervention objectives - how the intervention is supposed to affect its various target outcomes and what success would look like.",
    "Creating a Theory of Change usually involves working backwards from impacts to inputs:",
    "- Start with the expected impacts (the ultimate objectives of the intervention, the long-term economic, social and environmental outcomes such as reduced energy costs or change in employment attributed to the intervention).",
    "- Then work backwards through the outcomes (what specific changes in attitudes, behaviors or skills BEIS wants to achieve \u2013 such as take up of energy efficiency measures; increased skills; new technologies developed), the outputs (what is expected to happen as a direct result of the proposed activities \u2013 such as the number of projects supported), and finally the expected policy inputs (resources provided such as grant funding, equipment and information).",
    "The boxes in the diagram (containing inputs, outputs, outcomes and impacts) should be linked by arrows that represent causal relationships (the box at the end of the arrow is caused as a result of the box at the start of the arrow).\n\n17\n\n# BEIS Monitoring and Evaluation Framework\n\nIn addition to the summary of the context and rationale for intervention the accompanying narrative should include:",
    "- The assumptions made about how these elements link together and enable the project to successfully progress from one element to the next (e.g. consumers are engaged/informed; firms are aware of and take up loans; investors can be identified; new technologies are used, etc.)\n- Potential risks (businesses seek subsidies to raise returns on investments they would have made anyway, applicants don\u2019t have the skills and resource to manage the delivery) and mitigating activities.",
    "- Quality of evidence underlying the causal chains throughout, signposting where the weaker, riskier aspects of the Theory of Change are.\n- Areas of uncertainty or interest on which the evaluation should focus.\n- The evaluation objectives \u2013 the focus of the systematic assessment.",
    "The Theory of Change should be updated regularly as the policy develops using the monitoring and evaluation evidence. A Theory of Change template can be found in appendix A3.\n\n# Assess what level of monitoring and evaluation is proportionate",
    "# Assess what level of monitoring and evaluation is proportionate\n\nAs outlined in the Magenta Book (1.9) not all interventions will require the same level of scrutiny or have the same learning needs. High-risk, high status policy breaking new ground is likely to require a large-scale evaluation (such as a pilot and investments that are scrutinised by PIC).",
    "While low-risk, well-evidenced and low priority interventions may only necessitate a light-touch monitoring and evaluation exercise to ensure it has been delivered as intended and achieved the predicted outcomes. The RPC proportionality guidance and Magenta Book supplementary guidance for conducting regulatory post implementation reviews discuss the levels of evidence required for post implementation reviews.\n\n# Assess what evaluation evidence is needed, who will use it, and when will it be required",
    "It is important to have a clear idea from the start about the intended use and audience (see 1.10. in the Magenta Book) for the evaluation: what will the findings feed into and inform? When is the information required by different stakeholders? This will help direct all stages of the evaluation and ensure maximum impact from the findings and should be set out in the monitoring and evaluation plan, aligning with the high-level project timetable.",
    "Assumptions tend to occur in three categories: 1. Related to causality \u2013 what you think is going to lead to what 2. Programme implementation \u2013 how you assume it will be implemented 3. External factors that you assume will happen and that are necessary for success.\n\nLinks to resources:\n\n- The Magenta Book\n- Proportionality in Regulatory Submissions Guidance\n- The Magenta Book\n- The Magenta Book\n\n# BEIS Monitoring and Evaluation Framework",
    "# BEIS Monitoring and Evaluation Framework\n\nTo ensure the evaluation provides useful evidence, it is important to consider the requirements of the users of the findings before the evaluation commences so it can be designed to answer these specific questions and deliver findings when they are needed.\n\nWhen developing the evaluation plan, it is important to understand:",
    "- Who the target end-users of the findings will be \u2013 e.g. project/programme managers and the project/programme board; other policy managers and analysts within BEIS; other government departments; delivery bodies; key stakeholders including industry bodies and energy suppliers, local community groups, etc.",
    "- When findings are required, and the timing of decisions they need to feed into \u2013 e.g. whether to roll out the pilot, improvements and learning that can help decide whether to invest in future programme waves, or inform Spending Review prioritisation\n- What form of evidence is required \u2013 e.g. a quantitative cost-benefit assessment of impacts may be required by HM Treasury, while detailed information about effective delivery mechanisms may be sought by project/programme managers of similar policies",
    "By understanding the range of requirements for the evaluation and their relative priority, the evaluation can be tailored to generate the relevant evidence to the required timescales, and/or (just as importantly) a decision can be taken early about the questions which can realistically be answered in the desired timescales.\n\n# Identify the evaluation objectives and questions\n\nThe next step is to clarify the evaluation objectives, and what questions the evaluation needs to address.\n\nEvaluation Objectives",
    "Evaluation Objectives\n\nEvaluation objectives need to be clearly defined and meaningful to narrow the focus of the evaluation and ensure that the findings are relevant to decision makers.\n\nReturning to the Theory of Change should help you formulate the evaluation objectives and specific evaluation questions, since this will have identified the anticipated outcomes and impacts and the underlying assumptions that might need to be tested, and the evidence gaps that need to be filled.",
    "Spending reviews typically take place every two to five years. They normally set departmental budgets for three to five years ahead and shape the scale and nature of public service programmes and public investment.\n\n# BEIS Monitoring and Evaluation Framework\n\nExamples of evaluation objectives include:",
    "Examples of evaluation objectives include:\n\n- Offering \u2018lessons learned\u2019 to inform development of the planned main Heat Network Investment Project (HNIP) pilot scheme and any future similar schemes\n- Identifying what economic and social impact the Catapult activities have had; understanding the contribution made from the Catapult to businesses, academia and the wider research community\n- Understanding the impact of providing additional funding to Local Authorities to target energy efficiency installations among private rented sector (PRS) properties within the Green Deal Communities project",
    "Evaluation questions Evaluations can be designed to answer a wide range of potential questions. It is important to be clear from the outset what these questions are and how the findings from them are expected to be used, by whom and when. This will define the scope of the evaluation and inform the evaluation approach. Well-developed evaluation questions should reflect the objectives of the evaluation, the objectives of the intervention, as well as the priorities and evidence needs of stakeholders.",
    "Questions that will help with the development of the evaluation questions include:\n\n- How will you determine if the intervention is a success?\n- How will you know if the intervention is on track?\n- Do you need to understand why the intervention does/does not achieve anticipated outcomes?\n- What contextual factors might affect delivery (e.g. economic climate, other policy - measures, innovation developments, etc)?\n- What learning from this intervention could be transferrable to other policies or future waves?",
    "Heat networks typically convey hot water from a shared heat source (or sources) to meet demand for space and water heating, and space cooling distributed across a number of buildings. Heat networks are important because they can provide an opportunity for greater energy efficiency, lower prices for consumers and carbon savings compared with conventional gas or electric heating. The Heat Network Investment Project is a funding mechanism that responds to the current low levels of heat networks in place and the difficulties relating to financial investment. Above all, the aim of",
    "levels of heat networks in place and the difficulties relating to financial investment. Above all, the aim of HNIP is, alongside other measures, to contribute to the establishment of a self-sustaining UK heat networks market that does not require Government subsidy. See: Heat Networks Investment Project Evaluation",
    "The Catapult centres are a network of world-leading centres designed to transform the UK's capability for innovation in specific areas and help drive future economic growth, see Catapult Programme Evaluation Framework\n\nEvaluation of the Green Deal Communities Private Rented Sector Funding\n\n# BEIS Monitoring and Evaluation Framework\n\nExamples of evaluation questions include:",
    "# BEIS Monitoring and Evaluation Framework\n\nExamples of evaluation questions include:\n\n- What has worked well, less well and why?\n- To what extent and how have the anticipated outcomes been achieved?\n- How has the context influenced outcomes?\n- How much of the impact can be attributed to the intervention?\n- Have different groups been affected in different ways, how, why, and in what circumstances?\n- Is the intervention a good use of resources?",
    "A long list of evaluation questions should be developed, which can then be grouped by theme and prioritised into a smaller number of high-level questions under which the more detailed questions will sit, according to:",
    "- How important they are to stakeholders (those funding, designing, implementing, or impacted by an intervention)\n- Whether they reflect the objectives of the policy and wider departmental priorities\n- Whether they reflect the key elements of the Theory of Change\n- Whether answering them will provide the information required at key decision points\n- Whether answering them will fill evidence gaps\n- Whether they will provide information which can be acted upon to make delivery improvements",
    "- Whether they will provide information which can be acted upon to make delivery improvements\n- Whether they can be answered using the available resources (such as budget and staff) and within the appropriate timeframe",
    "# Identify the appropriate monitoring approach\n\nMonitoring data relates to information collected and used as part of the ongoing intervention delivery to understand progress against objectives. Data which is directly useful to those collecting it tends to be better quality than data collected solely for the purpose of research. All projects and programmes should undertake monitoring of appropriate indicators (referred to as \u2018measures\u2019 in benefits management).",
    "Monitoring and evaluation are complementary activities, and ideally the design and requirements for each should be considered together, so the comprehensive data needs of the policy can be considered in the round. This will facilitate the collection of relevant and high-quality data and avoid duplication or missed opportunities for the collection of key data. Early",
    "For example, number of applications, number of firms introducing new/improve products and services, number of smart meters in homes and small businesses across Britain, or % of people with smart meters who say they\u2019ve taken steps to reduce their energy use, number of jobs safeguarded.\n\n# BEIS Monitoring and Evaluation Framework\n\nIdentification of any existing data or other ongoing data collection processes that can be utilised for the evaluation will ensure best use of resources and effort.",
    "Create a reporting plan for monitoring data. Regular reporting (e.g. through monthly highlight reports to programme boards, or more frequent updates to key stakeholders) of key performance indicators will provide management assurance that an intervention is on track. Using emerging evaluation evidence to understand why this is the case, the evidence can inform changes to the intervention to manage performance and help realise the anticipated benefits.",
    "To achieve the benefits of monitoring and evaluation, it is important that the evidence flows to the right decision makers (policy and other stakeholders) at the right time. Interventions often conduct annual reviews, an assessment of policy progress, including against agreed milestones. See the Smart Metering Implementation Programme Progress Report and the UK Climate Finance Results.",
    "At departmental level, BEIS\u2019 vision for internal reporting is that all directorates and full business cases that have been approved by PIC have at least one indicator to measure achievements of outputs that should be regularly reviewed and used in reporting to Cabinet Office.",
    "All directorates and programmes/projects in the departmental portfolio are required to complete a performance report each month, for submission to the Portfolio Office via the Department\u2019s Online Reporting in BEIS (ORB) system. Through this, P&R assesses the performance of Directorates and Partner Organisations, and commissions reviews, stocktakes and deep dives to further scrutinise the performance of specific policy commitments, programmes and projects.",
    "Creating appropriate indicators. Monitoring is underpinned by the selection of appropriate indicators. They should be SMART, use good quality established data sources and be proportionate. They should also reflect the direct consequence of the activity undertaken - this can be difficult for outcomes and impacts which can be affected by other activities. Figure 6 shows the basic principles to be considered when identifying an indicator or a series of indicators.",
    "Principles of a good indicator\nSimple \u2013 it should have a clear definition and be easy to measure, such pat its calculation and interpretation can be easily understood in pe same way by different parties wipout any complicated context. A small number of indicators pat cover only pe important factors are easier to collect and for stakeholders to engage wip.",
    "Relevant \u2013 it should reflect only what can be controlled, be clear about how pat activity can influence pe indicator, and about what good progress looks like. This can be achieved by relating to pe intervention\u2019s Theory of Change, alpough wider effects have increasing influence along pe impact papway.",
    "Sources: Smart Meter Progress Report 2018, UK Climate Finance Results\n\n# BEIS Monitoring and Evaluation Framework\n\n3. Timely - it should use regularly available data that does not have any lag which is difficult to explain.\n\n4. Reliable \u2013 it should be objectively verifiable, use a good quality data source and be applicable over time. It should also be robust, and not overly sensitive, for example to noise. It should also stand up to external scrutiny.",
    "5. Comparable - it should be readily trackable over time, but also be consistent with other indicators within a policy area, and across the department. This should feed into a logframe (see 3.2.3.8 Identifying data requirements for monitoring and evaluation) and a Key Performance Indicator (KPI) framework against which projects and programmes can report.",
    "While indicators associated with inputs and outputs are typically used in performance monitoring, outcome and objective indicators are most suited to longer term policy impact. However, many impacts may not be realised for some time after implementing the intervention. It is important to use the Theory of Change to understand the pathway to impact and what is needed to achieve the ultimate objective. For early-stage monitoring, this may mean relying upon input and/or output indicators, which will often be easier to record sooner and more frequently. These therefore can",
    "and/or output indicators, which will often be easier to record sooner and more frequently. These therefore can be used to indicate progress towards longer term objectives, and to inform any outstanding policy development or implementation decisions.",
    "# Identify the appropriate evaluation approach\n\nTypes of evaluation used in BEIS are outlined in 2.2.1. The initial step in deciding on evaluation approach should always be to clearly articulate the objectives and questions for the evaluation, the context and the information or data available; decisions on the approach and methodologies then follow.",
    "Reviewing the evaluation questions will help inform the type of evaluation required. The scale and depth of the questions will inform whether the evaluation needs to be a full impact, value for money and process evaluation, a light touch assessment, or somewhere in-between (see 3.2.3.3).",
    "Additionally, the level of intervention activity and the availability of data may constrain the number of questions an evaluation can realistically answer and type of evaluation which can be conducted (see the Magenta Book38 for further information).\n\n# Identify data requirements for monitoring and evaluation",
    "Monitoring and evaluation data requirements should be built in from the start of any intervention, so that collection processes are established alongside policy design and related legislation. This includes accounting for legal, ethical, and practical issues associated with the data collection. It will often involve building the collection of data, to required standards, into delivery arrangements with external partners. Relevant evaluation data would then normally be collected before the intervention starts (baseline), during the delivery (interim), and after completion (follow-up), to allow",
    "starts (baseline), during the delivery (interim), and after completion (follow-up), to allow \u2018before and after\u2019 change to be assessed. The exact length of time for collection of \u2018after\u2019 data should be aligned with when outcomes and impacts are expected to materialise.",
    "38 https://www.gov.uk/government/publications/the-magenta-book\n\n# BEIS Monitoring and Evaluation Framework\n\nIt may be possible to address some evaluation questions using data collected after delivery, but this tends to result in weaker designs and less robust findings. For example, asking people retrospectively to remember what they did or thought sometime before the intervention was delivered is an unreliable method of measuring behavioral or attitudinal change.",
    "First consider what sources of data already exist and may be appropriate for monitoring and evaluation. You may need to weigh up the quality and usefulness of existing data (e.g. Local Authority, existing survey, or administrative data) against the practicality and feasibility of collecting new data to assess outcomes (see chapter 4 for the Magenta Book for further information).\n\n# Creating a logical framework (logframe)",
    "# Creating a logical framework (logframe)\n\nTheories of Change provide a basis for the development of a monitoring logical framework (logframe). Impacts, outcomes, and outputs from the Theory of Change should all have appropriate indicators to show what is being measured. They also need milestones and targets to show the desired value or direction of progress, and baselines to show the starting point and a source. KPIs should then be recorded in ORB. The basic indicator principles are:",
    "- Use existing sources of information where available\n- Indicators should be specific and measurable\n- Indicators should be disaggregated where possible (such as by size/type of business, geography, age, gender, ethnicity, disability \u2013 specific to intervention objectives)\n- Consider including both quantitative and qualitative indicators\n- Logframes should be developed with delivery partners and responsibility for data collection built into delivery contracts",
    "Arrangements will need to be negotiated and sometimes legislated. Commercial sensitivity, consent, data validation, transfer and storage and disposal will need to be considered carefully. Baseline data (collected at a time point before the policy is implemented) will commonly need to be collected from participant and non-participant groups. There can be costly consequences of leaving this too late.\n\n# BEIS Monitoring and Evaluation Framework",
    "|PROJECT NAME|World Bank Partnership for Market Readiness (PMR)|\n|||\n|IMPACT|Impact Indicator|Baseline (31 Oct 2016)|Milestone|Milestone (31 Oct 2020)|Target|\n|Substantial CO2 abatement as a result of market mechanisms|(KPI 11)|(May 2011)|Planned|E2.3m|E4.6m|\u00a37m|",
    "|Impact Indicator 2|Baseline|Milestone|Milestone 2|Target|\n|Extent to which ICF intervention is likely to have transformational Impact.|(Qualitative KPI 15)|(May 2011)|(31 Oct 2016)|(31 Oct 2018)|(31 Oct 2020)|\n|OUTCOME|Outcome Indicator|Baseline|Milestone|Milestone 2|Target|",
    "|OUTCOME|Outcome Indicator|Baseline|Milestone|Milestone 2|Target|\n|Market mechanisms in at least developing countries by 2020.|No. of participating countries implementing market mechanisms|Planned| | |Source: PMR Secretariat Partnership Assembly meetings PMR website|\n|Outcome Indicator 2|Baseline|Milestone|Milestone 2|Target|",
    "|Outcome Indicator 2|Baseline|Milestone|Milestone 2|Target|\n|Quantity of emissions reductions (in MtcO2e) directly resulting from implementation of market mechanisms supported by the PMR| |Planned|TBC|TBC| |Source: PMR Secretariat Partnership Assembly meetings PMR website|",
    "A good logframe should:",
    "- Have at least one indicator per impact, outcome, activity, and output\n- Include indicators that are relevant and appropriate to their impact, outcome, or output\n- Have distinctly labelled indicators\n- Have clear, measurable indicators\n- Detail a baseline for each indicator\n- Detail targets and milestones (including month and year) for each indicator and record whether these are achieved\n- Indicate when they will be updated",
    "- Indicate when they will be updated\n- Include a clearly labelled source and identify who is responsible for collecting the data for each indicator",
    "Impact indicator 2 KPI 15 measures \u2018transformational change\u2019. The ICF will have greater impact if it can be \u2018transformational\u2019 by, for example, encouraging others to replicate and scale-up successful activities and facilitating substantive institutional and policy change toward a low carbon and climate resilient future. ICF programme/portfolio managers should annually report an overall assessment score of the likelihood that transformation is linked to the ICF support (programme, country, region or sector portfolio) and a qualitative",
    "linked to the ICF support (programme, country, region or sector portfolio) and a qualitative narrative.",
    "When identifying key indicators time should be taken to ensure they are the most appropriate to measure progress against the policy objectives; for example, should you be collecting gross or net data? Will patents signify commercial applications within your timeframes?\n\n# BEIS Monitoring and Evaluation Framework\n\nAny targets and milestones that cannot be set initially should state TBC and the risks and assumptions identified in the Theory of Change should also be actively monitored through risk indicators so that mitigating actions can be taken if required.",
    "Define indicator methodologies\n\nIt is best practice to agree methodologies for the data collection to inform your indicators, providing a rationale for data collection, defining what should and should not be included, and explaining what will be reported. This should facilitate data consistency and quality and be included in guidance for those conducting the data collection.\n\nSee International Climate Finance results indicator methodologies here.\n\nMonitoring data for regulatory post-implementation reviews",
    "Monitoring data for regulatory post-implementation reviews\n\nMonitoring data can provide a relatively light-touch evidence base for a regulatory Post Implementation Review (PIR), which is particularly useful in cases where it is not proportionate to undertake substantial primary data collection or additional analysis. In some cases, monitoring information relevant to a regulation may already be captured on a regular basis. In other circumstances, it will be necessary to plan to ensure that the data that will be useful for a PIR are collected.\n\nEvaluation data",
    "Good quality evaluations are only possible with consistent, accurate and complete data. In addition to monitoring and administrative data, this would often include new data collected specifically for the evaluation, including data to inform any value for money assessment. It is important to note that an impact evaluation will often require data to assess the counterfactual, i.e. data from a comparison or control group (e.g. businesses or households) who are not affected by, or in receipt of, the intervention. Information from such",
    "or households) who are not affected by, or in receipt of, the intervention. Information from such groups can often be challenging to obtain.",
    "The data required for an evaluation will be used to assess the inputs, outputs, outcomes and impacts of the intervention. It will also be used to test how these elements are linked together (i.e. the processes and assumptions). Generally, baseline (i.e. pre-intervention) data will be required to show what changes have occurred since the policy has been implemented to assess changes in attitudes and behavior, for both \u2018treated\u2019 and comparison or control groups.",
    "Quantitative data would normally be collected to assess change in outcomes in both the \u2018treated\u2019 and counterfactual groups \u2013 e.g. surveys conducted before and after intervention implementation to assess change in attitudes and behavior.",
    "Qualitative data should be collected to assess implementation processes and whether anticipated outcomes and impacts have materialized (those which cannot be assessed quantitatively); identify any unintended outcomes, recognize the influence of wider contextual factors, or participants\u2019 experiences or perceptions of implementation; and examine the processes involved in transforming inputs into outcomes to understand what works for whom, how, and in what context.\n\nFurther information on evaluation data collection, including secondary data and matching data can be found in appendix A5.",
    "Secure the resources\n\nAs already mentioned, a judgment needs to be made about the scale and form of evaluation that is required for an intervention, including whether it should be commissioned externally or\n\n# BEIS Monitoring and Evaluation Framework",
    "# BEIS Monitoring and Evaluation Framework\n\nConducted (either partly or wholly) in-house. In some circumstances, it may be useful to undertake a scoping or feasibility study to support this decision-making process. This form of assessment can foster greater understanding of what can and cannot be evaluated, and therefore what level of investment is required and can support the development of an appropriate evaluation design for large or complex evaluations.",
    "All evaluations, even those commissioned to an external contractor, will require significant internal input to ensure they are designed and delivered successfully. Several resources will need to be considered, as follows:",
    "- Financial resources: It is important to think about the budget and resources required for evaluation early, as part of the PIC business case. It should be noted that a substantial part of the evaluation costs can often be incurred after the intervention delivery has completed. It is not possible to give a fixed sum or proportion of budget for evaluation, as it will vary with the considerations above and the type of data required. Externally commissioned evaluation budgets can range from tens of thousands to millions of pounds depending on",
    "required. Externally commissioned evaluation budgets can range from tens of thousands to millions of pounds depending on the level of evidence and resource required. It is important to bear in mind that collection of new data is costly \u2013 particularly if large-scale survey and/or qualitative data collection are required.",
    "- Management and ownership: In keeping with the Aqua Book43 principles (the Aqua Book is the cross-Government guidance on quality analysis (QA)), the BEIS Evidence Framework is a QA approach with clear and explicit roles and responsibilities, where QA is continuous and proportionate and there is an explicit audit trail that records the evidence sources, review and clearance associated with evidence and analytical products.",
    "The Framework includes standard roles that are applied to any analytical activity in BEIS:",
    "Senior Responsible Officer (SRO), often pe budget holder, who holds overall accountability for pe success of pe project.\nThe Assurer, a senior analyst wip responsibility for sign-off analysis plans and clearing pe analysis outputs prior to submission to pe SRO.",
    "The Project Manager, pe day-to-day project manager is responsible for ensuring pe required project QA takes place. Evaluation managers need to have capabilities in four areas: scoping, leading and managing, mepods, and use and dissemination (see chapter 7 of pe Magenta Book for more information44).",
    "Peer Reviewers are independent analysts wipin BEIS who review pe analysis and use of evidence and make recommendations for improving pe analysis. In some cases, particularly for complex or controversial policies or evaluations, peer review is also advisable at pe design stage to ensure pe best quality evaluation design is being used and to demonstrate openness and transparency.\nPeer reviews by external evaluation experts are also required for all BEIS evaluations prior to publication, see 4.5.",
    "Project teams assign roles according to pe Evidence Framework. They will also follow QA procedures for peir team, Directorate or Group, as appropriate for pe project.",
    "43 https://www.gov.uk/government/publications/the-aqua-book-guidance-on-producing-quality-analysis-for-government\n\n44 https://www.gov.uk/government/publications/the-magenta-book\n\n# BEIS Monitoring and Evaluation Framework\n\nThe level of input required will be greatest at key points (in particular, the design and commissioning stages), but throughout monitoring and evaluation there is an ongoing resource requirement that should not be underestimated.",
    "Steering groups: Establishing a steering group for the evaluation will help ensure it is designed and managed to meet the requirements of relevant stakeholders and remains on track. A steering group will usually include the policy lead(s) and relevant team members, analyst(s) supporting the evaluation, the economist responsible for the project appraisal, key delivery partners, other government departments, etc.",
    "Steering groups are particularly important for large-scale evaluations, but this type of scrutiny and support is always useful, even for light-touch evaluations. Existing governance groups could be utilised in these cases.",
    "Delivery bodies: A successful evaluation will depend on the engagement and cooperation of those organisations and individuals involved in delivery of the programme, in many cases they will be the face of the policy and will have huge impact on the data quality and its usefulness for monitoring and evaluation. It will be important to work with delivery stakeholders to ensure they are aware of, and signed up to: what the monitoring and evaluation seeks to address, how, and what input will be required from them; and how they can potentially",
    "seeks to address, how, and what input will be required from them; and how they can potentially benefit from the findings, for example, by sharing data or taking part in interviews. As part of the monitoring and evaluation plan, you should consider the burden being put on stakeholders, and how this can be minimised.",
    "# Conduct or commission the evaluation\n\nAs highlighted above, the evaluation should be managed by a dedicated internal project manager and a monitoring and evaluation plan should be included in a PIC business case and a summary included in a regulatory impact assessment.\n\nThe necessary steps for an externally commissioned evaluation are as follows:",
    "Define pe terms of reference and establish pe steering group\nObtain procurement and Research Committee approval \u2013 pis is required for all commissioned evaluations, see 4.2.1\nWrite pe evaluation specification: pis should set out pe background and objectives to pe policy; clarify pe evaluation scope and questions; and indicate pe size, scale and timing and deliverables for pe evaluation, agreed by pe programme lead, analysts and oper key stakeholders",
    "Commission (see chapter 5 of pe Magenta Book45) pe evaluation via a competitive tendering exercise\nInception and ongoing management: an inception meeting wip pe contractor and steering group to clarify pe scope of pe work, timings, expectations for outputs and arrangements for ongoing management. Steering group meetings should be scheduled at appropriate decision and reporting points for pe evaluation. The process for regular progress reporting should also be agreed.",
    "Magenta Book\n\n# BEIS Monitoring and Evaluation Framework\n\nAn internally conducted evaluation should follow similar steps of having defined terms of reference and a project specification.\n\n# Use and publish the evaluation findings\n\nAt the time of planning an evaluation it is a good idea to give some thought to how the findings will be used and disseminated. BEIS expects all our evaluations to adhere to the Government Social Research Protocol, which has five principles:",
    "- Principle 1. The products of government social research and analysis (which includes evaluation) will be made publicly available\n- Principle 2. There will be prompt release of all government social research and analysis\n- Principle 3. Government social research and analysis must be released in a way that promotes public trust (findings should not be influenced by political concerns)",
    "- Principle 4. Clear communication plans should be developed for all social research and analysis produced by government (departments should publicly announce what research projects have been commissioned and publish high-level information regarding those projects)\n- Principle 5. Responsibility for the release of social research and analysis produced by government must be clear (in BEIS\u2019 case that would be the Government Social Research Heads of Profession)",
    "As outlined in the following chapter BEIS requires all externally commissioned evaluation reports to be sent for independent peer review prior to publication, and reviewers\u2019 comments to be addressed.",
    "As well as publication, other ways of disseminating and using the evaluation evidence should also be considered early and reviewed regularly by the steering group, such as identifying the best communication channels, and outputs, to reach users. One-page summaries, video outputs, infographics and engaging tools can be considered to help key messages reach the right audience. The format of outputs should be agreed with all evaluation stakeholders, inform delivery and key decision points and feed into new policy development.",
    "Evaluation project closure procedures facilitate the use of findings:\n\n- all quantitative evaluation data should be appropriately anonymised and submitted to the national Archive to facilitate further analysis\n- all relevant materials should be submitted to BEIS\u2019 internal online database of key analytical documents\n- BEIS evaluation reports should be published on gov.uk or devtracker\n- external expert peer review comments that are not addressed should be published\n- evaluation outputs need to be easily accessible for policy makers and available when they can influence decisions",
    "Sources: Government Social Research Publication Protocols, Devtracker\n\n# BEIS Monitoring and Evaluation Framework",
    "\u2022 every report should have a transparent technical annex, so findings could be replicated Designing and conducting an evaluation after an intervention has been implemented does happen, but there are risks (a robust evaluation might not be possible) and the quality of the evidence is likely to be poorer. It will be necessary in this situation to identify what relevant information is available, e.g. data collected as part of ongoing performance monitoring and/or administrative data. However, this often misses baseline data, collected before the intervention was",
    "performance monitoring and/or administrative data. However, this often misses baseline data, collected before the intervention was implemented, or data collected for a comparison or control group. This will result in a less robust assessment of the policy being possible, and key questions, such as \u2018would the outcomes have happened anyway?\u2019 being unanswerable. Nonetheless, it will normally be possible to undertake some assessment of the delivery process and immediate outputs of an intervention if the evaluation is planned and undertaken after policy implementation.",
    "For further information in the use and dissemination of evaluation findings see chapter 6 of the Magenta Book.48\n\n48 https://www.gov.uk/government/publications/the-magenta-book\n\n30\n\n# BEIS Monitoring and Evaluation Framework\n\n4. Achieving the BEIS monitoring and evaluation vision",
    "# BEIS Monitoring and Evaluation Framework\n\n4. Achieving the BEIS monitoring and evaluation vision\n\nBEIS is a diverse department with a wide range of policy areas and responsibilities, many of which are implemented through delivery partners. We need to build on our successes so that best practice in monitoring and evaluation is extended across all our work to influence policy implementation and design.",
    "Earlier chapters set out the steps required to achieve effective monitoring and evaluation. This is supported by a range of governance processes outlined below, including using the BEIS Peer Review Group to provide independent, transparent quality assurance of findings. We are also continuing to build capacity and capability amongst policy and analytical colleagues, which will contribute towards a positive learning culture. These are discussed in turn below.\n\n# 4.1 Establish comprehensive monitoring and evaluation coverage",
    "# 4.1 Establish comprehensive monitoring and evaluation coverage\n\nAs stated previously well-developed monitoring and evaluation are important for learning and accountability. Monitoring and evaluation learning should inform and influence better policy implementation and decision-making across the department to achieve its objectives.\n\n# 4.1.1 Requiring monitoring and evaluation plans",
    "# 4.1.1 Requiring monitoring and evaluation plans\n\nBEIS is committed to investing in monitoring and evaluation as tools for assessing and improving current and future policies across our policy areas. We aim to provide an open and transparent view of monitoring and evaluation coverage of BEIS interventions and support good quality evaluations.",
    "BEIS requires monitoring and evaluation plans in all its PIC business cases for interventions that meet the criteria for the Departmental Portfolio and are considered by PIC and a more detailed PIR review plan to be completed for all high impact regulatory impact assessments in addition to the summary plan included in the impact assessment.\n\n# 4.1.2 Encouraging best practice in our partner organisations",
    "BEIS is a ministerial department supported by 41 partner organisations (POs). These range from large organisations such as UK Research and Innovation and the Nuclear Decommissioning Authority, through medium-size organisations working on regulation such as the Competition and Markets Authority, to advisory committees including the Committee for Climate Change. A significant proportion of BEIS\u2019 expenditure is through POs, for which BEIS is ultimately accountable to Parliament.",
    "Source: https://www.gov.uk/government/organisations#department-for-business-energy-and-industrial-strategy\n\n# BEIS Monitoring and Evaluation Framework\n\nPOs are responsible for the monitoring and evaluation of the policy areas which they deliver. Given the importance of POs in the delivery of our policies and investment, the department encourages best practice monitoring and evaluation in our partner organisations.\n\nTo do this, the department:",
    "To do this, the department:\n\n- Encourages POs to work together with policy teams in BEIS to produce monitoring and evaluation plans, before policy implementation, so that good quality evaluation evidence is possible, and the supporting data can be successfully collected\n- Shares best practice and quality standards with POs\n- Reviews and collates information about POs\u2019 monitoring and evaluation coverage and quality\n\n# Embed monitoring and evaluation into governance processes",
    "# Embed monitoring and evaluation into governance processes\n\nTo ensure proportionate monitoring and evaluation is delivered, even in challenging circumstances, BEIS needs to firmly embed monitoring and evaluation in governance processes. This section outlines BEIS\u2019 current governance processes for monitoring and evaluation, which will be reviewed and strengthened.\n\n# Evaluation governance\n\nThere are several governance processes that apply to the commissioning and implementation of research and evaluation projects in BEIS.",
    "|Project scoping|Business Case|Procurement|Research Design|Data Collection|Analysis|Reporting & Dissemination|Publication|\n|||||||||\n|BEIS Research Committee|Steering Groups|Evaluation|GSR Publication|Protocol| | | |",
    "A scoping exercise initiates the commissioning of research (including evaluation), and a Project Initiation Document (PID) is developed as a product of the exercise for senior stakeholders/owners to give direction on whether the project should go ahead.\n\nResearch and evaluation projects with a budget of over \u00a310k are required to seek procurement and BEIS Research Committee approval before they can be procured. Those below this amount follow their policy team approval procedures.\n\n# BEIS Monitoring and Evaluation Framework",
    "# BEIS Monitoring and Evaluation Framework\n\nThe BEIS Research Committee is made up of senior analysts who quality assure and approve the commissioning of new research and consultancy in core BEIS, including evaluations, by reviewing research business cases. The Committee reviews the rationale for the research, how it fits with the existing evidence base, how the findings will be used and learning shared, the proposed methodology, risk management and quality assurance.",
    "Approved research and evaluation projects should inform project and policy development and implementation. The findings from these projects should be included as evidence in investment business cases and impact assessments.\n\nDuring a project policy teams will follow their own quality assurance procedures in line with the BEIS Evidence Framework. They will also convene steering groups at key stages during the research to quality assure and influence strategic decisions. Further detail on both these measures can be found in 3.2.3.8.",
    "Prior to publication all evaluation reports are submitted for external peer review by evaluation experts. Reviewers\u2019 comments are then addressed ahead of publication, see 4.5 for more information.\n\nFinally, all BEIS research and evaluation projects are expected to adhere to the Government Social Research (GSR) Publication Protocol as outlined in 3.2.3.10.\n\n# Investment governance",
    "# Investment governance\n\nThe BEIS Projects and Investments Committee (PIC) scrutinises and approves significant, risky or contentious investments of taxpayer funds undertaken by the department. If PIC approval is not required, the Business Case should follow the Director General Group\u2019s procedure for spending approval.\n\n|Business Case Development|Keyholder review and agreement|Project Investment Committee (PIC) Approval|\n||||\n|PIC approval not required|Follow Director General Group\u2019s procedure for spending approval| |",
    "If PIC approval is required, the first stage is the keyholder stage where experts from across the department review business cases to assess whether they provide PIC with enough information to make an informed decision. A monitoring and evaluation plan is part of the management case in a business case. P&R provides assurance on the performance and delivery of BEIS\u2019s major programmes and the overall portfolio of BEIS projects.",
    "Operational researchers, statisticians, social researchers, economists, scientists, engineers and policy leads depending on topic and specialism.\n\nProjects with Whole Life Cost over \u00a320m (or other delegated authority where it applies).\n\n# BEIS Monitoring and Evaluation Framework",
    "# BEIS Monitoring and Evaluation Framework\n\nDepartments and partner bodies are required to produce impact assessments (IAs) assessing the costs and benefits of regulatory changes prior to consultation, enactment, and implementation. The evidence and analysis used within IAs are scrutinised by the independent Regulatory Policy Committee (RPC).\n\nAn impact assessment summarises the rationale for government regulatory intervention, objectives, the options considered and the expected costs and benefits. This information is crucial to inform policy decisions.",
    "In 2011 it became mandatory to include a review clause in any legislation that regulates business and civil society organisations (referred to as \u2018business\u2019) except where the effect is deregulatory or the net costs or benefits to business are less than or equal to \u00a31 million in any given year (in 2017, this was increased to \u00a35 million). This was part of the Government\u2019s commitment to reduce both the stock of existing regulation and the flow of new regulation.",
    "Review clauses impose a statutory duty to carry out a review of the relevant legislation within a specified timescale. In most cases, the date of publication of the statutory review is no later than five years from the date the legislation came into force, and subsequent reviews should take place at five-year intervals, at most.",
    "The level of evidence required for both IAs and PIRs is based on the scale of impact, contextual factors, and existing evidence base. Those with a high impact require Central Analysis on behalf of or BEIS Director of Analysis sign-off, prior to ministerial sign-off.",
    "|Figure 10: Regulatory governance| | |\n||||\n|IA/PIR and Clearance Statement Development|Submit for Central Analysis Team review|Submit for Chief Analyst sign-off|\n|Chief Analyst sign-off not required|Follow policy team analytical sign-off procedures| |\n|Submit for Special Advisor and Ministerial sign-off|Submit for Regulatory Policy Committee opinion|Submit for cabinet write-round|",
    "Further information on PIC and P&R can be found in 3.2.1. The evidence in PIC business cases should build on previous monitoring and evaluation learning.\n\nFurther information on post-implementation reviews (PIRs) can be found in 3.2.2.\n\n52 An independent body sponsored by BEIS which assesses the impact on business of new regulatory and deregulatory proposals: https://www.gov.uk/government/organisations/regulatory-policy-committee/about",
    "53 https://www.gov.uk/government/collections/impact-assessments-guidance-for-government-departments\n\n54 https://www.gov.uk/government/publications/better-regulation-framework\n\n55 an equivalent annual net direct cost to business (EANDCB) or Net Present Value (NPV) of greater than +/- \u00a350 million",
    "56 an equivalent annual net direct cost to business (EANDCB) or Net Present Value (NPV) of greater than +/- \u00a3150 million, or that are high profile, novel or risky approaches, based on untested assumptions, or where learning will directly inform future policy making.\n\n# BEIS Monitoring and Evaluation Framework",
    "# BEIS Monitoring and Evaluation Framework\n\npublication57. Those that do not require this level of sign off follow their policy team analytical sign off procedures. The evidence and analysis used within PIRs over a certain size58 are also scrutinised by the independent Regulatory Policy Committee (RPC).",
    "Final versions of IAs and PIRs are published on legislation.gov.uk, alongside the relevant IA(s) and any59 RPC opinions on the quality of evidence. PIRs should also be published on gov.uk.\n\n# Build policy and analytical capacity and capability\n\nTo facilitate coverage and embed learning that informs and influences better policy implementation and decision-making across the department, BEIS takes several measures to build the monitoring and evaluation capacity and capability amongst its policy officials and analysts.",
    "BEIS offers internal monitoring and evaluation training for policy colleagues.\n\nBEIS also takes several measures to increase analytical evaluation capacity and capability:",
    "- Evaluation training is part of core training for BEIS analysts to ensure our analysts have the skills required to deliver evaluations\n- BEIS has an internal Evaluation Practitioners Network (EPN). The network meets monthly to support the delivery of good quality evaluation in BEIS through shared knowledge and learning",
    "- BEIS has an External Peer Review Group (PRG) comprised of independent evaluation experts who review our reports prior to publication and can be consulted at key stages of evaluation projects, especially to review evaluation designs. Their comments increase the capability of individual project managers and the wider analytical community, as they are stored centrally and accessible to the wider department, see 4.5 for more information",
    "To share knowledge and learning BEIS sits on cross-government groups:",
    "- The Cross-Government Evaluation Group (CGEG) aims to improve the supply of, stimulate demand for, and encourage the use of, good quality evaluation evidence in government. The purpose of this is to improve policy development, delivery and accountability across government. CGEG is a cross-departmental, cross-disciplinary group, with representation from most major departments\n- The Cross-Government Monitoring, Evaluation and Learning Official Development Assistance (ODA) Group is a knowledge exchange with monitoring and evaluation",
    "# BEIS Monitoring and Evaluation Framework\n\nAdvisers across government working on ODA \u2013 an overseas aid budget to support and deliver the government\u2019s 2015 Aid Strategy in developing countries.\n\n# 4.4 Facilitate a positive learning culture",
    "# 4.4 Facilitate a positive learning culture\n\nThe effectiveness of our policies depends on a strong and safe culture for monitoring and evaluation, where timely and accurate feedback and analysis assess what effect the intervention has had, how, why, and for whom, and this learning is fed back rapidly into policy decisions. Learning and feedback are therefore pivotal elements of a successful monitoring and evaluation framework.",
    "An important part of this learning process is to acknowledge that when policies do not deliver the desired effects \u2013 indeed, even when they produce unexpected or unwanted effects \u2013 these are still valuable opportunities to develop our knowledge, so we can refine and adapt future policies and eventually secure better outcomes. BEIS must enable this flow of information as far as is possible. This is about reinforcing existing good practice to make the learning process integral to the way we work in BEIS.",
    "Our aim is to increase the number of evaluations that inform and influence better policy delivery and decision-making across our policy areas. This is facilitated by an internal online database of key analytical products from the policy cycle which are stored centrally, in real-time, coded, and searchable, creating links across policy areas and between appraisal and evaluation.\n\n# 4.5 Maintain independent, transparent quality assurance of findings",
    "# 4.5 Maintain independent, transparent quality assurance of findings\n\nThe BEIS External Peer Review Group (PRG) helps BEIS maintain independent, transparent quality assurance of findings. The group is comprised of independent evaluation experts. BEIS requires all evaluations to be sent for independent peer review prior to publication, and reviewers\u2019 comments to be addressed.",
    "Two peer reviewers with expertise in the relevant policy area and evaluation methodologies are invited to comment on evaluation reports prior to publication and evaluation teams work with them to address their comments in the published version of the report. Where all comments are not addressed a summary of the review should be published alongside the report.",
    "BEIS also encourages teams to consult the PRG at key stages of the evaluation projects, especially at the earliest stages of scoping and design. This independent scrutiny provides assurance of quality and increases the credibility of our work.\n\nPublications also include detailed and transparent technical annexes, with data published, where possible, to allow further independent scrutiny of BEIS evaluations by external experts.",
    "BEIS regularly reviews and refreshes the PRG, by direct invitation from the department, to ensure appropriate capacity levels and give policymakers access to a broader range of expert advice.\n\n# BEIS Monitoring and Evaluation Framework\n\n5. Next steps\n\nBEIS recognises the value of monitoring and evaluation. While there are several examples of good quality assessments in the department, BEIS aims to expand this across our policy areas to inform and influence better decision-making within the department to achieve its objectives.",
    "This will be achieved by further embedding monitoring and evaluation into governance processes, building policy and analytical capacity and capability, and facilitating a positive learning culture. We will also continue to develop and maintain BEIS\u2019 central database: an internal online database of key analytical products from the policy cycle which are stored centrally, in real-time, coded, and searchable, creating links across policy areas and between appraisal and evaluation.",
    "To ensure the departmental approach to monitoring and evaluation is open and transparent we maintain an expert external peer review system and aim to publish key investment monitoring and evaluation activity.",
    "We recognise that it will take time to deliver the Framework in full. This will require consistent demand and expectation for monitoring and evaluation evidence from senior managers and decision makers, adequate resourcing and evaluation capability from our policy makers and analysts, and effective collaboration with our partner organisations. We will need to commit the necessary resources, including administrative resource in both central and policy teams to support this expansion.\n\n37\n\n# BEIS Monitoring and Evaluation Framework Appendices",
    "37\n\n# BEIS Monitoring and Evaluation Framework Appendices\n\n- A1 Glossary of terms\n- A2 BEIS evaluation case studies\n- A3 Theory of Change template\n- A4 Logframe template\n- A5 Further information on additional data requirements and linking\n- A6 Regulatory post implementation review plan template\n\n38\n\n# BEIS Monitoring and Evaluation Framework",
    "|Term|Definition|\n|||\n|Activity|What actions must be taken, and by whom, to produce the required outputs \u2013 e.g. what is done by staff to achieve the policy objectives, such as applicant support|",
    "|Analyst|Analyst is a blanket term used to describe several analytical professions that exist within government. Analysts provide comprehensive advice, challenge and insight based on evidence to help meet government objectives. There are four analytical professions in BEIS that commission, manage and quality assure monitoring and evaluation projects: 1) social researchers use the methods of social scientific enquiry to understand how people and businesses think and behave 2) economists assess the costs, benefits and risks of alternative ways to meet government objectives",
    "and behave 2) economists assess the costs, benefits and risks of alternative ways to meet government objectives 3) statisticians collect, quality assure, analyse and publish data to help government, business and the public make informed decisions 4) operational researchers look closely at complex systems and develop models that predict the way they behave.|",
    "|Appraisal|Appraisal is the process of assessing the costs, benefits, and risks of alternative ways to meet government objectives.|\n|Baseline data collection|Measurement of conditions prior to the implementation of a policy against which subsequent progress can be assessed, such as behaviors, attitudes, employment, turnover, emissions.|\n|BEIS|Department for Business, Energy & Industrial Strategy|",
    "|BEIS|Department for Business, Energy & Industrial Strategy|\n|BEIS External Peer Review Group (PRG)|A group of independent evaluation experts who review BEIS evaluation reports prior to publication and can be called upon to review the design of proposed evaluations.|\n|Benefit|A measurable positive outcome for one or more stakeholders that is linked to a strategic objective such as an increase in energy security.|",
    "|Benefits management|A program management approach that aims to make sure the desired business change or policy outcomes have been clearly defined, are measurable and provide a compelling case for investment.|",
    "|Benefits map|A logic map, in the form of a visual diagram, used in benefits management to link all the drivers, enablers and business change to the benefits from a project/policy change, and linking benefits to objectives and strategic goals. A benefits map is similar to a Theory of Change used in evaluation.|",
    "|CBA|Cost-Benefit Analysis estimates whether the benefits of a policy outweigh its costs, and by how much relative to other alternatives \u2013 usually in comparison to what would have happened without the intervention.|\n|Comparison or control group|A group alike to the treatment group in every way.|",
    "|Comparison or control group|A group alike to the treatment group in every way.|\n|Counterfactual|The counterfactual scenario is what would have occurred in the absence of the policy. The counterfactual scenario cannot be observed, because it is defined as what did not happen. So, the challenge of impact measurement is to find some way to reconstruct what would have occurred in the absence of a program, so we can compare those two scenarios, and determine the true impact.|",
    "|Cost-effectiveness|Social cost-effectiveness analysis compares the costs of alternative ways of producing the same or similar outputs.|\n|Data collection|The collection of information to use in monitoring or evaluation; this can be quantitative or qualitative.|\n|Evaluation|Evaluation is the systematic assessment of the design, implementation and outcomes of an intervention.|",
    "# BEIS Monitoring and Evaluation Framework",
    "|Evaluation approach|The way that the answering of evaluation questions is approached; for example, impact evaluations may use a theory-based approach and/or an experimental approach.|\n|||\n|Evaluation design|The overarching design of the whole evaluation, which includes how the evaluation will meet the learning aims specified.|",
    "|Evaluation methods|The way that information is collected and analysed in order to test theories and answer the evaluation questions (e.g. difference in difference, modelling, randomised control trials).|\n|Evaluation questions|Evaluation questions are high-level questions that an evaluation is designed to answer.|\n|Evaluation types|The types of evaluation are defined by the evaluation questions. Common types of evaluation include process, impact, and value for money.|",
    "|Experimental evaluation|These evaluation designs are referred to as randomised control trials (RCTs). Treatment and control groups are randomly assigned. The aim is that the only difference between the treatment and control groups on average is that the control group does not receive the intervention.|",
    "|Impact|The ultimate result to which the policy contributes. Impact is defined as a change in social, environmental, or economic outcomes (positive or negative) that is directly attributable to an intervention, such as reduced energy costs or change in employment.|",
    "|Indicators|What you might want to know or expect to see to indicate you were on track to achieve your benefits and impacts, such as number of applications, number of firms introducing new/improved products and services, number of smart meters in homes and small businesses across Britain, or % of people with smart meters who say they\u2019ve taken steps to reduce their energy use.|\n|Input|What the policy is expected to provide such as grant funding or equipment.|",
    "|Input|What the policy is expected to provide such as grant funding or equipment.|\n|Intervention|Anything intended to elicit change, including a programme, policy, project, regulation, and changes in delivery method, such as activities to mitigate climate change, set a minimum wage or funding to stimulate productivity in the UK economy.|\n|Monitoring|Monitoring is the ongoing collection and analysis of data (specified indicators) about an intervention to understand progress against its objectives.|",
    "|Net Present Value (NPV)|The sum of future streams of net benefits. These are discounted to bring them into today\u2019s value using the standard rate agreed by the civil service.|\n|Outcome|An outcome is any social, environmental, or economic effect that a policy is interested in maintaining or improving in some way. For example, labour force participation or accumulation of knowledge and skills.|",
    "|Output|What is expected to happen as a direct result of the intervention activities, such as the number of projects supported.|\n|Performance and Risk Committee (P&R)|The Performance and Risk Committee (P&R) is a delegated Committee of the BEIS Executive Committee (ExCo), which provides assurance for ExCo and the Permanent Secretary on the performance and delivery of BEIS\u2019 programme and policy commitments, as well as its Partner Organisations.|",
    "|Policy|A government policy is an objective or course of action planned by the Government on a particular subject. Policies are usually developed by a Government department, such as BEIS, to achieve their objectives.|\n|Policy objectives|How the policy is supposed to affect its various target outcomes.|\n|Policy Official|The Policy Profession designs, develops, and proposes appropriate courses of action to help meet key government priorities and ministerial objectives.|",
    "|Portfolio|A portfolio comprises part or all of an organisation\u2019s investment required to achieve its objectives. Governed through its portfolio (or business) plan, a|",
    "# BEIS Monitoring and Evaluation Framework",
    "|Process|Process evaluations tend to examine activities involved in an intervention\u2019s implementation and the pathways by which the policy was delivered. They will often cover subjective issues (such as perceptions of how well a policy has operated) and objective issues (the factual details of how an intervention has operated, typically using administrative data, where available).|\n|||",
    "|||\n|Programme|A programme is a temporary, flexible organisation created to co-ordinate, direct and oversee the implementation of a set of projects and other work components to deliver outcomes and benefits related to a set of strategic objectives. Programmes can be undertaken in one or more tranches (phases), each of which is structured around distinct step changes in capability and benefit realisation.|",
    "|Project|A project is a temporary management environment, undertaken in stages, created for the purpose of delivering one or more business products or outcomes. A project might be standalone within a portfolio or part of a programme.|",
    "|Project Investment Committee (PIC)|The BEIS Projects and Investments Committee (PIC) scrutinises and approves significant (with a whole life cost of \u00a320m or above either to BEIS or the economy as a whole), risky or contentious investments of taxpayer funds undertaken by the department.|",
    "|Quantitative data collection|Quantitative data collection generates numerical data or data that can be transformed into usable statistics. It is used to quantify attitudes, opinions, behaviours, and other defined variables \u2013 and generalise results for a particular population.|",
    "|Qualitative data collection|Qualitative data collection is primarily exploratory. It is used to gain an understanding of underlying reasons, opinions, and motivations. It provides insights into the problem or helps to develop ideas or hypotheses for potential quantitative research. Qualitative Research is also used to uncover trends in thought and opinions, and dive deeper into a topic, opinion or experience.|",
    "|Quasi-experimental evaluation|Quasi-experimental evaluation involves examining the impact of an intervention by taking measurements before and after it is implemented, in a similar way to experimental evaluation. However quasi-experimental evaluations do not randomly assign groups to the treatment and comparison group. Evidence is collected to ascertain whether any change that has occurred can be attributed to the intervention or other causes, controlling for variables that influence the outcome.|\n|Regulation|The ongoing processes of monitoring and enforcing the law.|",
    "|Regulation|The ongoing processes of monitoring and enforcing the law.|\n|Regulatory Policy Committee (RPC)|RPC is an advisory non-departmental public body, sponsored by the Department for Business, Energy & Industrial Strategy.|",
    "|Regulatory post implementation review|A PIR is a process to review a regulation or policy decision after it has been implemented and operational for a period of time. A PIR should assess if the objectives of the regulation have been achieved, if the objectives are still relevant and if they could be achieved in a less burdensome way.|",
    "|Research Committee|The BEIS Research Committee is made up of senior analysts who quality assure and approve the commissioning of new research and consultancy in core BEIS, including evaluations, by reviewing research business cases.|",
    "Sources: Project Delivery Functional Standard\n\n# BEIS Monitoring and Evaluation Framework",
    "|SMART|Specific, Measurable, Achievable, Realistic, Time-bound.|\n|||\n|Single Departmental Plan|This sets out the department's objectives and how they will be achieved.|\n|Spending Review|Spending reviews take place every two to five years. They set departmental budgets for three to five years ahead and shape the scale and nature of public service programmes and public investment.|",
    "|The Aqua Book|HM Treasury guidance on producing quality analysis for government.|\n|The Green Book|HM Treasury guidance on how to appraise and evaluate policies, programmes, and projects.|\n|The Magenta Book|HM Treasury guidance on what to consider when designing an evaluation.|",
    "|The Magenta Book|HM Treasury guidance on what to consider when designing an evaluation.|\n|Theory of Change|Theory of Change is essentially a comprehensive description and illustration of how and why a desired change is expected to happen in a particular context. It is focused on mapping out the \"missing middle\" between a programme or change initiative's activities or interventions and how these lead to desired goals being achieved.|",
    "|Theory-based evaluation|Theory-based approaches to evaluation use an explicit theory of change to draw conclusions about whether and how an intervention contributed to observed results.|\n|Treatment group|The group that participates in the policy, also referred to as the test group.|",
    "|Value for Money evaluation|Value-for-money evaluation methods compare benefits to the costs of interventions, including adverse and unintended aspects. Two widely used methods are social cost-effectiveness analysis and social cost-benefit analysis, both of which allow for comparison of two or more alternative options (interventions).|",
    "Sources:\n\n63 Department for Business, Energy and Industrial Strategy Single Departmental Plan\n\n64 The Aqua Book - Guidance on producing quality analysis for government\n\n65 The Green Book - Appraisal and evaluation in central government\n\n66 The Magenta Book\n\n# BEIS Monitoring and Evaluation Framework\n\nA2. BEIS evaluation case studies\n\n# Process evaluation approaches used in BEIS",
    "A2. BEIS evaluation case studies\n\n# Process evaluation approaches used in BEIS\n\nProcess evaluations employ a variety of methods used in the social sciences, normally including both qualitative and quantitative methods. The purpose of a process evaluation is to explain how an intervention generates outcomes or effects. Data collection and analysis in process evaluations are structured around Theories of Change which illustrate the causal pathways thought to be operating in the intervention. These causal pathways are the \u2018processes\u2019 of process evaluation.",
    "In BEIS process evaluations are conducted to understand how the effect of an intervention is achieved - how an intervention operates to produce outcomes (what has been achieved). This includes:\n\n- knowing which aspects of an intervention are important\n- how different aspects of an intervention work together\n- how an intervention can be implemented in a given context",
    "BEIS process evaluations illuminate the mechanisms through which an intervention produces change. This is particularly important if the department aims to roll out an intervention more widely in the future. Findings from a process evaluation can help implementation and adaptation of the intervention, as necessary, to other populations and contexts. A process evaluation can also explain why an intervention failed and indicate how it might be improved.\n\n# Process evaluation case study: The Newton Fund67",
    "# Process evaluation case study: The Newton Fund67\n\nBEIS delivers some of the UK government\u2019s Official Development Assistance (ODA). One of these is The Newton Fund68 a \u00a3735 million UK investment which aims to support cutting-edge research that addresses the challenges faced by developing countries.",
    "BEIS give allocations to 17 Delivery Partners (UK Research and Innovation, UK Space Agency, the Met Office, British Council, Academies etc.) These Delivery Partners\u2019 primary funding mechanism is through research grants to research institutions.",
    "A process evaluation was conducted in the latter half of 2017. This allowed enough time for BEIS and stakeholders delivering the Fund to learn lessons and make improvements during the remainder of the Fund\u2019s lifetime (until 2021) as well as informing the development of similar programme approaches and other interventions in the future, for example the Global Challenges Research Fund (GCRF).69",
    "The methodologies included a review of programme documents (for example, business case, meeting minutes, progress reports, country strategies, contracts, and procurement), which was used in interviews with key stakeholders (10 BEIS, 15 in-country teams, 15 UK delivery partners, 8 overseas partners). The data was systematically coded, reviewed and triangulated.\n\n67 https://www.newtonfund.ac.uk/about/newton-fund-evaluation/",
    "67 https://www.newtonfund.ac.uk/about/newton-fund-evaluation/\n\n68 For more information see: https://www.newtonfund.ac.uk/\n\n69 https://www.ukri.org/research/global-challenges-research-fund/\n\n# BEIS Monitoring and Evaluation Framework",
    "# BEIS Monitoring and Evaluation Framework\n\nThe evaluation found that since the initial roll out, the Newton Fund has undergone important developments in terms of how it is run, as well as fine-tuning of its objectives. There is evidence that the Newton Fund supports partnerships that promote economic development and welfare, and that the flexibility around 'match' is one of the key successes.\n\nRecommendations were to:",
    "Recommendations were to:\n\n1. publish a single strategy document to set direction and establish priorities, simplify decision-making, and drive alignment\n2. formally document processes and responsibilities, as they are currently understood but not documented leaving scope for inconsistency and inefficiency in delivery in an already highly complex programme\n3. maintain momentum to develop a new centralised, semi-automated reporting tool which is critical to more efficient portfolio management, better financial reporting, and transparency",
    "BEIS is addressing these by working on strategy documents, recording processes and responsibilities, and delivering on the reporting tool.\n\n# Impact evaluation approaches used in BEIS\n\nTheory-based evaluation\n\nTheory-based approaches to evaluation use an explicit Theory of Change to draw conclusions about whether and how an intervention contributed to observed impacts and often considers the context at the time that the intervention is being implemented.",
    "Theory-based methods tend to be particularly suited for the evaluation of complex interventions or simple interventions in complex environments. In these situations, where determining the effect size can often be difficult, theory-based methods can confirm whether an intervention had an effect in the desired direction. They can also explain why an intervention worked, or not, and inform translation to other populations, places or time periods.",
    "Theory-based evaluation case study: Transitional Arrangements (TA) for Demand Side Response (DSR) in the Capacity Market (CM)\n\nThe TA aimed to encourage the development of Demand Side Response (DSR) to balance supply and demand in a decarbonised electricity grid.\n\nIt is a requirement for partner countries and their funding agencies to match (in money, resources, such as facilities or equipment, or effort, i.e. labour) the contributions they are receiving from the UK",
    "Source: https://www.gov.uk/government/publications/the-magenta-book\n\nThe Capacity Market is a mechanism introduced by the Government to ensure that electricity supply continues to meet demand as more volatile and unpredictable renewable generation plants come on stream. It will ensure there is sufficient generation or load-management capacity in the system to cope with times of stress on the network when, for example, the wind stops blowing or there is a surge in demand.\n\n# BEIS Monitoring and Evaluation Framework",
    "# BEIS Monitoring and Evaluation Framework\n\nThe evaluation was designed to answer five high-level questions including: What outcomes can be attributed to the TA and were they as intended by BEIS? What outcomes occurred for whom and under what circumstances?",
    "The approach to the evaluation was realist and theory based. The realist approach emphasised the importance of understanding not only whether a policy contributes to outcomes (which may be intended or unintended) but how, for whom and in what circumstances. The development of a \u2018theory\u2019 of the TA was central to implementing a realist evaluation as it allowed the rigorous examination of the design and execution of the scheme, and test policy assumptions against available evidence. An initial theoretical framework was developed, setting out the",
    "scheme, and test policy assumptions against available evidence. An initial theoretical framework was developed, setting out the realist hypotheses to be tested.",
    "Evidence was gathered to test and revise the initial theoretical framework. This involved in-depth telephone interviews with representatives of organisations; an email survey on cost information; and case study research. The evaluation found that the first auction will contribute to involvement in other auctions as providers were able to build experience, customer bases and learn lessons about participating in the capacity market. Phase 2 of the evaluation found that a large amount of capacity dropped out after the auction due to difficulties signing up clients and difficulties complying with complex operational",
    "amount of capacity dropped out after the auction due to difficulties signing up clients and difficulties complying with complex operational tests. Findings from Phase 3 suggest that the second auction stimulated learning about demand reduction or shifting demand to another time period for some participants.",
    "# Experimental evaluation\n\nRandomised Control Trials (RCTs) use randomised access to interventions creating a treatment and control group. They compare outcomes between those two groups to give an indication of the impact of the policy. The control group acts as a proxy for the counterfactual.\n\nRCT evaluation case study: The Business Basics Fund",
    "RCT evaluation case study: The Business Basics Fund\n\nThere is wide variation in productivity across UK firms with small and medium sized businesses (SMEs) making up most of the least productive 10%. SMEs adopting proven technologies and management practices has the potential to add \u00a3100bn to the UK\u2019s gross value added (a measure of the value of goods and services produced).",
    "The Business Basics Fund provides grants to test the most effective ways of encouraging SMEs to adopt such technology and management practices. The Fund is run as an open competition, crowdsourcing ideas from businesses, business support delivery bodies, academia, trade bodies and other parties.\n\nReferences:",
    "References:\n\n- Evaluation of the Transitional Arrangements Phase 1\n- Evaluation of the Transitional Arrangements for Demand Side Response Phase 2\n- Evaluation of the Transitional Arrangements for Demand Side Response Phase 3\n- Business Basics Programme\n\n# BEIS Monitoring and Evaluation Framework",
    "# BEIS Monitoring and Evaluation Framework\n\nRobust evaluation is a key element of the Business Basics Programme. The UK is taking a lead in applying experimental methods to boosting productivity in its evaluation of the Business Basics Programme. To ensure that all projects are evaluated properly and can be compared with other business support activities, an overarching Evaluation Framework has been developed.",
    "The programme aim is to deliver randomised control trials testing the effectiveness of proposed approaches to increase adoption of basic technologies and management practices. The trials will assess whether there is a causal link between the approaches being tested and the desired outcomes. In the short-term they will measure behaviour change, intention to adopt, and where possible, adoption of identified technologies and management practices. Longer term impact evaluation can only take place in three to five years\u2019 time to allow productivity impacts to emerge and be measurable. The appropriate",
    "place in three to five years\u2019 time to allow productivity impacts to emerge and be measurable. The appropriate data collection and data sharing agreements have been put in place to enable the longer-term learning from the programme.",
    "With most of the projects still to complete it is too early to start answering the big policy questions set for the fund. However, valuable insights into programme delivery, experimental design, and qualitative evidence on what works to encourage adoption are already being gained, such as evidence that progress in the adoption process is not linear. These lessons have and will continue to be fed back to improve the delivery of the Programme.\n\nThe next Business Basics report will be produced upon completion of BBF1 projects.",
    "The next Business Basics report will be produced upon completion of BBF1 projects.\n\n# Quasi experimental evaluation",
    "Quasi experimental design takes a similar approach to experimental design but lacks random assignment. Instead of a control group it identifies a comparison group that is as similar as possible to the treatment group in terms of baseline (pre-intervention) characteristics. The comparison group captures what would have been the outcomes if the intervention had not been implemented (i.e. the counterfactual). Quasi experimental evaluation methods typically used by BEIS include matching methods, duration modelling, interrupted time series, instrumental variables, synthetic",
    "typically used by BEIS include matching methods, duration modelling, interrupted time series, instrumental variables, synthetic control, and difference-in-difference.",
    "Quasi-experimental case study: CRC Energy Efficiency Scheme (formerly Carbon Reduction Commitment)\n\nThe CRC was designed to drive energy efficiency and reduce carbon emissions in large non-intensive energy users, both public and private sector, across the UK. Collectively these are estimated to be responsible for around 10% of the UK\u2019s greenhouse gas emissions.",
    "The aim of the evaluation was to understand the actual impact of the CRC in driving action on energy use and to understand which elements of the policy have or have not worked and how this varied for different types of scheme participant.\n\nThe evaluation involved a quantitative survey, in-depth qualitative interviews and micro-econometric analysis, and these findings were drawn on for the synthesis report, along with:",
    "- Business Support Evaluation Framework\n- Business Basics Programme Progress Report October 2019\n- Evaluation of the CRC Energy Efficiency Scheme\n\n# BEIS Monitoring and Evaluation Framework",
    "# BEIS Monitoring and Evaluation Framework\n\nWith desk research. To assess the impact of the CRC on energy use and carbon emissions, it was important to explore what would have happened in the absence of the CRC. Therefore, the micro-econometric analysis used difference-in-difference to assess the difference in energy consumption between CRC and non-CRC comparison groups. The qualitative and survey research also used non-CRC comparison groups to support the analysis and allow triangulation of evidence.",
    "The evaluation found that while increasing energy costs were the single biggest driver for action on energy efficiency in recent years, there was evidence that the CRC had an impact on energy efficiency behavior and carbon emissions at least as sizeable as, if not greater than, the original impact assessment. Confidence in this conclusion was built because it was supported by all three research workstreams: micro-econometric, quantitative and qualitative.",
    "Not all CRC participants were significantly influenced by the scheme: some were already taking early action on energy efficiency before the CRC. The quantitative and qualitative research found that the CRC was reported to have less impact (because of early action) on organizations which were:\n\n1. relatively energy intensive\n2. sensitive to reputation\n3. sensitive to environmental factors\n4. larger scale (in terms of capacity to address energy management)",
    "The evaluation also found that the main drivers for energy efficiency action were raising awareness, especially at board level; the legal requirement for compliance; and through increasing the overall cost of energy. However, the influence of the CRC on reputation was less marked than its influence on finance or awareness.",
    "Impact evaluation design in BEIS should be appropriate to the design of the policies being evaluated, the evaluation questions being asked, and the resources available. For example, an RCT may be the best method where interventions are relatively simple, and the outcome is easily measurable.",
    "The role of qualitative research alongside experimental or quasi experimental methods While experimental and quasi experimental designs can derive estimates which robustly capture the causal effect of an intervention on its desired outcomes, they are to some extent \u2018black-box\u2019 in nature. They can attribute changes in outcomes to an intervention; often, however, these methods do not shed enough light on why or how the intervention had the reported impact.",
    "Qualitative methods can offer insight on the process of the intervention and the experience of the participants, and comparison group, which is useful for project design (adapting the design for a different context with different participants, in a different location, or in a different manner) and improvement. For example, if a robust quasi experimental assessment found that a matched funded business training program had no positive impact, or a negative impact on turnover or employment for participants, qualitative research can help uncover why this was the",
    "or a negative impact on turnover or employment for participants, qualitative research can help uncover why this was the case (e.g. participants may have felt wary about the quality of support they would receive, or that the funding they were required to contribute alongside the program funding was unaffordable). Similarly, if a positive impact is found, qualitative research can help to",
    "# BEIS Monitoring and Evaluation Framework\n\nUnderstand whether an intervention might have similar effects in future waves, based on contextual factors such as whether businesses want to grow, or have the expertise and investment required to do so.\n\nBEIS requires qualitative research to be conducted alongside all experimental and quasi-experimental methodologies to learn why and how different groups were affected, to improve delivery and inform future policy design.\n\n# Value for Money evaluation approaches used in BEIS",
    "# Value for Money evaluation approaches used in BEIS\n\nValue for Money evaluation aims to identify the value gained from resources used to implement an intervention.",
    "BEIS tends to use cost-effectiveness or cost-benefit analysis (CBA). The first compares the costs of alternative ways of producing the same or similar outputs. The second quantifies in monetary terms as many of the costs and benefits of an intervention as feasible. CBA estimates whether the benefits of a project or policy outweigh its costs, and by how much relative to other alternatives \u2013 usually in comparison to what would have happened without the intervention.",
    "# Value for money evaluation case study: Contracts for Difference (CfD)\n\nThe Contracts for Difference (CfD) scheme aims to give developers a higher level of confidence and certainty to invest in low carbon electricity generation, by agreeing to a fixed price for the sale of electricity over a 15-year contract.",
    "The evaluation required a mix of impact, process, and economic evaluation. The evaluation is theory-based, adopting principles of realist approaches to address questions around how differences in context influence how developers respond to the scheme. It combines qualitative interviews with scheme participants and non-participants, with quantitative data collection and analysis. A modelled counterfactual was developed to conduct economic cost-benefit analysis to address questions around whether the scheme presents good value for money.",
    "The publication of the evaluation and PIR are forthcoming.\n\n|BEIS Monitoring and Evaluation Framework|A3. Theory of Change template|\n|||\n|Outcomes or Impacts or End Benefits|BEIS Strategic Objectives|\n|Benefits|8|\n|Intermediate Benefits|Secondary benefits|\n|Main Tangible Output(s)| |\n|Behaviour Change| |\n|Barriers Overcome| |\n|Capabilities Achieved| |\n|Activities and Outputs|49|",
    "|BEIS Monitoring and Evaluation Framework|Level of Mechanism|Level of Evidence|Strengths|Weaknesses|\n||||||\n|Lovel|Mechanism|Evidence|Lovel|Mechanism|Evidence|Strengths|Weaknesses|Evidence|Strengths|Weaknesses|\n|Cayil|Mechanism|Evidence|Cuil|Mechanism|Evidence|Strengths|Weaknesses|Evidence|Strengths|Weaknesses|",
    "BEIS Monitoring and Evaluation Framework\nA4. Logical framework (logframe) template\n51\n\n# BEIS Monitoring and Evaluation Framework\n\nAs it says in the Magenta Book83 the collection of data required for an evaluation should be planned alongside the development of the intervention; where this does not occur, an evaluation may be impossible, severely limited or unnecessarily expensive.\n\nIn planning data collection, the following should be considered:",
    "In planning data collection, the following should be considered:\n\n- the evaluation questions to be answered;\n- who can provide the relevant data;\n- data access constraints.\n\nPlanning activities should include:",
    "- Identifying existing data from administrative and monitoring systems or larger-scale (long-term) surveys, to create a richer data set. This can enhance analysis, improve the quality of data, and avoid duplication of data collection. It combines different sets of data that have been collected for different purposes and can allow evaluators to answer complex questions in a cost-effective way. For this to be successful, accurate identifiers need to be collected to allow records to be paired. Unique identifiers can be straightforward to pair,",
    "identifiers need to be collected to allow records to be paired. Unique identifiers can be straightforward to pair, in their absence careful thought needs to be given to the matching protocols.",
    "- Identifying additional data requirements: considering whether accurate contact details for the individuals or organisations in the treatment and comparison or control groups (name, surname, landline number, mobile number, email address, etc.) are required in baseline and follow-up data collection or data linking; whether financial data relating to policy expenditure, or outcome related data will be required; and identifying who will collect it to ensure the appropriate systems are in place to do so, to the required standard, with the appropriate permissions for",
    "the appropriate systems are in place to do so, to the required standard, with the appropriate permissions for use.",
    "- Data protection and security requirements: ensuring data protection requirements are considered in the intervention and evaluation design so data can be shared, used, and stored appropriately.\n- Considering whether secondary data can be used to provide information about the wider context in which the policy is being delivered - e.g. national data on GDP, public attitudes, business performance, fuel use, prices, etc. - to help understand how external factors have affected outcomes, or monitor outcomes over time.",
    "If you plan to conduct data linking (such as HMRC data, the Business Structure Database, Inter-Departmental Business Register (IDBR), or industry data and commercial datasets such as FAME (Forecasting Analysis and Modelling Environment)) to analyse impacts on longer-term turnover or employment, the minimum data required would be:\n\nCompanies House Registration Number (CRN)\nCompany Name and trading name if different\nVAT Number\nPAYE Number",
    "83 https://www.gov.uk/government/publications/the-magenta-book\n\n# BEIS Monitoring and Evaluation Framework\n\nTo track recipients of BEIS business support over time you would also want to:\n\n- Unique Tax Reference Number\n- Postcode\n\nTo track recipients of BEIS business support over time you would also want to:",
    "- collect address, age of business, Sector (SIC) code, number of employees\n- collect treatment and comparison or control baseline data before the intervention begins, so you can measure impact of the intervention on the treatment group (on turnover, Gross Value Added (GVA), labour productivity, for example)\n- collect data in a timely manner to enable real-time feedback and improvement of the project delivery",
    "- collect data in a timely manner to enable real-time feedback and improvement of the project delivery\n- build in any data requirements from external organisations into contracting from the outset (such as format and frequency of reporting)\n- ensure ethical and data protection requirements are considered prior to data collection",
    "To track energy interventions over time to the National Energy Efficiency Data Framework84 or gas and electricity meter point data, you need to collect:\n\n- the full address, including postcode of the properties you wish to match85\n- the dates of the intervention so any potential effects can be identified.\n\nFor further information on data linking see section 4.6 in the Magenta Book86 and the how-to articles published as part of the data linking methods review87.",
    "84 https://www.gov.uk/government/collections/national-energy-efficiency-data-need-framework\n\n85 A scoping or feasibility study may help understanding of whether sufficient numbers of cases can be found in the data to undertake an impact evaluation. Low matching rates often indicate methods need to be adapted.\n\n86 https://www.gov.uk/government/publications/the-magenta-book",
    "86 https://www.gov.uk/government/publications/the-magenta-book\n\n87 https://www.gov.uk/government/publications/joined-up-data-in-government-the-future-of-data-linking-methods\n\n# BEIS Monitoring and Evaluation Framework\n\n# A6. Regulatory post implementation review plan template",
    "|Part A: Basic Information|\n||\n|Measure and link to legislation:|e.g. Right to request flexible working|\n|Policy description:|e.g. taken from IA|\n|Policy objectives:|e.g. taken from IA|\n|Net cost to business per year (\u00a3m)|Net Present Value (\u00a3million)|Total Cost (Present Value) (\u00a3million)|",
    "|e.g. \u00a3xm taken directly from IA|e.g. \u00a3xm taken directly from IA|e.g. \u00a3xm taken directly from IA|\n|Senior Responsible Officer:|e.g. SCS lead|\n|Lead officials:|e.g. G7 working level leads (analyst and policy)|\n|Statutory Review clause: Yes / No|Review Date: e.g. the deadline in legislation for statutory reviews|",
    "# Part B: PIR Plan\n\nReview objective:",
    "- What is the objective of the review and what will it cover (e.g. the aim of the PIR is normally to assess the effectiveness of a regulation after it has been implemented and operational for a period of time. Will the PIR cover the whole regulation, recent amendments or parts of the regulation)?\n- What are key milestones for the policy (using SMART targets) and when will you report against them?",
    "- What factors are critical to success (what are the outputs / outputs as set out in the \u2018Objectives\u2019 section of the IA) and will this information be collected before, during and after implementation?",
    "Baseline and Monitoring arrangements:\n\n- Describe the counterfactual position against which the impact of the change will be measured\n- Provide details on the data you are collecting, or expect to have available, that will feed into the post implementation review (e.g. what data sources will you be drawing on (surveying/sampling businesses, official statistics etc)?)\n- Outline your monitoring targets and reporting plan\n\n# BEIS Monitoring and Evaluation Framework\n\nReview approach and rationale:",
    "# BEIS Monitoring and Evaluation Framework\n\nReview approach and rationale:\n\n- Briefly describe your proposed methodology and planned interactions with stakeholders\n- Please give your reasons for using this approach\n- Expected intensity of the review (i.e. light-touch, impact evaluation, economic evaluation etc.). Page 15 of the PIR guidance will help inform this.\n- Justification for intensity of the review\n- Include details of the evaluation expert that has reviewed the design\n\n# Part C: Internal Information:",
    "# Part C: Internal Information:\n\nDirectorate / Partner organisation\n\nBudget and resource for monitoring and evaluation\n\n- If you have a budget, please give details and what it will be spent on (e.g. data collection, commissioning external research / evaluation).\n- What resource do you intend to dedicate to data collection, analysis and review, over what timescales?\n\nLegacy arrangements:",
    "Legacy arrangements:\n\n- Please provide details on your legacy arrangements to ensure evaluation can continue beyond the active phase of the policy \u2013 this often means having some resources devoted to evaluation even after the regulation has been implemented\n- When will formal work on the PIR begin (remember that the PIR will need to go through internal, RPC & Cabinet Clearances).\n\n# BEIS Monitoring and Evaluation Framework\n\n56",
    "# BEIS Monitoring and Evaluation Framework\n\n56\n\nThis publication is available from: www.gov.uk/government/publications/beis-monitoring-and-\nevaluation-framework\n\n\nIf you need a version of this document in a more accessible format, please email\nenquiries@beis.gov.uk. Please tell us what format you need. It will help us if you say what\nassistive technology you use.",
    "# Standards\n\nFood Agency\n\nfood.gov.uk\n\nEvaluation Action Plan\n\nJune 2022\n\nAuthor: Anna Cordes\n\nAnalytics Unit | Science, Evidence and Research Directorate\n\n# Evaluation Action Plan\n\nForeword\n\nExecutive Summary\n\n# 1. Introduction\n\n1.1 Monitoring and evaluation in the policy cycle\n\n1.2 This document: the FSA\u2019s Evaluation Action Plan\n\n1.3 Scope\n\n# 2. Our Vision for the Evaluation Action Plan",
    "1.3 Scope\n\n# 2. Our Vision for the Evaluation Action Plan\n\n2.1 Anticipated outcomes of the Evaluation Action Plan\n\n# 3. Supporting the Evaluation Action Plan\n\n3.1 Our Evaluation Approach\n\n3.2 Quality assurance\n\n3.3 Building evaluation skills and capability across the FSA\n\n3.4 Further raising the profile of evaluation\n\nNext Steps\n\n# Annex A: Guidance for when and how to evaluate\n\nEvaluation criteria\n\nEvaluation scale\n\nEvaluation type",
    "Evaluation criteria\n\nEvaluation scale\n\nEvaluation type\n\n# Annex B: Table of recommendations\n\n# Annex C: Current and Forthcoming Evaluation Activity\n\n# Annex D: How the FSA uses Evidence\n\n# Foreword\n\nThis is the Food Standards Agency\u2019s (FSA) first Evaluation Action Plan, which sets out the broad principles by which we will monitor and evaluate our work.",
    "The UK food system is rapidly changing; technology is transforming how food is produced, bought and sold, whilst economic, social and environmental pressures make accessing a healthy and sustainable diet challenging for many.",
    "The FSA\u2019s ability to deliver on its mission \u2013 food you can trust - relies on the generation and use of high quality evidence, including evaluation evidence. Evaluation highlights good practice, improves our performance and identifies approaches that work, why and for whom. Conducting thorough evaluations ensures the effective use of public funds and helps create regulation that is proportionate and fit for purpose.",
    "This Action Plan builds on the good practices that are already in place within the FSA and aligns with our existing commitment to be an evidence-led organisation. It also makes a number of recommendations that will strengthen the organisation\u2019s ability to generate and act on evaluation evidence, helping us navigate the complex challenges of the food system now and into the future.\n\nRobin May\n\nChief Scientific Adviser\n\n# Executive Summary",
    "Robin May\n\nChief Scientific Adviser\n\n# Executive Summary\n\nThe Food Standards Agency (FSA) is launching a new Evaluation Action Plan, which will guide the operation and development of its monitoring and evaluation activities.",
    "Our vision and overarching goal for this Action Plan is that it facilitates proportionate, good quality evaluation evidence generation and application across the FSA. This is rooted in our belief, as articulated in the FSA Strategy, that effective policy and regulation needs to be evidence led. Monitoring and evaluation is one source of the evidence needed to guide our work. It allows for systematic learning from past and current activities, facilitates understanding of what works, why and for whom, and supports the effective use of public funds",
    "facilitates understanding of what works, why and for whom, and supports the effective use of public funds and proportionate regulation.",
    "This Action Plan builds on existing monitoring and evaluation activities conducted by the FSA and is intended to complement benefits measurement activities already undertaken within the organisation. It sets out a framework to support FSA colleagues in identifying and prioritising areas for evaluation, guides evaluation approaches and outlines actions that will support high quality evaluations across the FSA.",
    "The success of this Action Plan relies on further strengthening evaluation capability and skills within the FSA and the fermentation of an evaluation mindset among staff. This is already facilitated by the FSA\u2019s existing activities, including our established performance reporting systems, commitments to measuring progress and track record of conducting evaluations. These activities will be further supported by the implementation of the actions recommended in this plan.",
    "This is a living document and will be periodically reviewed to ensure it is fit for purpose and having the intended impact. We will work to deliver the recommended actions listed in this document to ensure the FSA becomes known across government for the high quality of its evaluation activities.\n\n# Introduction",
    "# Introduction\n\nThe FSA is the independent government department responsible for protecting public health and consumers\u2019 wider interests in relation to food in England, Wales and Northern Ireland. Our core mission is to ensure that people can trust food. We undertake a range of activities to ensure food is safe, what it says it is, and is healthier and more sustainable.",
    "We invest significant resources in delivering against our mission. While providing value for money has always been an organizational priority, it is critical that our investments and interventions deliver as intended, are appropriately targeted, and provide the greatest possible economic and social return. This is even more important in times of constrained resources, and with many of our partner organizations and stakeholders (for example, Local Authorities, Food Business Operators) working to recover from the impact of the Covid Pandemic.",
    "Generating good quality monitoring and evaluation evidence is vital to the FSA\u2019s ability to do this. This evidence ensures we are continually learning and accountable to our funders and those we serve. It also allows us to design and deliver policies, programs, communications, and regulations effectively. We can also use information and data gathered through monitoring and evaluation activities to avoid unnecessary burdens being placed on businesses while ensuring consumers can have confidence in the food system.\n\n# Monitoring and evaluation in the policy cycle",
    "# Monitoring and evaluation in the policy cycle\n\nMonitoring and evaluation form a key part of the policy development and delivery lifecycle at the FSA and reflects best practice policy development guidance from both HM Treasury and the National Audit Office (NAO). Examples of current and forthcoming evaluations and how we have used evaluation evidence to inform actions are included in Annexes C and D respectively.",
    "HM Treasury\u2019s Green Book states that each level of policy development, be that setting policies, portfolios, programs, or projects, follows broadly the same policy development and review pattern: the ROAMEF cycle (see Figure 1).\n\n# Figure 1 The ROAMEF Cycle\n\n|Rationale|Feedback|Objectives|\n||||\n|Evaluation|Monitoring|Appraisal|",
    "ROAMEF \u2013 standing for Rationale, Objectives, Appraisal, Monitoring, Evaluation and Feedback \u2013 places the generation and application of evidence throughout the policy development lifecycle. Evidence can be generated through evaluation and monitoring as well as other activities, such as primary research. It should support options appraisal and inform changes to (or continuations of) organisational activities. Evidence demonstrates whether activities are having the intended impact, shows progress on delivery and identifies barriers and facilitators to implementation and impact.",
    "The NAO similarly highlights that evidence generated through evaluation and monitoring, and through other organisational activities, are critical for effective regulation. The collection and analysis of data and information and the monitoring of compliance with regulations can help identify problems that need intervention and can enable the prioritising and targeting of activities and resources. Likewise, developing theories of change and evaluating the impact and outcomes of regulation on an ongoing basis help evidence value for money, provide insight into unintended outcomes and refine regulatory interventions to improve outcomes.",
    "Appraisal, monitoring and evaluation are core ways evidence is generated during policy making. While deployed at different points, they typically use similar approaches (for example, quantitative and qualitative research) and tools (for example, Logic.\n\nModels/Theories of Change). The Magenta Book provides comprehensive advice and guidance on how to design evaluations and appropriate approaches.",
    "For brevity, in this document the term \u2018evaluation\u2019 will be used as shorthand to refer to the FSA\u2019s use of appraisal, monitoring and evaluations. The HM Treasury Green Book provides the following definitions of these terms:",
    "- Appraisal: the process of assessing the costs, benefits and risks of alternative ways to meet government objectives. It helps decision makers to understand the potential effects, trade-offs and overall impact of options by providing an objective evidence base for decision making.\n- Monitoring: the collection of data, both during and after implementation. This can form a baseline against which any changes to implementation can be measured.",
    "- Evaluation: the systematic assessment of an intervention\u2019s design, implementation and outcomes. It tests how far an intervention is working or has worked against expected, if the costs and benefits were as anticipated, whether there were significant unexpected consequences, how it was implemented and, if changes were made, why.",
    "1.2 This document: the FSA\u2019s Evaluation Action Plan\n\nThis Action Plan is an opportunity to strengthen the work the FSA already does to evaluate its work. Chapter 2 describes our vision for evaluation. Chapter 3 focuses on the practical steps we will take to deliver this vision.",
    "This is a living document. We plan to periodically review the Evaluation Action Plan, and the FSA\u2019s implementation of it, to ensure the Plan is having the intended impact. We anticipate the first review of the Evaluation Action Plan will occur in 2025, in line with the development of a Benefits Management Action Plan (BMAP). The BMAP will focus on how the FSA can systematically measure and realise the benefit of its work.\n\n1.3 Scope",
    "1.3 Scope\n\nThe FSA\u2019s remit is broad with the FSA carrying out different roles in the food system to help ensure food is safe, what it says it is, and healthier and more sustainable. The FSA shares responsibility for food policy in the United Kingdom: Food Standards Scotland is\n\n7",
    "7\n\nresponsible for food policy and implementation in Scotland; the FSA has different responsibilities within England, Wales and Northern Ireland, and partners with different bodies to deliver its work accordingly; and, responsibility for food policy is itself split across different government departments.",
    "This Action Plan applies to work directly undertaken by the FSA to deliver its Strategy. As outlined in Annex C, this includes FSA programmes (for example, Achieving Business Compliance (ABC) and the Operational Transformation Programme (OTP)), activities undertaken as part of our business as usual (for example, measuring the effectiveness of our official control interventions and the difference our support and guidance make to our stakeholders) and activities commissioned by the FSA to facilitate delivery (for example, non-routine surveillance",
    "stakeholders) and activities commissioned by the FSA to facilitate delivery (for example, non-routine surveillance activities, special projects). Policies, projects and initiatives outside of the direct control of the Agency (for example, decisions made by other government departments) or where we do not contribute data are outside this Action Plan\u2019s scope.",
    "The cross-government, multi-agency nature of much of the FSA\u2019s work means the FSA will need to collaborate with other government departments and delivery partners to conduct effective evaluations. It will also need to reflect variations in activities or implementation approaches in different nations in its evaluations and provide proportionate support to other government departments when requested to do so.\n\n# 8\n\n# Our Vision for the Evaluation Action Plan",
    "# 8\n\n# Our Vision for the Evaluation Action Plan\n\nThe FSA\u2019s vision for this Action Plan is that it creates a climate in which proportionate, good quality evaluation evidence is consistently generated and used. This vision is grounded in our belief that effective policy and regulation needs to be evidence led, something outlined in our guiding principles.",
    "Strengthening our existing evaluation capability and skills and ensuring consistent practice across the FSA is key to achieving this. This section sets out our vision for what fortifying evaluation capability and skills will deliver. The next chapter provides detail on how this can be delivered.\n\n# Anticipated outcomes of the Evaluation Action Plan\n\nThe FSA\u2019s Evaluation Action Plan is centred on strengthening evaluation capability and skills across the FSA. This will deliver the following outcomes:",
    "Increased understanding and awareness of pe value and need for evaluation evidence.\nIncreased capacity to design and deliver robust evaluations internally.\nImproved ability to commission evaluations wip mepodologies pat are fit for purpose and in a timely manner.\nImproved ability to apply evaluation and monitoring evidence in policy development and operational delivery.\nStrengpening of an evaluation mindset among FSA.",
    "This last outcome \u2013 a stronger evaluation mindset \u2013 is perhaps the most important outcome of this Action Plan, as it will drive the realization of the other benefits. Having an evaluation mindset means being focused on the organization\u2019s ultimate mission, rather than the success (or failure) of individual activities. This mindset means that evaluations, the findings of which can sometimes be uncomfortable, are seen as valuable learning tools which support mission delivery and organizational outcomes, even when they demonstrate that activities are not working optimally.",
    "# Supporting the Evaluation Action Plan\n\nDelivering on the FSA\u2019s vision for this Action Plan depends on colleagues knowing when and how to evaluate. This in turn relies on there being appropriate governance structures and colleagues having the skills necessary to design and commission evaluations.",
    "This section sets out questions and considerations that should guide when, how and at what scale to evaluate, the quality assurance measures that are in place to ensure evaluations are robust and fit for purpose, how FSA staff\u2019s evaluation capability and skills can be supported and how the profile of evaluation can be raised across the FSA.\n\n# Our Evaluation Approach",
    "Evaluation is important. The FSA already conducts a range of evaluation activities pre- and post-policy implementation, in line with guidance provided in the Green Book; all of the FSA\u2019s discretionary spending in Northern Ireland is required to undergo proportionate post-project evaluation. Examples of approaches take across the FSA include our benefits measurement approach within our business case process and our use of establishing project impact (EPI) forms pre and post award to capture the intended and realised impact of our work.",
    "It is critical, however, that evaluation activities are proportionate and meet the needs of decision makers and those scrutinising our activities. While good quality evaluation evidence supports the delivery of our mission, it is not an end in itself: evaluation should facilitate the FSA\u2019s work, integrate into our existing processes and be timely to support effective organisational delivery.",
    "When deciding what form of evaluation to conduct, FSA colleagues should consider the potential value of the evidence generated from evaluation (i.e., the filling of knowledge gaps), alongside reporting requirements (i.e., the need to demonstrate accountability and transparency, scale of investment / use of public funds) and the practicalities of delivering an evaluation (i.e., whether it is feasible to evaluate in a timely manner). This should reflect how best to evidence anticipated benefits described in business cases. Areas that make a greater",
    "This should reflect how best to evidence anticipated benefits described in business cases. Areas that make a greater contribution to the evidence base, where it is a requirement, where it is",
    "# feasible and where activities sit within the FSA\u2019s priority programmes should be prioritised.\n\nThese considerations also have a bearing on evaluation scale and approach. Activities can be evaluated in different ways with the most appropriate form of evaluation depending on the questions being addressed, the profile and cost of the activities being evaluated, and the risk/uncertainty surrounding what can be learnt through evaluation.",
    "There is no one-size-fits-all approach for choosing an evaluation approach and robust evaluation methods can take many forms. While commissioning third-parties to conduct evaluations may be desirable, it is not always necessary, feasible or proportionate; internally designed and delivered evaluation activities can provide the required insight in a timely way and more efficiently but can sometimes be perceived as less independent. Likewise, although experimental methods allow the impact of activities to be clearly demonstrated, and are considered by default at the FSA,",
    "allow the impact of activities to be clearly demonstrated, and are considered by default at the FSA, they cannot always be operationalised. That said, gathering quantitative insights through our evaluations is generally desirable.",
    "Colleagues leading FSA activities should engage broadly and consult with Science, Evidence and Research Division (SERD) colleagues early in the policy formation and business case development process to decide what type of evaluation is required, whether evaluation activities can be conducted in-house and whether independently conducted evaluation is appropriate. SERD colleagues will also be able to advise on evaluation approach, timing and any practical or ethical issues that may support or prevent a particular approach.",
    "Further detail on how we identify and prioritise areas for evaluation and factors which inform our choice of evaluation type and approach are detailed in Annex A.\n\n# 3.2 Quality assurance\n\nEvaluations can be resource intensive; doing them well requires active and early engagement with subject matter specialists and understanding of the value evaluation evidence can offer.\n\nSupporting evaluation capability and skills, and thereby supporting an evaluation mindset, will support the early consideration of evaluation and ensure research questions",
    "can be addressed with appropriate research methods. This will help ensure colleagues are able to identify when evaluation is needed, what the implications of evaluation are for policy implementation and or rollout, and that appropriate colleagues with the FSA (for example, SERD, operations, policy and so forth) and in delivery partners (for example, local authorities) are engaged to support data collection and evaluation delivery.",
    "In conjunction with ensuring colleagues have the right capabilities and conducting evaluation is appropriately incentivised, use of our existing quality assurance mechanisms will ensure FSA evaluations are robust and their findings credible. Within the FSA, we have the following mechanisms to support quality assurance and the delivery of robust evaluations, which are drawn on where applicable:",
    "- Our existing business case process which captures anticipated benefits of business activities and prompts consideration of how these benefits will be evidenced and in time realised.\n- Reference of the Analytical Quality Assurance (Aqua) Book, the Magenta Book and Annexes to ensure processes align with best practice guidance.\n- Use of the Advisory Committee for Social Science (ACSS) to act as a critical friend and provide input to our evaluation plans and approach, and to support with peer review.",
    "- Use of external peer reviewers to advise on evaluation approaches and to quality assure evaluation outputs.\n- Involvement of a suitably experienced project officer and / or project manager to ensure that evaluations run to time and budget, that risks are managed and key milestones hit.",
    "We will also explore the feasibility of introducing the following additional quality assurance measures:\n\n- Publication of evaluation plans, publication plans, trial protocols, results and datasets as appropriate to evaluation methods before/at the start of evaluations\n\n# where possible and where doing so will not compromise the efficacy of the evaluation or policy development process.",
    "- Publication of supporting documentation (for example, Logical Models/Theories of Change, Project Plans) alongside final outputs.\n- Use of the Assurance working group to support the impartial commissioning and delivery of evaluations, including a review of the types of data collected and research questions, through provision of critique of evaluation method and Logic Models etc.\n\n# Building evaluation skills and capability across the FSA",
    "# Building evaluation skills and capability across the FSA\n\nEnsuring FSA colleagues have the skills they need to commission, design, deliver and quality assure evaluations is critical to the delivery of this Action Plan; it also underpins an evaluation mindset.\n\nDelivering the following activities will support the development of evaluation skills across the FSA and the design and delivery of timely, robust evaluation:",
    "- Measure existing levels of awareness and understanding of evaluation and evaluation skills at the FSA to identify key gaps, create a baseline to measure changes in awareness and understanding and to appropriately target learning activities. While this would be a cross FSA activity, conducting focused work within SERD to baseline existing evaluation experience and expertise would support targeted learning and development activities to fill any unmet needs.",
    "- Development of bespoke training programmes for FSA colleagues to increase evaluation skills and awareness. This could include the provision of introductory training for policymakers on the value of evaluation, how it can be integrated into the policy development process, and the strengths, limitations and requirements of different types of evaluation, and the provision of specialist training on the",
    "Note, the FSA already pre-registers protocols for its behavioural trials and makes use of the trial advice panel.\n\ndesign and delivery of evaluation to increase the capacity for conducting evaluation, with SERD colleagues prioritised.",
    "- Review of existing FSA resources and tools for evaluation to ensure consistency and sharing of good practice. Where gaps are identified, the development of toolkits to support conduct of discrete stages of evaluation (for example, scoping, development of Theory of Change).\n- The creation of evaluation drop-in surgeries whereby colleagues can engage with an evaluation expert, discuss options for evaluation and troubleshoot potential evaluation challenges.\n\nIn addition to the above, the following tools could be developed:",
    "In addition to the above, the following tools could be developed:\n\n- A checklist of key evaluation considerations for colleagues to use during the business case process in order to identify appropriate evaluation approaches and the implications of these choices for implementation/rollout of business activities.\n\n# Further raising the profile of evaluation",
    "# Further raising the profile of evaluation\n\nThe FSA already supports the evaluation of its work and incorporates monitoring and benefits measurement into its business practices (see Annex C). Our Strategy includes the guiding principles of being \u2018science and evidence led\u2019 and \u2018open and transparent\u2019. These align with key precepts of evaluation - that it supports learning, informs action and supports accountability.",
    "To support wider awareness of our evaluation activities, we already publish and promote our evaluation findings, conduct lessons learned sessions and share best practice across the FSA. We propose doing the following to raise the profile of evaluation and support an evaluation mindset across the FSA:",
    "- Seek an evaluation champion(s) at senior level to support the use of evaluations, showcase evaluation activities and the benefits they have delivered, and advocate for training for staff on the benefits evaluation evidence delivers. This could include promotion of the FSA\u2019s evaluation criteria (Annex A), wider government resources designed to promote evaluation (for example, the Magenta Book) and the importance of benefits measurement to our activities.\n\n# Create an evaluation community of practice",
    "# Create an evaluation community of practice\n\nwho could supporting effective evaluation and the sharing of best practice across the FSA.\n\n# Include a prompt for colleagues",
    "to confirm they have considered how projects are to be evaluated when producing a business case. Tools used by the FSA in Northern Ireland, where proportionate post-project evaluation is a requirement for all discretionary spend, could be used as a template for activities across the FSA. Changes could include a prompt to engage with SERD on the development of an evaluation plan and a requirement to describe which evaluation approach is recommended, which have been considered and which have been discarded. Changes to the business case process will",
    "is recommended, which have been considered and which have been discarded. Changes to the business case process will need to align with the BMAP.",
    "# Ensure alignment between benefit measurement process and evaluation activities\n\nto ensure evaluation is proportionate and efficient.\n\n# Create an annual \u2018Evaluation Week'\n\nto promote understanding and awareness of the value and benefits of evaluation.\n\nCollectively, the above actions will remind colleagues to plan evaluation, demonstrate the value of evaluation and support the use of evaluation evidence across the FSA.\n\nNext Steps",
    "Next Steps\n\nThis Action Plan has set out the FSA\u2019s vision for evaluation and actions that will support the generation and application of evaluation evidence across the agency. We recognise that it will take time and resources to deliver this plan and to realise its benefits. To make this a success a concerted effort is needed by FSA colleagues as well as effective collaboration with our delivery partners.",
    "We intend to review this Evaluation Action Plan periodically to gauge the effect of its implementation and to ensure it remains relevant and fit for purpose. Crucially, we will need to ensure that its recommendations align with forthcoming Benefits Management Action Plan, which is intended to support the measurement and realisation of benefits provided by FSA activities.\n\nIn the interim we will work to build on our existing evaluation activities to ensure the FSA continues to be an evidence-led organisation.\n\n16",
    "16\n\n# Annex A: Guidance for when and how to evaluate\n\nThis annex contains more detail on the criteria FSA colleagues should consider when deciding whether to evaluate their work, how to decide the scale of an evaluation and how to decide what type of evaluation is required.\n\n# Evaluation criteria\n\nThe FSA has developed a set of criteria based on best practice guidance that should be considered when deciding whether and how to evaluate FSA activities. These are:",
    "Wheper pere is a knowledge gap to be filled by evaluation, including when pe policy or strategy being implemented is novel or untested or where evaluation evidence would inform future practice.\nWheper evaluation is required to demonstrate accountability and transparency over use of public funds and pe fitness for purpose of Agency interventions.\nWhat pe scale of investment associated wip pe policy, programme or project being evaluated has been.\nWheper conducting an evaluation would be feasible and deliver evidence in a timely manner.",
    "These criteria are intended to be used alongside and to complement existing tools, most notably the FSA\u2019s business case process. This process requires colleagues to articulate the anticipated benefit of their proposed activity, current performance in the area (for example, baseline data) and anticipated measures.\n\n# Evaluation scale",
    "# Evaluation scale\n\nActivities can be evaluated in many ways. The nature and range of work done by the FSA, which commonly involves a range of delivery partners as well as cross-organisation and cross-nation working, means a tailored approach to evaluation is required.",
    "# Although final decisions on the scale of evaluation required will have to be taken in the context of wider business needs, available resources, organisational priorities, and what has been specified within business cases, Figure 2 provides a rule-of-thumb guide which FSA colleagues can follow to help choose an appropriate scale of an evaluation. This framework is based on that developed by the UK Space Agency.\n\nChoosing Proportionate Evaluation",
    "Choosing Proportionate Evaluation\n\nRisk and uncertainty\nLow - Straightforward low-risk programme wip low uncertainty around pe outcomes\nMedium - Programme not especially complex or risky, but some uncertainty around outcomes\nHigh - Complex programme design, and/or significant risk and uncertainty around programme outcomes",
    "Budget and profile:\nHigh - Large programme wip significant budget, and/or high profile wip public interest, and potentially high impact\nMedium - Medium-sized programme wip moderate budget, and/or some public interest, expected to have a sizeable impact\nLow - Small budget and/or limited public interest",
    "|Category|Risk and uncertainty: Low|Risk and uncertainty: Medium|Risk and uncertainty: High|\n|||||\n|Budget and Profile:|Level 2|Level 3|Level 3|\n|Budget and Profile:|Level 2|Level 2|Level 3|\n\n# Budget and Profile: Low",
    "# Budget and Profile: Low\n\n|Level 1|Level 2|Level 2|\n||||\n|light-touch evaluation recommended, including before/after monitoring|consider commissioning externally, with appropriate budget allocation|detailed, externally commissioned evaluation with budget of 1-5% of total programme recommended|\n\nBudget thresholds: &lt; \u00a3150,000 Low; \u00a3150,001-\u00a3500,000 Medium; \u00a3500,001+ High",
    "The framework recommends that FSA colleagues consider the budget and profile of the activities being evaluated alongside the risk associated with the work and uncertainty over what outcomes and evaluation would deliver when deciding what level of evaluation is needed.\n\nActivities which are lower risk and where there is low uncertainty around the outcomes of an evaluation and where budgets are small would likely suit a \u2018light-touch\u2019 evaluation that could include before / after monitoring (Level 1).",
    "Activities where there is some uncertainty around outcomes, but which are not especially complex, may require a larger-scale evaluation, with appropriate budget allocation (Level 2). These activities may require a Level 3 evaluation \u2013 a detailed, externally commissioned evaluation with appropriate budget (1-5% of total programme recommended) - when the activity being evaluated has a significant budget, and/or there is high interest in the outcome, and potentially high impact in conducting an evaluation for the future of the policy or programme.",
    "Activities that have complex programme designs, and/or significant risk and uncertainty around programme outcomes typically require a Level 3 evaluation, although in situations where the activities have small budgets or where there is limited media/public interest a Level 2 evaluation may be suitable.\n\nLevels do not need to be distinct: there could be scope for including elements from two levels, for example. Likewise, not all Level 2 or Level 3 evaluations will require externally",
    "Here \u2018budget\u2019 includes allocation of funds and internal resources such as staff time.\n\n# commissioned evaluations\n\nIn some instances, internally led evaluations may be more appropriate and represent a more effective use of resources. Similarly, while benefits measurement may be delivered through evidence generated in-house, externally commissioned evaluation may provide evidence of unanticipated consequences of an intervention and a more nuanced understanding of the effect of activities.",
    "We anticipate that evaluation of the FSA\u2019s priority programmes and corporate priorities will require either Level 2 or Level 3 evaluations, but that constituent parts of these programmes may require Level 1 or Level 2 evaluations.\n\n# Evaluation type\n\nThere are three common types of evaluation: process, impact and value-for-money.",
    "|Process evaluations|consider whether an intervention is being implemented / delivered as intended, whether the design is working and what is working more or less effectively, for whom and why.|\n|||\n|Impact evaluations|involve an objective test of what changes have occurred, the scale of those changes in an assessment of the extent to which they can be attributed to the intervention.|\n|Value-for-money evaluation|involve comparing the benefits and costs of the intervention.|",
    "Which type of evaluation is appropriate depends on the questions being addressed by the evaluation. While further details are available in the Magenta Book, process evaluations typically seek to understand what can be learned from how the intervention was delivered, impact evaluations try to understand what difference the intervention made, while value-for money evaluations seek to address whether the intervention is a good use of resources.",
    "The Magenta Book recommends conducting scoping work prior to deciding the type of evaluation required. For this to be done effectively, a broad range of internal, and sometimes external, stakeholders need to be engaged. In line with the ROAMEF Policy Development Cycle, this scoping work should take place alongside policy development and prior to implementation. It is essential that evaluation be considered at the start of",
    "the policy development and implementation process; suitable benefit measures and plans for how these can be realised must be included in business cases. This is because how the policy is implemented and what data is collected during implementation holds implications for what types of evaluation are feasible. This is particularly the case for impact evaluations, where control/comparison groups are often necessary to demonstrate impact.",
    "Process, impact and value for money evaluations require different approaches and resources (see Figure 3). Specific guidance on this is available in the Magenta Book Annex A. This details the analytical methods for use within an evaluation, including generic research methods for use in process and impact evaluations, methods for experimental and quasi experimental methods for impact evaluation, theory based methods, methods for value for money evaluation and methods for the synthesis of existing evidence. Regardless of evaluation type, most evaluations benefit from quantitative methods being included",
    "for the synthesis of existing evidence. Regardless of evaluation type, most evaluations benefit from quantitative methods being included in some capacity.",
    "|Scoping, Designing and Conducting an Evaluation from the Magenta Book|\n||\n|Define the rationale of the intervention, what it aims to achieve|\n|Identify the purpose of the evaluation; determine the type(s) of evaluation required|\n|Process|Impact|Value-for-money|\n|What can be learned from how the intervention was delivered?|How and why did the impact occur?|What difference did the policy make?|",
    "|Is it the best use of resources?|\n|Approach based on collecting primary data to|Experimental (counterfactual approach)|Quasi-experimental (counterfactual approach)|\n|Approach weighs up costs and benefits using monetary|\n|data|\n|Choose the Evaluation Method(s) (depends on many factors; data from the scoping stage for further negotiation)|\n|Collect; review & synthesise data|",
    "|Collect; review & synthesise data|\n|Primary data collection includes performance monitoring, consultative/deliberative methods, observational study; surveys|\n|Synthesis evaluation includes meta-analysis or meta-evaluation, realist synthesis, Bayesian synthesis. Reviews include rapid evidence review; systematic review|\n|Many evaluations can also be participatory (e.g. using developmental action research methods). These are particularly useful in complex settings (see Complexity Guide)|",
    "The type of evaluation approach and methods used should always be informed by the research questions being addressed and considerations of what is feasible in a given context. While best practice guidance may recommend experimental approaches that can demonstrate causal relationships between control and comparison groups (for example, Level 3 in The Nesta Standards of Evidence), and such experimental methods",
    "are considered by default by the FSA, this may not always be practicable within an organisation\u2019s structure or its regulatory responsibilities. Likewise, certain evaluation types can only be conducted if appropriate baseline data is collected. It is therefore essential that evaluation is not an afterthought but instead integrated into the policy development process.\n\n# 22\n\n# Annex B: Table of recommendations",
    "|Recommendation / Action|Anticipated Benefit|Priority (high / medium / low)|\n||||\n|Creation of an evaluation group within SERD who could lead on supporting effective evaluation across agency.|Support evaluation mindset; raise profile of evaluation; position evaluation as organizational norm.|High|",
    "|Showcasing completed evaluations and lessons learned sessions at both a programme and FSA level to highlight the value of evaluations among colleagues.|Support evaluation mindset; raise profile of evaluation; position evaluation as organizational norm.|High|\n|Measure existing levels of awareness and understanding of evaluation at the FSA to identify key gaps, create a baseline to measure changes in awareness and understanding and to appropriately target learning activities.|Support delivery of robust evaluation; support development of tailored training programme within FSA.|High|",
    "|Alignment between benefit measurement and realisation and wider evaluation activities.|Ensure evaluation is proportionate, efficient and to avoid duplication of effort.|High|",
    "|Recommendation / Action|Anticipated Benefit|Priority (high / medium / low)|\n||||\n|Inclusion of a prompt for colleagues to confirm they have considered how projects are to be evaluated when producing a business case.|Support delivery of robust evaluation; quality assurance; position evaluation as organisational norm.|High|",
    "|Explore feasibility of publishing evaluation plans, publication plans, and trial protocols before/at the start of evaluations where possible and where doing so will not compromise the efficacy of the evaluation or policy development process.|Support delivery of robust evaluation; quality assurance|High|\n|Publication of evaluation results and datasets as soon as possible following completion of the evaluation and where doing so will not compromise the policy development process.|Support delivery of robust evaluation; quality assurance|High|",
    "|Publication of supporting documentation (for example, Logical Models/Theories of Change, Project Plans) alongside final outputs.|Support delivery of robust evaluation; quality assurance|High|",
    "|Recommendation / Action|Anticipated Benefit|Priority (high / medium / low)|\n||||\n|A checklist of key evaluation considerations for colleagues to use during the business case process in order to identify appropriate evaluation approaches and the implications of these choices for implementation/rollout of business activities.|Support delivery of robust evaluation; Ensure evaluation is proportionate, efficient and to avoid duplication of effort.|High|",
    "|Conduct a skills audit to baseline existing evaluation experience and expertise in delivering specific types of evaluation and using particular methods.|Support delivery of robust evaluation; support development of tailored training programme within FSA.|Medium|",
    "|Seek an evaluation champion(s) at senior level to support the use of evaluations, showcase evaluation activities and the benefits they have delivered, and advocate for training for staff on the benefits evaluation evidence delivers.|Support evaluation mindset; raise profile of evaluation; position evaluation as organizational norm.|Medium|",
    "|Creation of an annual \u2018Evaluation Week\u2019 in through which to promote understanding and awareness of the value and benefits of evaluation.|Support evaluation mindset; raise profile of evaluation; position evaluation as organizational norm.|Medium|",
    "|Recommendation / Action|Anticipated Benefit|Priority (high / medium / low)|\n||||\n|Use of the Assurance working group to support the impartial commissioning and delivery of evaluations, including a review of the types of data collected and research questions, through provision of critique of evaluation method and Logic Models etc|Support delivery of robust evaluation; Ensure evaluation is proportionate; quality assurance|Medium|",
    "|Development of bespoke training programmes for policy professionals and SERD colleagues to increase evaluation skills.|Support delivery of robust evaluation; quality assurance.|Medium|\n|Review of existing FSA resources and tools for evaluation to ensure consistency and sharing of good practice across the organisation.|Support delivery of robust evaluation; quality assurance.|Medium|",
    "|The creation of evaluation drop-in surgeries whereby colleagues could engage with an evaluation expert, discuss options for evaluation and troubleshoot potential evaluation challenges.|Support delivery of robust evaluation; quality assurance.|Low|",
    "# Annex C: Current and Forthcoming Evaluation Activity",
    "|Evaluation Activity|Description|Schedule|\n||||\n|Evaluation of implementation of Prepacked for Direct Sale (PPDS) Legislation|Mixed method evaluation|Autumn 2022/Spring 2023|\n|Evaluation of Remote Assessments for FHRS Requested Re-inspections|Qualitative evaluation of use of remote reassessment|Autumn/Winter 2022|",
    "|Evaluation of the pilots for the new Food Standards Delivery Model|Mixed method quasi-experimental evaluation|Ongoing, scheduled to complete Autumn/Winter 2022|\n|Evaluation of Achieving Business Compliance (ABC) programme|Modernizing the way food businesses are regulated|Ongoing, scheduled to be reviewed at the end of financial year 24/25|",
    "|Evaluation of Operational Transformation programme (OTP)|Modernizing delivery of Official Controls for meat, dairy, and wine|Planned, covering various elements|",
    "Ongoing reporting activities:\n\n[Annual] Mandatory Local Aupority Returns, Collected as part of pe Management Information System [Annual]\n[Annual] The Annual Report and Accounts\n[Quarterly] The Quarterly Performance and Resource Report\n[Rolling] Benefits Measurement and Tracking of Realisation\n[Rolling] Setting and Monitoring of Key Performance Indicators (KPIs)\n\n# Annex D: How the FSA uses Evidence",
    "# Annex D: How the FSA uses Evidence\n\nThe FSA is an evidence-led organisation. Below are some case studies illustrating how we have used evaluation evidence to inform our decisions.\n\n# Case study 1: Evaluating the use of remote assessments by local authorities for regulating food businesses (2021)",
    "As part of its response to the coronavirus pandemic the FSA advised local authorities that they may use remote assessments in some circumstances to determine areas to target during a subsequent onsite visit and in other circumstances to inform the need for an onsite visit to assess and address potential public health risks.\n\nThese remote assessments can take a variety of forms, for example, a phone call, a video call, or exchange of information online.",
    "The FSA commissioned ICF to undertake a short evaluation of local authorities and food business operators experience of using remote assessments. This was to help inform the FSA\u2019s thinking about future regulatory practice and to ensure the most efficient and effective use of local authority resources.\n\nICF took a qualitative approach to the evaluation. They first conducted a scoping and desk research phase followed by 20 interviews with local authorities.",
    "The evaluation found that remote assessments were perceived as a helpful tool in the context of the pandemic and for use with low-risk businesses and for Food Standards Controls (for example, assessing menus, labelling). It also highlighted that there were concerns about a perceived potential for food businesses to conceal information and falsify/mask problems with their businesses.",
    "Evidence from this evaluation provided the FSA with valuable insight on the viability of using remote assessments with food businesses. It subsequently informed the decision to conduct further research on the use of remote reassessment in the scenario in which a business requests to be re-rated if they did not achieve the top food hygiene rating and if they have made the required improvements.\n\nMore information on this evaluation is available on the FSA\u2019s website.\n\n# Case study 2: Evaluations of recalls and withdrawals",
    "# Case study 2: Evaluations of recalls and withdrawals\n\nBetween 2016 and 2017, the FSA and Food Standards Scotland (FSS) undertook a review of the withdrawal and recall system in the UK food retail sector, to identify if improvements were needed to enhance the current system.",
    "This system redesign aimed to increase consumer awareness of the recall process, outline clear recall roles and responsibilities (for Food Business Operators, local authority enforcement officers, consumers and the FSA) and increase legislative compliance among food business operators (FBOs). The system redesign resulted in the creation of a package of tools, including UK guidance on Traceability, Withdrawals and Recalls, best practice guidance on communicating food recalls to consumers, a template point of sale notice and a Root Cause Analysis",
    "practice guidance on communicating food recalls to consumers, a template point of sale notice and a Root Cause Analysis (RCA) package.",
    "RSM UK Consulting LLP (RSM) was commissioned jointly by FSA/FSS in 2021 to conduct a process evaluation to explore the programme processes and the partnership approach used as part of these processes, and the success (or otherwise) of achieving: clear and distinct roles/ responsibilities in the new system; consistent and accessible information provided to consumers, and cross industry sharing of approaches and impact; increased public awareness of food recalls and actions they need to take; and, commitment to continuous system",
    "increased public awareness of food recalls and actions they need to take; and, commitment to continuous system improvement.",
    "The evaluation involved a desk review of existing programme documents and data to understand the original evidence base and rationale for change, interviews with external stakeholder reference group members, case studies with FBOs and enforcement agencies involved in recent recalls, exploration of hypothetical scenarios to glean learning on the ability of the redesigned recalls system, focus groups with consumers to explore consumer awareness of product recalls, and secondary data analysis to establish baseline and explore implementation for the post system redesign.",
    "The final report of findings and recommendations is due to be delivered to the agency in August 2022. This evaluation is expected to inform provision of new and tailored guidance for FBOs, communications work to educate and raise awareness of the recalls procedures and resources amongst businesses and consumers, and, an improved system for sharing Root Cause Analysis findings.",
    "Learnings from the evaluation will also be shared across programmes within the FSA to support learning across workstreams. The evaluation report will also recommend that the Agency use the successful Recalls system redesign approach, with clearly designed workstreams and regular engagement with key stakeholders, for any future projects that require partnership working.\n\n# Case study 3: Evaluation of the Food Hygiene Rating Scheme and the Food Hygiene Information Scheme",
    "In 2010/11 the FSA introduced two schemes intended to provide consumers with information about the hygiene standards of food premises so that they can make informed decisions about where to buy food and eat away from home: The Food Hygiene Rating Scheme (FHRS) in England, Wales and Northern Ireland and the Food Hygiene Information Scheme (FHIS) in Scotland (which now comes under the responsibility of FSS). The schemes aim to improve food hygiene standards among food businesses, which are expected",
    "responsibility of FSS). The schemes aim to improve food hygiene standards among food businesses, which are expected to respond to public demand for higher standards, and their overarching goal is to reduce the incidence of food-borne illnesses in the UK population. The schemes are FSA/local authority partnership initiatives which provide consumers with information about hygiene standards in food premises at the time they are inspected to check compliance with legal requirements. The FHRS rating or FHIS result given to the business reflects the inspection findings. Under the",
    "The FHRS rating or FHIS result given to the business reflects the inspection findings. Under the FHRS, businesses are given one of six ratings on a numerical scale from \u20185\u2019 (very good hygiene standards) at the top to \u20180\u2019 (urgent improvement required) at the bottom. Under the FHIS, businesses are given either a \u2018Pass\u2019 result or an \u2018Improvement required\u2019 result.",
    "In 2011, the FSA and FSS commissioned the Policy Studies Institute (PSI) to evaluate these programmes. The overall aim of the evaluation was to assess whether the FHRS and FHIS were operating as intended; whether the schemes improved food hygiene standards at food premises and ultimately contributed to a reduction in food-borne illnesses.\n\nIt had two main strands:",
    "It had two main strands:\n\n- A process evaluation, which took place between autumn 2011 and summer 2013 and collected data from the perspectives of local authority food safety team staff, food business operators and consumers. The process evaluation took a case study approach in which fieldwork took place within a sample of UK local\n\nauthorities. Data collection included interviews with local authority officers and food business operators, a survey of food business operators and focus groups with consumers.",
    "An impact evaluation which focused on those local authorities that launched the FHRS or FHIS during the 2010/11 financial year and tested the causal effect of the FHRS/FHIS on two sets of outcomes: i) compliance with food hygiene standards and ii) food-borne illnesses. The study used a difference-in-differences (DID) methodology: outcomes for local authorities that introduced the FHRS/FHIS (in financial year 2010/11) were compared to outcomes",
    "the FHRS/FHIS (in financial year 2010/11) were compared to outcomes for local authorities that did not. The difference between the outcomes observed for the two groups of local authorities provided an estimate of the causal effect of the FHRS/FHIS. Impacts were observed in the first and second years after local authorities launched a national scheme (early adopters), financial years 2011/12 and 2012/13.",
    "To support the evaluations, theories of change were developed to set out the scheme logic and assumptions underpinning behaviour changes for each stakeholder group (local authorities, food business operators and consumers). These theories of change served as the conceptual framework for the evaluation.",
    "The process evaluation validated the theory of change, in the most part, with the UK-wide implementation of the schemes on target at the point the evaluation reported. All but one LA was operating or had committed to run the schemes by the end of the evaluation. That said, there were variations in how the schemes were implemented across the nations with the marketing of the schemes varying a great deal in Northern Ireland and Wales.",
    "The impact evaluation found that the FHRS had a statistically significant positive impact on food hygiene standards; the FHIS scheme was not shown to have had a statistically significant impact on compliance but that the trends were broadly the same as in the rest of the United Kingdom. Business compliance had also improved and there had been a significant decrease in the volume of poorly performing premises due to FHRS. However, due to serious data limitations it was not possible to derive reliable impact estimates testing the effect of the",
    "due to serious data limitations it was not possible to derive reliable impact estimates testing the effect of the FHRS/FHIS on the incidence of food-borne illnesses.",
    "The evaluation provided valuable evidence for FSA to inform and support the take up of FHRS by local authorities, becoming mandatory in Wales in 2013 (and extended to",
    "business to business in 2014) and in Northern Ireland in 2016. It has been used to build an evidence case in support of England introducing a mandatory scheme. The evaluation also enabled the FSA to take targeted actions to support Local Authorities operating the schemes, encourage businesses to comply and support consumer awareness and engagement. Learnings were also circulated internally to ensure colleagues could reflect these in their workstreams.\n\n33\n\n#\n# Standards Food Agency\n\n\u00a9 Crown copyright 2022",
    "33\n\n#\n# Standards Food Agency\n\n\u00a9 Crown copyright 2022\n\nThis publication (not including logos) is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. Where we have identified any third party copyright information you will need to obtain permission from the copyright holders concerned.\n\nFor more information and to view this licence:",
    "For more information and to view this licence:\n\n- visit the National Archives website\n- email psi@nationalarchives.gov.uk\n- write to: Information Policy Team, The National Archives, Kew, London, TW9 4DU\n\nFor enquiries about this publication, contact the Food Standards Agency Social Science Team.\n\nFollow us on Twitter @foodgov\n\nFind us on Facebook facebook.com/FoodStandardsAgency",
    "Evaluation Strategy\n\n\nJuly 2023\n\n# Contents\n\nForeword ........................................................................................................... 2\n\nExecutive Summary............................................................................................ 3\n\n1.    Introduction ................................................................................................. 4\n\n2.    Incorporating Evaluation into Governance .................................................. 5\n\n3.    Establishing a Proportionate Evaluation Programme ................................... 6\n\n4.    Embedding a Culture of Evaluation .............................................................. 7\n\nAnnex A \u2013 Forward Programme ......................................................................... 8\n\nForeword",
    "Defence of the nation is the first duty of government. In an age of constant competition, we need to maintain a robust evidence base to help us get the most out of the defence budget. Finding out whether our investments and policy decisions have been effective, delivered value for money, and are achieving anticipated outcomes is an essential part of maintaining and developing that evidence base. As a professional and learning organisation, the Ministry of Defence is committed to understanding what we do well and where we should aspire to do",
    "the Ministry of Defence is committed to understanding what we do well and where we should aspire to do better. This is a fundamental part of delivering an effective defence for the nation. I am therefore pleased to introduce this strategy, which is part of a wider set of activities to enhance our approach to evaluation. The strategy sets our direction moving forward, and outlines where we aspire to be in the future. It is intended to provide greater assurance and accountability that, in times of constrained resources, the department is making",
    "is intended to provide greater assurance and accountability that, in times of constrained resources, the department is making the right decisions and is maximising the impacts of its spending.",
    "David Williams, Permanent Secretary\n\n# Executive Summary\n\nThe Ministry of Defence Evaluation Strategy is being launched to guide policy and programme evaluation activity in the department and to demonstrate a public commitment to undertaking high-quality evaluation.\n\nThe strategy sets out how the department will meet the objectives of improving the quantity, quality, and impact of policy and programme evaluations over the next three years and ensuring that quality evaluation evidence is actively used in decision making.",
    "To achieve these aims MoD are delivering against the following strategic pillars that have proved effective in other government departments:",
    "- Incorporating the requirement for evaluation into existing corporate governance frameworks for decision making.\n- Developing a proportionate forward plan of evaluation reports to be reviewed by the Defence Board and its sub-committees and which can be used as evidence for accountability bodies.\n- Creating a culture of evaluation within the Ministry of Defence so that information on performance is deliberately sought out to learn how to improve policy and programme delivery in defence.",
    "To embed these changes, the department has established a multidisciplinary evaluation team within the Analysis Directorate and employed evaluation professionals within key programmes and policy areas who operate together in a \u2018Community of Practice\u2019 following the principles and guidance in the HMT Magenta Book.\n\nEvaluation is seamlessly integrated into the ROAMEF cycle and other existing processes, such as IPA reviews. Evaluation both complements and benefits from these other processes.",
    "The strategy will take time to implement and progress will be reviewed annually. We plan to have fully embedded these changes by December 2024.\n\n# Introduction\n\n1.1. The Integrated Review has increased the need for an improved evaluation capability in defence. Our aim is for quality evaluation evidence to become embedded into the existing governance frameworks for decision making.",
    "1.2. The objective is for the Ministry of Defence to be using systematic and consistent evaluations to better inform evidence-based decision making. This will ensure that lessons are systematically being learned from past performance and objective data is being routinely collected to inform the department about its activities and how to better ensure their success.",
    "1.3. To achieve our goals, the department is following the three-pillared strategy of: (i) explicitly linking evaluation to existing governance frameworks for decision making; (ii) developing a proportionate forward plan of evaluations to be seen by the Defence Board; and (iii) creating a culture of evaluation within the department. We have established a central evaluation team within the Analysis Directorate, employed evaluation professionals to key programmes, and are expanding the use of evaluation methods through a growing Community of",
    "employed evaluation professionals to key programmes, and are expanding the use of evaluation methods through a growing Community of Practice.",
    "# Scope\n\n1.4. The scope of this strategy covers programme and policy evaluation: specifically, (i) evaluation of projects and programmes that are subject to MOD approval processes; (ii) evaluation of policies most critical to delivering the priority outcomes set out in the Outcome Delivery Plan.\n\n# Objectives\n\n1.5. The Ministry of Defence will:",
    "# Objectives\n\n1.5. The Ministry of Defence will:\n\n- Improve the quantity, quality, and impact of policy, programme, and project evaluation.\n- Guarantee that evaluation is used to make sure that lessons are truly learned and evidence around performance against objectives exists to support accountability processes.",
    "1 Outcome Delivery Plans set out each government\u2019s department priority outcomes and the department's strategy for achieving them. Outcome Delivery Plans - GOV.UK (www.gov.uk) [Accessed 10/07/2023]\n\n# Incorporating Evaluation into Governance\n\n2.1. An important first step is to embed the requirement for monitoring and evaluation into the framework for corporate governance and approvals so that it becomes part of key decision-making processes.",
    "2.2. Ministry of Defence Director General Finance has agreed to champion evaluation by:\n\n- Formally stating that the Investment Approval Committee will demand that all projects have central evaluation team-endorsed evaluation plans alongside every business case submission.\n- Requiring that any new business case (in particular Strategic Outline Cases) or review notes must reference existing evaluation evidence to inform spending decisions.",
    "2.3. The central evaluation team are working with Front Line Commands to embed evaluation requirements for projects and programmes that are approved within the commands. In doing so, we are applying templates and guidance aligned to the Magenta Book to ensure a high-quality, consistent, and proportionate approach. We are aware of comments highlighting similar work such as IPA reviews, but there are distinct differences and benefits from carrying out an evaluation over an IPA review.",
    "2.4. The Ministry of Defence uses performance metrics to measure progress against departmental outcomes and inform evidence-based strategic decision making. Working with the Cabinet Office, we are strengthening our policy evaluation capability and adopting best practices from those in the centre of government responsible for overseeing and evaluating the implementation of the Integrated Review.\n\n2 The Magenta Book - GOV.UK (www.gov.uk) [Accessed 10/07/2023]\n\n# Establishing a Proportionate Evaluation Programme",
    "# Establishing a Proportionate Evaluation Programme\n\n3.1. It would not be cost-effective to try to evaluate every project and programme. The central evaluation team has therefore developed a prioritised forward plan for programme evaluations to be reviewed by Defence Board and its sub-committees, and which can be provided as evidence to accountability bodies. The prioritised forward plan is in Annex A.\n\n3.2. The following principles have been applied to prioritise the forward plan:",
    "3.2. The following principles have been applied to prioritise the forward plan:\n\n- The intervention\u2019s strategic importance;\n- Its financial value;\n- Interventions in response to statutory obligations;\n- The risk and uncertainty surrounding project outcomes of an intervention; and\n- Whether the evaluation will be of use to either the project or policy team and/or the department.",
    "3.3. At present, this forward plan outlines intended evaluation activity for MOD centrally approved business cases. Work to enhance the evaluation of priority outcomes within the Outcome Delivery Plan is ongoing and is intended to be embedded in future iterations of the plan as appropriate.\n\n# 4. Embedding a Culture of Evaluation",
    "# 4. Embedding a Culture of Evaluation\n\n4.1. Key to the strategy\u2019s success is the need to create a learning culture to understand what works, for whom, and in what context, and use this knowledge to advance understanding and decision making.",
    "4.2. A core activity is to build awareness of what evaluation is and is not. This is being achieved through outreach at conferences and presentations to individual policy and programme delivery areas, and is supported by broadcasts from senior leaders championing and promoting the importance of evaluation.\n\n4.3. We have developed a community of practice for evaluation practitioners in defence to share best practice and coordinate evaluation activity within the department.",
    "4.4. In the future, we will develop and deliver targeted training packages that bring evaluation to life in a defence context, focusing on fundamentals of evaluation to ensure that relevant tools and methodology can be applied in day-to-day work.\n\n# Annex A \u2013 Forward Programme",
    "# Annex A \u2013 Forward Programme\n\nMOD is committed to publishing programme evaluations to the public on Gov.uk in accordance with Government Social Research publication protocol. In cases where evaluations are unsuitable for release into the public domain (due to commercial and national security considerations), they will not be published publicly. These evaluations are exempt from the Freedom of Information Act (2000) under Section 43 (Commercial) and Section 26 (National Security).\n\nPre-Full Business Case Project Evaluation",
    "Pre-Full Business Case Project Evaluation\n\nPrioritised programmes where key projects are expected to submit a Full Business Case in the next three years. These will provide lessons for other programmes and decision makers about the design of programmes and the route to Full Business Case Approval.\n\n# Project Summary",
    "|Armed Forces Recruitment Programme.|Tri-Service programme responsible for delivering a single, common Tri-Service Recruiting Operating Model for the Armed Forces.|\n|||\n|Collective Training Transformation Programme.|The Collective Training Transformation Programme seeks to transform both the management and delivery of training and the experience for those going through training.|",
    "|E7. The Wedgetail AEW Mk1 Programme|will provide a 5th generation Airborne Early Warning and Control (AEW&C) capability, with a Multi-role Electronically Scanned Array sensor, that is interoperable and interchangeable with key allies to an anticipated Out-of-Service date of at least 2042.|",
    "|Ground Based Air Defence (GBAD).|The Land Ground Based Air Defence programme will modernise the Army's air defence capabilities in the face of rapidly developing threats and is a very high priority for the Army. This includes the development of an integrated layered air defence system comprising Counter-Small Aerial Target, Short Range Air Defence and Medium Range Air Defence capabilities, and technology that enables early warning and engagement of targets at optimum range, plus multi domain integration.|",
    "|Joint Crypt Key Programme.|Description Exempt under Section 26 of the Freedom of Information Act 2000 (Defence)|\n|Mine Hunter Programme.|The Mine Hunting Capability Programme will provide an agile, interoperable, and survivable capability using emerging Maritime Autonomous Systems.|",
    "|SKYNET 6 Integrated Enterprise Solution.|This project has been instigated to explore future collaborative arrangements to improve the delivery and integration of the future SKYNET programme. SKIES will explore collaborative arrangements with industry and more effective ways of working. This will enable better decision making in areas including route to market, and the integration and exploitation of future technology and innovation, whilst making more efficient and effective use of available resources.|",
    "# Post-Full Business Case Project Evaluation\n\nPrioritised programmes with key delivery milestones within the next three years. This evaluation will help determine whether objectives were met and what lessons can be learned through delivery.\n\n# Project Summary",
    "|A400M|The A400M Programme is intended to deliver and support ATLAS aircraft into service.|\n|||\n|Armoured Cavalry|The aim of the programme is to deliver by 2025, a versatile and agile multi-role capability, operating at the heart of the Deep Reconnaissance Strike and Heavy Combat Brigades. Enabling success on current and future operations in the most complex and demanding operational environments.|",
    "|Armour Main Battle Tank|The Armour Main Battle Tank (MBT) Programme will deliver the Army's new Main Battle Tank capability as part of a balanced force; credible and employable against current and emerging threats until an out of service date out to at least 2040. The programme will address obsolescence as well as improve lethality and survivability. Mobility is being addressed through a separate pan-platform project.|",
    "|Astute Boats 1-7|To deliver the seven Boat Astute Class within approved performance, cost and time parameters, while actively contributing to the sustainment of the UK submarine design and manufacturing capability.|\n|Crowsnest Programme|Crowsnest is a critical enabler for the strategic Carrier Enabled Power Projection Programme. It provides an organic Airborne Surveillance and Control capability as role fit to the Anti-Submarine Merlin Mk2 helicopter.|",
    "|Defence Estate Optimisation|Defence Estates Optimisation is a long-term investment to modernise the defence estate. It is an ambitious 25-year portfolio of construction activity, unit and personnel moves, and site disposals that will deliver a better structured, more economical and modern estate that more effectively supports military capability.|\n|Fleet Solid Support|Fleet Solid Support will provide Auxiliary Shipping for stores, ammunition, and food sustainment to Naval Forces at Sea.|",
    "|Land Environment Tactical Communication and Information System|The Land Environment Tactical Communication and Information System programme is a military capability and business change programme that will deliver the Land Domain's deployed digital backbone in support of tactical (front line) users in the Army, Royal Marine, RAF Regiment and RAF Deployed Operating Bases. It will be achieved through the ongoing sustainment, evolution or replacement of Communication Information Systems and associated applications to underpin the transition to a Single Information Environment for users across Defence.|",
    "|Lightning Programme|The F-35 provides the UK with a survivable, sustainable, expeditionary, Fifth Generation air capability in order to contribute to the widest possible range of operations.|",
    "# MARSHALL\n\nMarshall enables military terminal air traffic management services in the UK and abroad. Marshall combines some 70+ previous equipment and support contracts into a single service delivery contract. It is delivered through 15 technical services and replaces previous arrangements with a regionalised support model, supporting hub and satellite geographically clustered services.\n\n# Mechanised Infantry Vehicle",
    "# Mechanised Infantry Vehicle\n\nThe Mechanised Infantry Programme will deliver modern wheeled Armoured Personnel Carriers that can perform a range of roles to support the Infantry, Combat Support, and Combat Service Support elements across new Brigade Combat Teams - a new concept emerging from the Integrated Review. The vehicles will significantly contribute to enabling a highly deployable, networked force to operate differently from conventional industrial age combat forces. They will offer a unique competitive advantage whether fighting, peacekeeping or delivering humanitarian aid.",
    "# New Accommodation Offer\n\nA new accommodation policy for the Armed Forces is due to be launched in financial year 2023/2024. The policy will widen entitlement to subsidised military accommodation for Service Personnel who are in long-term relationships and/or with children resident over 80 nights a year. Accommodation will be provided based on need (number of dependent children), rather than rank and there will be more accommodation options for single Service Personnel.\n\n# PROTECTOR",
    "# PROTECTOR\n\nProtector will provide a certified remotely piloted air system with enhanced capabilities (to 2040) over those currently provided by the in-service Reaper air system.\n\n# Type 26 Global Combat Ship Programme",
    "Type 26 Global Combat Ship Programme will procure 8 x Anti-Submarine Warfare ships and associated support. This will deliver an interoperable, survivable, available and adaptable capability that is operable globally within the maritime battle space. This ships contribute to sea control for the Joint Force and maritime force projection and Joint Force command and control; providing the flexibility to operate across and within the range and scale of Contingent and non-Contingent operations. This 8-ship programme will",
    "scale of Contingent and non-Contingent operations. This 8-ship programme will deliver Anti-Submarine Warfare capability to protect strategic assets, sustain national shipbuilding capability and increase resilience of the Naval Service.",
    "# Type 31",
    "The Type 31 general purpose frigate programme is designed to deliver a general-purpose frigate capability and act as the pathfinder programme for the National Shipbuilding Strategy. From the mid-2020s, Type 31 will be at the heart of the Royal Navy's surface fleet, deterring aggression and maintaining the security of the UK's interests. They will work alongside our Allies to deliver a credible UK warship presence across the globe. Flexible and adaptable by design, Type 31",
    "deliver a credible UK warship presence across the globe. Flexible and adaptable by design, Type 31 frigates will undertake missions such as interception and disruption of those using the sea for unlawful purposes, collecting intelligence, conducting defence engagement and assisting those in need.",
    "# ONS Evaluation Strategy\n\nThe ONS's vision for evaluation, embedding best practice and supporting\nevaluations across government through the Analysis Function and\nIntegrated Data Service.\n\nIn this section\n\n1.  [[Foreword]{.underline}](https://www.ons.gov.uk/aboutus/transparencyandgovernance/onsevaluationstrategy#foreword)",
    "2.  [[Executive\n    summary]{.underline}](https://www.ons.gov.uk/aboutus/transparencyandgovernance/onsevaluationstrategy#executive-summary)\n\n3.  [[Introduction]{.underline}](https://www.ons.gov.uk/aboutus/transparencyandgovernance/onsevaluationstrategy#introduction)",
    "4.  [[Implementing the monitoring and evaluation\n    strategy]{.underline}](https://www.ons.gov.uk/aboutus/transparencyandgovernance/onsevaluationstrategy#implementing-the-monitoring-and-evaluation-strategy)\n\n5.  [[Next\n    steps]{.underline}](https://www.ons.gov.uk/aboutus/transparencyandgovernance/onsevaluationstrategy#next-steps)",
    "6.  [[ONS organisational theory of\n    change]{.underline}](https://www.ons.gov.uk/aboutus/transparencyandgovernance/onsevaluationstrategy#ons-organisational-theory-of-change)\n\n**1.Foreword**",
    "The UK Statistics Authority strategy, Statistics for the Public Good,\nsets out the need for high quality data to inform the UK, improve lives\nand build for the future. As set out in the strategy, the data\nrevolution continues at pace and there is a big prize for the\nstatistical system, the Civil Service and the UK -- if statisticians and\nanalysts have access to the best evidence and can communicate\neffectively, helping to inform the country while reducing the potential",
    "effectively, helping to inform the country while reducing the potential\nfor misrepresentation.",
    "The Office for National Statistics (ONS) has a lead role to play in this\nrevolution. It benefits from the statutory independence of the UK\nStatistics Authority, supported by the Code of Practice for Statistics,\nkey ethical principles and its ability to convene experts and analytical\nresources. The ONS also produces important reference statistics that\nshow how the country is changing. We are responsible for important\naspects of the Digital Economy Act, which provides access to data in",
    "aspects of the Digital Economy Act, which provides access to data in\nsupport of its remit from across the UK. The ONS also has an important\nrole in ensuring that the country\\'s evidence base is inclusive and\nreflective of the full characteristics of the UK.",
    "The development of effective programmes to deliver the UK Statistics\nAuthority strategy depends on timely and accurate monitoring and\nevaluation to understand and assess progress against objectives, and\nunderstand what difference is made and whether it is value for money.\nWithout continuous monitoring and evaluation, it is not possible to\nunderstand how far programmes are achieving their goals, nor generate\nthe evidence needed to understand successes and learn from failures to\nimprove the future delivery of our statistics.",
    "This strategy reflects our strong commitment to maintaining and\ndeveloping a robust evidence base across the ONS, providing a clear\npathway to embedding monitoring and evaluation throughout the\norganisation. It also recognises our contribution to the\ncross-government evaluation landscape through the leadership of the\nAnalysis Function and development of the Integrated Data Service, which\nwill provide a secure, integrated data system, allowing analysts from\nacross government access to monitor, evaluate and help design policy.\n\nProfessor Sir Ian Diamond",
    "Professor Sir Ian Diamond\n\nNational Statistician UK Statistics Authority\n\n[[Back to table of\ncontents]{.underline}](https://www.ons.gov.uk/aboutus/transparencyandgovernance/onsevaluationstrategy#toc)\n\n# 2.Executive summary",
    "Our vision of the future of evaluation at the Office for National\nStatistics (ONS) and across government is one of inclusive learning,\nradical improvements, and ambitious and sustainable programmes. It is of\na future where analysts have the capability to deliver robust\nevaluation, there is clear ownership of standards for evaluation, and\nevaluation is built into departmental analysis practices. All projects,\nprogrammes, and workstreams will be evidence based and continually\nimproved based on their evaluations.",
    "We must overcome fundamental barriers, including:\n\n-   the resourcing of evaluations\n\n-   technical barriers associated with the timing of evaluations and the\n    impact on their scope to influence programme design\n\n-   cultural barriers to ensure a positive and open approach to\n    introducing evaluation practices and publishing reports\n\nThis document outlines the ONS evaluation strategy, across four pillars\nof activity, to address these barriers faced by the ONS and wider\ngovernment to achieving our vision.",
    "**The four pillars**\n\n-   Pillar one: developing the hub and spoke model for evaluation at the\n    ONS.\u00a0\n\n-   Pillar two: embedding evaluation into intervention design and\n    governance processes\u00a0at the ONS.\n\n-   Pillar three: facilitating a culture of continual learning among\n    colleagues at the ONS.\u00a0\n\n-   Pillar four: the role of the Analysis Function and Integrated Data\n    Service in cross-government evaluation.",
    "**Pillar one: developing the hub and spoke model for evaluation at the\nONS**\n\nThis pillar in our strategy is central to our ambitious and sustainable\nprogrammes to ensure we are using evidence from previous programmes to\nsupport new project and programme design.\n\nWe will continue to develop the hub and spoke model to strengthen\nevaluation capability across the organisation and deliver an\nincreasingly coordinated and coherent approach to stakeholder and user\nengagement for evaluation purposes.",
    "**Pillar two: embedding evaluation into intervention design and\ngovernance processes**\n\nTo ensure that our programmes and workstreams are evidence based and\ncontinually improved, we need to embed evaluation within the design and\ndelivery of projects and programmes from the initiation stage. This is\nbecause the scope to influence programme design and to incorporate and\nadequately fund evaluations reduces significantly if evaluation is not\nincluded from the outset.",
    "We will strengthen the evaluation governance that has been established\nwithin existing processes to inform future learning and decision making.\nThe development of the ONS-wide Theory of Change will provide a\nframework for all projects and programmes to understand their\noverarching impacts, align themselves to the ONS strategy and allow us\nto identify potential opportunities to centrally coordinate research\nthat demonstrates those impacts. The development of regular dashboard\nreporting will provide critical insight to inform decision making and",
    "reporting will provide critical insight to inform decision making and\nsupport the creation of the annual impact report.",
    "Publishing evaluation reports in line with our evaluation workplan will\nensure that we are transparent. It will also support our commitment to\nthe production of evaluations that include robust methodological designs\nfor evaluating public goods and non-policy programmes.\n\n**Pillar three: facilitating a culture of continual learning among\ncolleagues at the ONS**",
    "We will continue to develop an inclusive learning environment to upskill\ncolleagues, raising the level of evaluation capability at the ONS to\nachieve radical improvements to the evaluations of our programmes.\nBuilding upon our existing channels, we will seek to facilitate a\nculture of continual learning and champion the value of evaluation\nacross the organisation.\n\n**Pillar four: the Analysis Function and Integrated Data Service**",
    "The ONS will contribute to the cross-government evaluation landscape\nthrough the leadership of the Analysis Function. The Analysis Function\nleads on capability building through knowledge sharing and development\nof guidance, partnering with departments with less-mature evaluation\nfunctions to improve capabilities, and providing oversight and direction\nof The Magenta Book. In addition, the Integrated Data Service will\nprovide a secure, integrated data system, allowing analysts from across\ngovernment to monitor, evaluate and share evaluation techniques and",
    "government to monitor, evaluate and share evaluation techniques and\noutcomes.",
    "The strategy, and progress against it, will be reviewed at regular\nintervals.\n\n[[Back to table of\ncontents]{.underline}](https://www.ons.gov.uk/aboutus/transparencyandgovernance/onsevaluationstrategy#toc)\n\n# 3.Introduction\n\n**The UK statistics system and architecture of evaluation**",
    "The UK Statistics Authority is an independent body at arm\\'s length from\ngovernment, which reports directly to the UK Parliament, the Scottish\nParliament, the Welsh Parliament and the Northern Ireland Assembly.\nThe\u00a0Statistics and Registration Service Act 2007\u00a0(SRSA) established the\nAuthority with the statutory objective of \\\"promoting and safeguarding\nthe production and publication of official statistics that serve the\npublic good\\\". The public good includes:",
    "-   informing the public about social and economic matters\n\n-   assisting in the development and evaluation of public policy\n\n-   regulating quality and publicly challenging the misuse of statistics",
    "Our strategy\u00a0[[Statistics for the public good (PDF, 1.09\nMB)]{.underline}](https://intranet.ons.statistics.gov.uk/wp-content/uploads/2020/07/1342-UKSA-Strategy-2020-v1-00.pdf)\u00a0is\nan ambitious call to action. It sets out our aims, priorities, mission\nand values to make the case for change. This strategy covers the",
    "and values to make the case for change. This strategy covers the\nprincipal elements of the UK official statistics system for which the\nAuthority has oversight, including the Government Statistical Service\n(GSS), the Office for National Statistics (ONS) and the Office for\nStatistics Regulation (OSR).",
    "The ONS is the Authority\\'s statistical production function and is part\nof the GSS. Led by the National Statistician, the ONS is the UK\\'s\ninternationally recognised National Statistical Institute and largest\nproducer of official statistics. The ONS produces data, statistics and\nanalysis on a range of key economic, social and demographic topics.",
    "This evaluation strategy outlines the ONS\\'s vision and approach to\nembedding best practice evaluation as well as its pivotal role in\nsupporting evaluations across government through the Analysis Function,\nIntegrated Data Service and provision of data.\n\n**Context**",
    "Evaluation has been established as part of the policy cycle for some\ntime as a means of collecting evidence to understand the effectiveness\nof an intervention and the outcomes it achieves. Several recent\ndevelopments have highlighted the need\nfor\u00a0improving\u00a0evaluation\u00a0capability\u00a0in government, including the\nrecent\u00a0[[National Audit Office review of government provision and use of\nevaluation]{.underline}](https://www.nao.org.uk/report/evaluating-government-spending/),",
    "which\u00a0 included a focus on the Analysis Function\\'s role in integrating\nquality evaluation into policy development, and the associated Public\nAccounts Committee inquiry.",
    "There has also been a substantial movement within government to embed\nevaluation into policy making, for example through the requirements for\nclear evaluation plans and evidence bases associated with Spending\nReview bid settlement letters and in departmental Outcome Delivery\nPlans, and through increasing emphasis on improving transparency of\nevaluation outcomes. To enable the delivery of this ambitious agenda,\nthe Evaluation Taskforce and Analysis Function Evaluation Support team\nwere established. These teams collaborate with existing networks such as",
    "were established. These teams collaborate with existing networks such as\nthe Cross-Government Evaluation Group to enable a positive change in how\nevaluation is delivered, built into policy making, and open to scrutiny\nfrom the public.",
    "The ONS\\'s Central Evaluation Function was set up to address the\nchanging landscape, meet central government requirements of the ONS with\nevaluation, and increase the profile and importance of timely and robust\nevaluations within the department.\n\n**Our vision**",
    "**Our vision**\n\nOur vision of the future of evaluation at the ONS and across government\nis one of inclusive learning, radical improvements, and ambitious and\nsustainable programmes. It is of a future where analysts have the\ncapability to deliver robust evaluation, there is clear ownership of\nstandards for evaluation, and evaluation is built into departmental\nanalysis practices. All projects, programmes, and workstreams will be\nevidence based and continually improved based on their evaluations.",
    "Impact evaluation is central to improving the programmes we deliver at\nthe ONS by facilitating collective learning and accountability.\u00a0 Impact\nevaluations are now also a requirement by the Cabinet Office and HM\nTreasury for all large expenditure. The two key aims of delivering\nevaluations are:",
    "-   learning --evaluations can provide the evidence with which to manage\n    risk and uncertainty, especially in areas that are breaking new\n    ground; early learning can also illuminate what works and what does\n    not, and how this can be improved.",
    "-   accountability --government makes decisions on the public\\'s behalf\n    and spends tax collected from individuals and businesses (as it has\n    a responsibility to maximise public value and outcomes, and ensure\n    policies are effective), and these decisions should be evidence\n    based.",
    "Evaluation is defined by\u00a0[[The Magenta\nBook]{.underline}](https://www.gov.uk/government/publications/the-magenta-book)\u00a0as\n\\\"a systematic assessment of the design, implementation and outcomes of\nan intervention.\\\" Delivering a successful evaluation, or one that is\nfit for purpose, involves understanding how an intervention is being, or\nhas been, implemented and what effects it has, for whom, and why. It",
    "has been, implemented and what effects it has, for whom, and why. It\nidentifies what can be improved and estimates its overall impacts and\ncost-effectiveness.",
    "**The importance of monitoring and evaluation**",
    "[[The Green Book (PDF, 1.45\nMB)]{.underline}](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1063330/Green_Book_2022.pdf)\u00a0states\nthat monitoring and evaluation are \\\"approved thinking models and\nmethods to support the provision of advice to clarify the social or\npublic welfare costs, benefits, and trade-offs of alternative\nimplementation options for the delivery of policy objectives\\\".",
    "implementation options for the delivery of policy objectives\\\".\nEvaluation is important as it provides \\\"objective analysis to support\ndecision making\\\". The Analysis Function Standard more widely sets\nexpectations for the planning and undertaking of analysis across\ngovernment and provides a high-level overview of the role of appraisal\nand evaluation within this.",
    "Critical to understanding why the ONS has prioritised monitoring and\nevaluation, Sir Ian Diamond, the National Statistician and Head of the\nAnalysis Function Board notes in his foreword to The Magenta Book that:\n\\\"High quality monitoring and robust evaluation of our programmes and\nprojects will provide the evidence that the data and analysis can enable\ndecision-makers to better target their intervention; reduce delivery\nrisk; maximise the chance of achieving the desired objectives; and",
    "risk; maximise the chance of achieving the desired objectives; and\nincrease understanding of what works\\\".",
    "Sir Ian Diamond added that \\\"without robust, defensible evaluation\nevidence, government cannot know whether interventions are effective or\neven if they deliver any value at all. Routine, high-quality evaluation\nis part of a culture of continual improvement and should be core to the\nwork of all government departments\\\".",
    "Monitoring and evaluating programmes, projects and policies will also\nassist the ONS in achieving its organisational strategy, which is to\nproduce data that will inform the UK, improve lives and build the future\n(Statistics for the public good: 2020 to 2025).",
    "A comprehensive monitoring and evaluation framework will help the ONS\ntrack its progress towards the delivery of transformation, assess the\nplan outcomes, evaluate the public good impact (planned, unplanned,\npositive or negative) and offer learnings.\n\n**Monitoring and evaluation principles**",
    "**Monitoring and evaluation principles**\n\nThe ONS principles for delivering fit-for-purpose evaluations are those\noutlined in\u00a0[[The Magenta\nBook]{.underline}](https://www.gov.uk/government/publications/the-magenta-book).\nFor an evaluation to be \\\"fit for purpose\\\", it should be:\n\n-   useful --designed to meet the needs of the many stakeholders\n    involved and produce usable outputs at the right point in time",
    "-   credible --ensuring transparency and objectivity\n\n-   robust -- that means well-designed, with an appropriate evaluation\n    approach and methods, and well executed, as well as using\n    independent peer review and independent steering to help\n    quality-assure the design and execution of an evaluation where\n    possible\n\n-   proportionate --using a criteria-based approach to determine the\n    appropriate level of monitoring and evaluation",
    "**Case study on proportionate evaluation approaches used in the ONS: the\nAnalytical Hub\u00a0**\n\nThe Analytical Hub was set up to bring analysts together from across the\nONS to provide a centre of multi-disciplinary analytical and\nmethodological capability dedicated to cross-cutting analysis.\u00a0\n\nIts main functions are to:",
    "Its main functions are to:\u00a0\n\n-   deliver towards the ONS Strategic Business Plan\\'s ambition to\n    deliver a \\\"radically increased level of cross-theme analysis that\n    cuts across government and societal boundaries\\\"\u00a0\n\n-   respond flexibly and timely to the uncertainties and challenges of\n    coronavirus (COVID-19), the nation\\'s recovery from it and the\n    ongoing economic and public policy priorities of the day\u200b",
    "-   address cross-cutting issues and produce analysis to inform and\n    engage governments, policy makers and the wider public\u200b\u00a0\n\nThe purpose of the evaluation is:\u00a0\n\n-   to understand the impacts of our work, and to evaluate to what\n    extent ONS statistics support users to make better decisions for the\n    public good\u00a0\n\n-   to share lessons learnt and continuously improve our ways of\n    working",
    "We will gather evidence \u00a0by building case studies and using qualitative\ninterviews, surveys, sources of user engagement and management\ninformation relating to volume of queries, outputs and associated\nresponse times. We are using contribution analysis to understand the\nlikelihood that the intervention has contributed to outcomes identified\nwithin the Theory of Change. The evaluation is being resourced\ninternally including the use of ONS Evaluation Champions rather than\ncommissioning external evaluators.",
    "The production of the first annual evaluation report is forthcoming.\n\n[[Back to table of\ncontents]{.underline}](https://www.ons.gov.uk/aboutus/transparencyandgovernance/onsevaluationstrategy#toc)\n\n**4.Implementing the monitoring and evaluation strategy**\n\n**Establishing monitoring and evaluation at the Office for National\nStatistics (ONS)**",
    "**Establishing monitoring and evaluation at the Office for National\nStatistics (ONS)**\n\nThe Central Evaluation Function was created in October 2020, following\nthe 2020 Spending Review. Activities that have taken place since then\nwithin the evaluation space have enabled us to make significant progress\ntowards overcoming fundamental, technical and cultural barriers to\nevaluation, including:\n\n-   establishing the hub and spoke model to support the resourcing of\n    evaluations",
    "-   establishing the hub and spoke model to support the resourcing of\n    evaluations\n\n-   establishing evaluation governance and integrating evaluation best\n    practise into business case development to address issues with the\n    timing of evaluations, increasing the scope to influence project and\n    programme design, and develop robust methodologies from the outset\n\n-   leveraging senior management support to communicate the importance\n    of evaluation across the ONS and establish the Evaluation Champions\n    Network",
    "The implementation of the ONS evaluation strategy will be critical to\nbuilding upon that progress and embedding evaluation across the\norganisation.\n\n**Pillar one: developing the hub and spoke model for evaluation at the\nONS**\n\n**The hub and spoke model**\n\nThe ONS operates a hub and spoke model for evaluation capability where\nguidance and support are provided by the hub while evaluation activities\nare performed in the spokes.",
    "Following the Spending Review 2021 (SR21), the National Statistics\nExecutive Group (NSEG) approved plans to allocate a minimum of 1% of a\nprogramme's budget to evaluation. This currently enables the direct\nresourcing of evaluation spokes within the programmes and will support\ndevelopment of the existing hub over time to enable the organisation to\ndeliver more effectively and efficiently on the main commitments that\nsupport the ONS vision for evaluation.",
    "Programmes at the ONS experience a shared challenge in measuring the\nimpact of our statistics and analysis and the need for a coordinated and\ncoherent approach to stakeholder and user engagement for evaluation\npurposes. Plans to develop the capability of the hub will deliver\nagainst these needs, including:\n\n-   coordinating reputational research\n\n-   joint procurement exercises\n\n-   creating a dataset of evaluation activity\n\n-   upskilling colleagues through training and guidance\n\n-   supporting best practice",
    "-   upskilling colleagues through training and guidance\n\n-   supporting best practice\n\n-   working with a broader portfolio of projects and programmes\n\n-   maintaining a strong Evaluation Champions Network and stakeholder\n    relations with other departments, the Cross-Government Evaluation\n    Group, and the Evaluation Taskforce\n\n**Pillar two: embedding evaluation into intervention design and\ngovernance processes at the ONS**\n\n**Creating strategic alignment with the ONS-wide Theory of Change**",
    "**Creating strategic alignment with the ONS-wide Theory of Change**\n\nAn Organisational Theory of Change (OToC) will be developed to support\nthe alignment of programme monitoring and evaluation plans at an\norganisational level.\u00a0The OToC will highlight our organisational-wide\nimpacts and the metrics we will use to measure performance against the\nstrategic objectives, and it will form the basis of our impact report\nand impact dashboard.",
    "We have also recently established a Task and Finish Group with the aim\nof developing the impact dashboard, which will support learning and\ndecision making at different levels across the organisation. We will use\nthe dashboard, alongside programme evaluation reports, to inform the\nfuture ONS annual impact report.\n\n**Alignment of evaluation planning with investment appraisal and\ngovernance**",
    "**Alignment of evaluation planning with investment appraisal and\ngovernance**\n\nThe planning of monitoring and evaluation for spending proposals should\nfollow Her Majesty\\'s Treasury Better Business Case guidance for both\nprogrammes and projects, and this guidance has been integrated into\nbusiness case development at the ONS. This allows us to:\n\n-   use a wide range of analytical and logical thinking tools when\n    considering potential solutions of intervention",
    "-   bring evaluation to the forefront of programme and project design\n\n-   determine a proportionate approach and accountability for delivery\n    of the evaluation\n\nEvaluation best practise is integrated into ONS business case templates\nand ONS programme and project management lifecycle guidance, aligned to\nthe Her Majesty\\'s Treasury five case methodology and Magenta Book\nguidance on evaluation planning expected at each appraisal stage:\nstrategic outline business case, outline business case and full business\ncase.",
    "Practices outlined within the ONS programme evaluation cycle section\nnaturally support the development of the business case; namely, the\nTheory of Change, which provides an important foundation and focus for\ndevelopment of the strategic case. Subsequently, the detailed monitoring\nand evaluation framework requirements are integrated into the management\ncase as programmes and projects develop their management arrangements\nfrom outline business case through to full business case.",
    "The size and complexity of new projects or programmes determine the\nevaluation requirements and the level of service to be provided by the\nCentral Evaluation Function.\n\nMonitoring and evaluation plans (including Theory of Change) are\napproved by the Programme Evaluation Group as part of the ONS investment\napproval process.\n\n**Resourcing evaluations**",
    "Planning and provision of resources for monitoring and evaluation should\nbe proportionate when judged against the costs, benefits and risks of a\nproposal both to society and the public sector. Prescriptive guidance on\nhow much to budget for an evaluation\u00a0is difficult\u00a0given the varying\ndegrees of existing evidence and sizeof different programmes. Following\nthe Spending Review 2021 (SR21), the National Statistics Executive Group",
    "the Spending Review 2021 (SR21), the National Statistics Executive Group\n(NSEG) approved plans to allocate a minimum of 1% of a programme's\nbudget to evaluation.",
    "**Overview of the ONS programme evaluation cycle**\n\nThe ONS programme evaluation cycle is comprised of three main phases -\nbusiness case development, evaluation set up and monitoring - each of\nwhich is described in more detail in the following sections. The Central\nEvaluation Function currently supports programmes to undertake the\nactivities and develop the relevant products within each of these\nphases.",
    "Programme management approaches are essential to successful delivery of\nchange, benefits and outputs within the ONS, and ensuring that value for\nmoney is achieved. ONS programmes follow a standard lifecycle based on\nthe government\\'s project delivery functional standard definition of\nphases:\n\n1.  Identifying\n\n2.  Initiating\n\n3.  Managing\n\n4.  Closing\n\n5.  Reviewing",
    "3.  Managing\n\n4.  Closing\n\n5.  Reviewing\n\nThe ONS programme evaluation cycle has been aligned to the programme\nmanagement lifecycle to establish evaluation best practise within\nexisting programme design and governance processes.",
    "The initial focus of the Central Evaluation Function is to support\nportfolio programmes with impact evaluations. Impact evaluation focuses\non \\\"what difference has an intervention made\\\", or the changes caused\nby an intervention. These will be measurable achievements that either\nare, or contribute to, the objectives of the intervention.",
    "The next iteration of the ONS evaluation strategy will explore the use\nof process evaluation (\\\"What can be learned from how the intervention\nwas delivered?\\\") and value for money evaluation (\\\"Is this intervention\na good use of resources?\\\") in greater detail.\n\n**Business case development**\n\n**Stakeholder mapping**",
    "**Business case development**\n\n**Stakeholder mapping**\n\nStakeholder maps are used to identify all the teams or individuals\ninteracting with a project or programme, and they help with planning\nstakeholder management strategies. A stakeholder map grades stakeholders\nby their importance to the programme and recommends how to manage them\naccording to their influence over and interest in the work. Influence\ndiagrams can also be used.\n\n**Influence diagram**",
    "**Influence diagram**\n\nInfluence diagrams show the key elements within an intervention, how\nthey influence one another, and the dependencies. For example, one key\naspect of a programme might be the working relationship between the\nprogramme team and the customers. A second aspect might be the funding\navailable for the programme. The second may influence the first by\naffecting the resource available to dedicate to the relationship between\nthe programme team and the customers.",
    "Within any intervention, there are multiple elements or aspects, which\ncan affect one another in many different and complex ways. Influence\ndiagrams aim to capture all these in one place to inform the delivery of\na project or programme.\n\n**Theory of Change**",
    "**Theory of Change**\n\nA Theory of Change is a framework that outlines why a project or\nprogramme is needed, what it aims to achieve, and how it will achieve\nits aims. In more detail, it shows why the chosen approach will be\neffective and how change happens in the short, medium, and long term to\nachieve the intended aims. It is a project or programme\\'s \\\"theory\\\"\nabout how their work will change the current status.",
    "A Theory of Change draws on existing literature and past interventions\nto predict the outcomes and impacts that a piece of work will have, as\nwell as capturing the \\\"need\\\" or rationale for the piece of work by\nassessing the current status of the situation. This clarity of thought\nis crucial for not only the evaluation but the programme as a whole.\n\n**Logic framework**",
    "A log frame (logical framework) is an important document that provides\nan overview of a project or programme. Logic frameworks at the ONS\ninclude the aims, measures of success, stakeholders, risks, assumptions\nand dependencies, and a timeline for delivery. The framework operates as\na matrix so that teams can identify how the planned activities, aims and\nimpacts map onto plans for stakeholders, measures of success, strategic\ngoals, and programme timelines. This provides the foundation for the",
    "goals, and programme timelines. This provides the foundation for the\ndelivery workplan for a project or programme.",
    "**Evaluation set up**\n\n**Monitoring and evaluation framework**",
    "The monitoring and evaluation framework identifies the indicators\nrequired to monitor the progress of an intervention and evaluate its\nimpact. Its purpose is to support programme managers in continuously\nmonitoring the results of the intervention, making informed decisions at\nimportant points in the delivery of the intervention, based on timely\ndata, and providing effective departmental reporting on the progress of\nthe intervention. It will also ensure that the information that will\nsupport the final evaluation is being gathered at the appropriate times.",
    "Each output and outcome (immediate, intermediate, and final) identified\nin the Theory of Change should have a corresponding performance\nindicator that should be used for day-to-day programme monitoring as\nwell as for evaluation purposes.\n\nThe list of important performance indicators should be relatively brief\nto remain proportional and relevant. The purpose of these data should be\nexplained to those collecting the data required and monitoring the\nindicators.\n\nThere are two types of indicators:",
    "There are two types of indicators:\n\n-   quantitative performance indicators are made up of a number and a\n    unit --examples include \\\"number of mentions in academic\n    literature\\\" or \\\"number of people accessing a website\\\"\u00a0\n\n-   qualitative indicators represent qualitative assessments (for\n    example, \\\"excellent\\\", \\\"average\\\", \\\"below average\\\") --these\n    should stay consistent over time to allow for comparability\n\nThe criteria for good important performance indicators are:",
    "The criteria for good important performance indicators are:\n\n-   reliability --would the data collected be the same if collected\n    repeatedly under the same conditions and at the same point in time?\n\n-   affordability --is the data collection cost-effective?)\n\n-   availability - are the data necessary for the indicator readily\n    available for multiple collections?\n\n-   relevance -- does the indicator clearly track back to the output\n    and/or outcomes of the intervention?",
    "Performance data can only be used for monitoring and evaluation if there\nis something to which the data can be compared. The baseline serves as\nthe starting point for comparison while the target serves as the end\npoint for comparison. Thus, the baseline and the target allow us to\nassess the contribution of the intervention. This highlights the\nimportance of developing a monitoring and evaluation plan during project\ninitiation to ensure that a baseline can be gathered before the start of",
    "initiation to ensure that a baseline can be gathered before the start of\nany activities.",
    "Just like the important performance indicators, the baseline data and\ntarget data can either be quantitative or qualitative. While identifying\nthe important performance indicators, the sources that will be used for\nthe data collection can be identified.\n\nPossible data sources include:\n\n-   administrative data\n\n-   secondary data (information that has been collected for other\n    purposes)",
    "-   secondary data (information that has been collected for other\n    purposes)\n\n-   primary performance data (data obtained through collection exercises\n    tailored to the intervention, such as stakeholder focus groups or\n    stakeholder surveys)\n\nWhen assigning the responsibility of data collection, it is important to\nanswer the following questions:\n\n-   Will the data collection process be conducted by an internal or\n    external team or individual?",
    "-   If the responsible party will be internal, which parties have the\n    easiest access to the data sources identified?\n\n-   What systems need to be put in place to facilitate data collection?\n\n**Monitoring**\n\n**Mid-term evaluation**",
    "**Monitoring**\n\n**Mid-term evaluation**\n\nAt the programme planning stage, you should have identified a suitable\npoint to hold a mid-term evaluation. A mid-term evaluation is usually\neither mid-way through the duration of the programme or mid-way through\nthe completion of your important milestones, whichever is most\nappropriate to your area of work to check whether the programme is on\ncourse to deliver the expected impacts.",
    "**Case study: mid-term evaluation of the COVID-19 Infection Surveillance\nProgramme\u00a0**",
    "The emergence of the coronavirus (COVID-19) pandemic in early 2020 led\nto the Department of Health and Social Care (DHSC) commissioning the\nOffice for National Statistics (ONS) to develop and deliver a UK-wide\nsurveillance programme of COVID-19 infection at pace. Outputs from the\nCOVID-19 Infection Survey (CIS) and Schools Infection Survey (SIS),\nalongside wider activities, formed valuable sources of information that",
    "alongside wider activities, formed valuable sources of information that\nhave helped the Government make informed decisions about how to manage\nthe coronavirus pandemic.",
    "The programme developed a Theory of Change that identified important\noutcomes and impacts ultimately aligned to the production of\n\\\"Statistics for the Public Good\\\" and a monitoring and evaluation\nframework that supported their measurement.",
    "The mid-term evaluation focused on the ONS\\'s role in the programme,\nwhich was informed by internal data gathering, analysis and research\ncarried out with delivery staff and study participants, social media\nactivity and media interest. These sources of evidence were used to\nprovide a high-level test of the Theory of Change that was developed for\nthe programme, with the intention that an external evaluation would\nfully test the anticipated outcomes and impacts of ONS\\'s role in the",
    "fully test the anticipated outcomes and impacts of ONS\\'s role in the\nprogramme.",
    "Recommendations from the mid-term evaluation were presented around the\nfollowing themes:\n\n-   programme resourcing\n\n-   upskilling staff through partnerships\n\n-   stakeholder analysis and engagement\n\n-   communications\n\n-   setting tolerance levels for deviating from standard survey\n    processes\n\n-   the development of a playbook for future programmes in response to\n    health emergencies",
    "-   the development of a playbook for future programmes in response to\n    health emergencies\u00a0\n\nA full independent external evaluation of the ONS\\'s role in the\nCOVID-19 Infection Surveillance Programme is also planned.\n\n**End-term evaluation**\n\nA final evaluation is usually conducted once a project or programme has\ncompleted, or afterwards, depending on when you can reasonably expect\nyour outcomes and impacts to be realised.\n\n**Dissemination of evaluation results**",
    "All evaluations conducted at the ONS will be published and their\noutcomes will be stored centrally in a \\\"lessons learned\\\" log to be\nused as evidence of what works for new programmes in the future. All\nprojects and programmes are encouraged to further disseminate their\nevaluation reports and findings via communications with their\nstakeholders, colleagues, and at conferences. All ONS mid-term and final",
    "stakeholders, colleagues, and at conferences. All ONS mid-term and final\nevaluations will be published on the ONS\\'s website and on GOV.UK via\nthe Evaluation Task Force, where appropriate and not considered\nsensitive.",
    "**Publication**",
    "Final evaluation reports should be completed within three months of\nprogramme closure, and reports and associated data sources should be\npublished within six months of programme closure. The ONS maintains an\nopen, honest, transparent approach to publication of evaluation findings\naligned to\u00a0[[the Government Social Research Publishing\nProtocol]{.underline}](https://www.gov.uk/government/publications/government-social-research-publication-protocols),\nwhich presents principles for the publication of all government social",
    "which presents principles for the publication of all government social\nresearch. This includes outputs from the evaluation of policy and\ndelivery initiatives, and pilots and trials.",
    "Programmes will publish evaluation reports on the ONS website with the\nsupport of the publishing, content and design teams, aligned to the\ntimings within the ONS evaluation workplan.\n\n**Pillar three: facilitating a culture of continual learning at the\nONS**",
    "**Pillar three: facilitating a culture of continual learning at the\nONS**\n\nCultural and capability-based barriers are being addressed through\ntraining, communication, setting up guidance and templates, and managing\na growing Evaluation Champions Network. We are doing this to embed\nevaluation across the department and ensure evidence and theories of\nchange are used to support programme development and inform decision\nmaking.",
    "Communication is important in demonstrating the value of evaluation\ntools (for example, Theory of Change) to support collaborative planning\nbeyond the portfolio and evaluation.\n\n**Embedding evaluation into ONS culture**",
    "The perception of evaluations is becoming increasingly more positive\nacross the ONS. Providing colleagues with evaluation tools, training,\nand open dialogue has resulted in positive cultural shifts.\nCommunications and positive engagement between the Central Evaluation\nFunction and the Evaluation Champions Network, alongside an engaged\nSenior Leadership Team and engagement by the National Statistician, has\nencouraged ONS staff to work towards our vision of the future of\nevaluation at the ONS.\n\n**Improving capability**",
    "**Improving capability**\n\n**Intranet and the Evaluation Hub**\n\nThe Central Evaluation Function have developed, and will continue to\nmaintain, the evaluation homepage on the planning and project delivery\ncommunity hub to communicate news, evaluation standards, guidance,\ntools, templates, and training across the ONS.\n\n**Evaluation Champions Network**",
    "**Evaluation Champions Network**\n\nEstablished in April 2021, the Evaluation Champions Network, managed by\nthe Central Evaluation Function, is currently a group of over 60\ncolleagues from all grades and professions across the ONS with\nexperience or an interest in evaluation.\n\nThe purpose of the Evaluation Champions Network is to deliver a\nconnected, consistent approach to evaluation across the ONS by:\n\n-   promoting evaluation best practice across the business\n\n-   identifying areas for improvement",
    "-   promoting evaluation best practice across the business\n\n-   identifying areas for improvement\n\n-   reducing duplication of evaluation work across the ONS\n\n-   improving evaluation capability and capacity across the ONS\n\n-   establishing a pro-evaluation culture at the ONS\n\n-   sharing knowledge and providing support and resource across the ONS\n\n**Case study: establishing the ONS Evaluation Champions Network\u00a0**",
    "**Case study: establishing the ONS Evaluation Champions Network\u00a0**\n\nWe are currently conducting an impact evaluation to assess the extent to\nwhich the Evaluation Champions Network has made a difference to\nevaluation practice and culture across the ONS.\u00a0\u00a0\n\nAs a result of the comparatively low profile and small scale of the\nintervention, we will conduct the evaluation on a small scale, focusing\non the important questions of whether the network has achieved its\nintended outcomes and impacts.",
    "Evaluation activities have included a skills audit, output and\nperformance monitoring, surveys and network analysis. We will collect\nadditional data via in-depth interviews with programmes that have\ndeveloped monitoring and evaluation plans. All data collection outputs\nwill feed into a mixed methods contribution analysis and pre- and post-\nanalysis.",
    "The evaluation has, so far, highlighted improvements attributable to the\nintroduction of the Evaluation Champions Network. Firstly,\u00a0champions\nmade new connections across the ONS and valued being a part of the\nnetwork, and the professional development opportunities available to\nchampions helped to progress their career goals. There was an increase\nin pro-evaluation attitude within ONS teams and team members perceiving\nevaluation as useful for learning and accountability. Information,",
    "evaluation as useful for learning and accountability. Information,\nknowledge and learning regarding evaluations were also shared across the\nONS.",
    "**Training**\n\nTo facilitate continual learning and increase capability, the Central\nEvaluation Function run a regular programme of evaluation training and\nheld the ONS\\'s first evaluation month in February 2022.\n\nThe Evaluation Champions Network meets every month for two hours. The\nfirst hour is regular talks and trainings that are open to the ONS,\nwhile the second is an information cascade, tasks, and show and tells\nfor the Evaluation Champions Network.",
    "Adhoc trainings also take place, particularly on topics colleagues have\nrequested via surveys or feedback forms.\n\nThe ONS\\'s evaluation month included further training, talks from guest\nspeakers across government and showcases of evaluation practice at the\nONS. An evaluation log has been established and regularly maintained\nwith approved monitoring and evaluation plans stored centrally and\nshared across the network.",
    "All training sessions from the ONS are recorded and stored on the hub\nwith their presentations and transcripts so colleagues can take the\ntrainings at a time that suits them or refer to trainings they have\npreviously attended.\n\nColleagues are also encouraged to attend other evaluation trainings and\nevents outside the ONS.\n\n**Communications**",
    "The Central Evaluation Function communicates regularly via an evaluation\nnewsletter and on the ONS\\'s intranet via news and blogs, and cascades\ninformation via the private office, the Evaluation Champions Network,\nall-staff emails, and during information cascade meetings by\ndirectorates and all-staff meetings. The Central Evaluation Function\nuses multiple communication channels to ensure that everyone at the ONS\nhas the chance to understand new evaluation requirements, can see",
    "has the chance to understand new evaluation requirements, can see\nguidance and templates, understands the requirements, and has a direct\nchannel to respond to the Central Evaluation Function with questions and\nfeedback.",
    "Openness in communication and explaining evaluations, their purpose, and\nthe importance of evaluation at the ONS has helped tackle the fear of\nnegative outcomes and the consequences of publishing reports and\nimproved the perception of evaluation at the ONS.\n\n**Introducing evaluation into inductions**",
    "**Introducing evaluation into inductions**\n\nTo embed the cultural change, in 2022, the ONS will introduce a module\non the basics of evaluation within its inductions for new staff. This\nmodule will include the why, when and how of evaluation at the ONS, with\nlinks to the hub, trainings, and how to contact the Central Evaluation\nFunction for help or join the Evaluation Champions Network.",
    "**Pillar four: the Analysis Function and Integrated Data Service**\n\n**The Analysis Function\\'s approach**",
    "The Analysis Function supports the government\\'s 17,000 strong community\nof analysts across government and works in close collaboration with the\nmember professions to deliver our mission. The Analysis Function\nstrategy, which will be updated shortly, sets out our vision for\nanalysis in government, our priorities for the function and the support\nthat will be provided to analysts. In 2021, the Analysis Function\nStrategy and Delivery (AFSD) division was established within the ONS to",
    "Strategy and Delivery (AFSD) division was established within the ONS to\naddress the top priorities across government and ensure core functional\nresponsibilities are being met. The new division works closely with\npartners across the ONS and serves the wider government analysis\ncommunity. The division\\'s leadership has agreed the important\npriorities of the function with Departmental Directors of Analysis,\naligned to the\u00a0[[Cabinet Office\\'s Government Functional Standard (PDF,\n365",
    "aligned to the\u00a0[[Cabinet Office\\'s Government Functional Standard (PDF,\n365\nKB)]{.underline}](https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/972732/6.7260_CO_GFS_GovS_001_Govt_Functions_WEB.pdf).",
    "Evaluation was identified as a cross-cutting topic requiring AFSD\nsupport. This was highlighted by the\u00a0[[National Audit Office\\'s (NAO)\nEvaluating government spending report (PDF, 469\nKB)]{.underline}](https://www.nao.org.uk/wp-content/uploads/2021/12/Evaluating-government-spending.pdf)\u00a0published\nin 2021, which identified the need to further strengthen the central",
    "in 2021, which identified the need to further strengthen the central\ncoordination and leadership of government analysis to set consistent\nstandards and solve cross-system problems. The AFSD will work with\npartners across the ONS including the Central Evaluation Function and\nthe Analysis Function to deliver improvements to the system in which\nevaluations are developed and implemented, including the governance of\nstandards and the provision of support to analysts across government.",
    "standards and the provision of support to analysts across government.\nThe AFSD will draw on best practice in the ONS where possible and look\nto share this across the analytical community where appropriate. The\ndivision\\'s work on evaluation covers:",
    "-   developing the Analysis Function standard for analysis, and\n    accompanying assessment framework, including guidance on evaluation\n    and appraisal (NAO recommendations 27 c and d)\n\n-   establishing an Analysis Function Standards steering group to\n    provide governance and oversight of The Magenta Book appraisal (NAO\n    recommendation 27 c)",
    "-   providing guidance, support and best practice on priority topics for\n    the evaluation community such as integrating Theory of Change into\n    policy development and value for money evaluation appraisal (NAO\n    recommendations 29 h)\n\n-   working with departments that have less-mature evaluation functions\n    to ensure they have the right tools and support to deliver effective\n    evaluations",
    "-   undertaking a review of analytical capabilities of the policy\n    profession, including on evaluation, and developing products to\n    improve analytical capabilities in priority areas (NAO\n    recommendations 30 k)\n\n-   working with the Cross-Government Evaluation Group to assess the\n    needs of evaluation practitioners across government and develop\n    products accordingly (NAO recommendations 30 j)\n\n**The Integrated Data Service**",
    "**The Integrated Data Service**\n\nEvaluation will require cross-cutting analysis to improve knowledge of\nwhat works. Analysis through the Integrated Data Service can streamline\nthe available evidence and make evaluation easier to do efficiently for\ndepartments. The potential benefits of a secure, integrated data system,\nallowing analysts from across government access to monitor, evaluate and\nhelp design policy, are huge.",
    "Such an integrated approach can support better policy formation,\ndelivery and evaluation, and it can facilitate a better and more common\nunderstanding of priority government policy areas. It can make data\navailable to researchers in an infrastructure that is safe and secure.\nTo give some comparison, the census is due to provide over \u00a35 billion\nworth of benefits, broadly built on one large survey giving near 100%\ncoverage. A new Integrated Data Service enabling deeper and ongoing",
    "coverage. A new Integrated Data Service enabling deeper and ongoing\nanalysis will allow benefits to accrue in terms of better policymaking,\ngranular assessment of what works and evaluation, efficient allocation\nof public funds, efficient public services, and better interventions\ntailored to population characteristics and locality.",
    "**ONS evaluation workplan**\n\nThe department\\'s evaluation workplan is outlined below, setting out the\nprogrammes to be evaluated over the Spending Review period, including\ntheir plans for evaluation, types of evaluation and indicative\npublication dates.\n\n**Programme**\n\nIntegrated Data Programme (IDP):",
    "**Programme**\n\nIntegrated Data Programme (IDP):\n\nThe Integrated Data Programme is responsible for delivering the\nIntegrated Data Service (IDS) for Government that will bring together\nready-to-use data to enable faster and wider collaborative analysis for\nthe public good.",
    "The IDS remains the best means to maximise the impact of data analysis\nand data science in the UK public sector. The IDS provides a secure\nanalytical environment enabling cross-government access to linked\nversions of the UK's richest datasets. It provides the cutting-edge\ncloud-based tools (such as Python and R) to analyse those data safely,\nsecurely and with ethical and legal considerations fully managed.",
    "The IDS enables a leap forward in efficiency, making it possible to run\nmore comprehensive policy evaluations earlier in time, to share prepared\ndata across departmental analytical teams so that we can solve multiple\nproblems at once (rather than repeating the same analysis in silos). It\nenables us to tackle cross-cutting policy problems by drawing together\ndata covering multiple disciplines (for example, bringing together data\non health, crime, and the labour market).",
    "The IDS will aim to improve decision making through evidence while\nmaking government more effective. Two broad measures of successful\noutcomes are:\n\n-   IDS improves UK-wide decision making; aiding policy development and,\n    crucially, policy impact evaluation, which will take place earlier\n    in, and continue throughout, the policy life cycle\n\n-   IDS is a value-for-money service\n\nThe key benefits and impacts for the programme, in line with the Theory\nof Change are:",
    "The key benefits and impacts for the programme, in line with the Theory\nof Change are:\n\n-   increased speed and lower costs for government departments in\n    identifying, acquiring, accessing and processing data to undertake\n    analysis and reduce platform costs, via the integrated data service\n    and virtualised data-sharing cloud facility\n\n-   social benefits through better statistical and analytical outputs,\n    delivering enhanced public policy outcomes",
    "-   social benefits through better statistical and analytical outputs,\n    delivering enhanced public policy outcomes\n\n-   increased culture of cross-government statistical and analytical\n    collaboration, for the greater public good, that builds greater\n    trust in statistics\n\nFor more information visit the\u00a0[[Integrated Data\nService]{.underline}](https://integrateddataservice.gov.uk/)\u00a0website.\n\n**Evaluation type**\n\nImpact, process, value for money",
    "**Evaluation type**\n\nImpact, process, value for money\n\nThe programme will be taking the following approach to evaluating the\noutcomes of the programme:",
    "-   develop quantifiable metrics, together with deeper qualitative\n    insight from users, to ensure the production of the high-quality\n    data and analysis the UK needs and an inclusive, trusted, and\n    engaging narrative on our social fabric and trends, including web\n    metrics; media reach from Prime Research; sentiment analysis on\n    social media; citations in policy documents, business reports and\n    speeches; monitoring errors and breaches",
    "-   evaluate outcomes from analytical projects and develop case studies\n    throughout the programme as they help to inform and shape the IDP's\n    rollout\n\n-   undertake surveys of Government senior analysts and policy makers on\n    the availability, use and impact of the integrated data service\n\n-   provide externally commissioned independent, value-for-money reviews\n\n**Evaluation Report Publication**\n\n-   Annual Report: Quarter 1 (Jan to Mar) 2023",
    "-   Annual Report: Quarter 1 (Jan to Mar) 2023\n\n-   Annual Report: Quarter 1 2024\n\n-   End-Term Evaluation: Quarter 3 (July to Sept) 2024\n\n**Programme**\n\nCoronavirus (COVID-19) Infection Surveillance (CIS) Programme",
    "The Programme aims to be the trusted source of detailed and timely data\nand analysis on the coronavirus (COVID-19) pandemic, providing regular\ninsights on incidence, prevalence and transmission of infection, on\nantibody prevalence and an understanding of the social impacts of the\npandemic across the UK population. It aims to enable informed decision\nmaking by citizens, services and Government, and to be recognised as",
    "making by citizens, services and Government, and to be recognised as\nplaying an important role in the UK's battle against the virus, while\nstriving to offer a lasting legacy for ONS's future health monitoring.",
    "The impacts identified within the Theory of Change for the programme\ninclude:\n\n-   decision making about the coronavirus pandemic is evidence based\n\n-   the ONS is more responsive to user needs -- informing the wider\n    debate on COVID-19 with a trusted, high-quality, timely, open and\n    collaborative service -- so policy becomes more evidence based for\n    the public good\n\n**Evaluation type**\n\nImpact, process",
    "**Evaluation type**\n\nImpact, process\n\nThe CIS Programme was set up to respond to the urgent requirements of\nthe pandemic. These requirements have continued to change over the\ncourse of the programme as the disease and its effects have evolved. It\nwas not possible to design or build in evaluation before Programme\nactivities began nor to plan any experimental evaluation methods or\ncollection of a counterfactual.",
    "The evaluation methods proposed fall within Section A5 of the Magenta\nBook -- Generic research methods used in both process and impact\nevaluation.\n\n**Evaluation Report Publication**\n\nEnd-term evaluation: Quarter 4 (Oct to Dec) 2022\n\n**Programme**\n\nCensus and data collection transformation programme (CDCTP)\n\nThe core objectives of the programme are to:\n\n-   deliver a successful Census 2021",
    "The core objectives of the programme are to:\n\n-   deliver a successful Census 2021\n\n-   provide evidence to support a decision, to be made in 2023, about\n    the future provision of population statistics after this Census\n\n-   transform how we collect, process, and analyse data\n\nThe outcomes and impacts identified within the business case for the\nprogramme include:",
    "The outcomes and impacts identified within the business case for the\nprogramme include:\n\n-   better-informed decisions made by central Government -- improved\n    statistics used by central Government for resource planning, service\n    planning, targeting investment and policy making and monitoring\n\n-   better-informed decisions made by local Government -- improved\n    statistics used by local Government for resource planning, service\n    planning, targeting investment, policy making and monitoring",
    "-   better-informed decisions made by commercial organisation --improved\n    statistics used by commercial organisations for targeting\n    investment, and market research and statistical benchmarking\n\n-   better-informed decisions made by society -- improved statistics\n    used by society informing public debate, for example on health of\n    carers, changes in religious affiliation or migration\n\n-   Improved efficiency across the ONS and the Government Statistical\n    Service (GSS) through reuse of services developed by the Census",
    "**Evaluation type**\n\nImpact, process, value for money\n\nThe programme was in place and a substantial way through its life cycle\nwhen the Central Evaluation Function was established in 2020. However,\nthe following evaluation activity was planned throughout its life cycle:",
    "-   [[testing the\n    Census]{.underline}](https://www.ons.gov.uk/census/planningforcensus2021/testingthecensus/2016censustestintelfordandthewrekin)\u00a0--\n    the Census underwent a small-scale test in 2016, a large-scale test\n    in 2017 and rehearsal in 2019, which contributed to understanding",
    "in 2017 and rehearsal in 2019, which contributed to understanding\n    the best way to implement the design of Census 2021, and helped\n    reduce risk and uncertainty",
    "-   user Research -- conducted on\u00a0[[the questions included in the Census\n    questionnaire]{.underline}](https://www.ons.gov.uk/census/planningforcensus2021/questiondevelopment),\n    Central Digital and Data Office (CDDO) assessment of the electronic\n    questionnaire and Assisted Digital offering, behavioural insight\n    analysis and tests on communications",
    "-   assessment of benefits of Census 2021 published -- evidence of how\n    the benefits of the programme are being realised will begin to be\n    captured once the Census outputs become available via a dedicated\n    project; the benefits realisation project will evaluate whether the\n    forecast benefits have been realised and will include engagement\n    with stakeholders in local and central government and the private\n    sector, and a variety of methods will be used to communicate with",
    "sector, and a variety of methods will be used to communicate with\n    those stakeholders and a variety of methods used to collect evidence\n    for the evaluation.",
    "-   National Statistician's Recommendation about the future approach to\n    the Census and population statistics -- the 2023 National\n    Statistician's Recommendation will also cover recommended approaches\n    to future Census and data collection, evaluating the evidence\n    produced by the output projects within the programme; consultations\n    will take place over the coming years alongside this research ahead\n    of publication",
    "-   general report -- the final review of the programme will come via\n    the general report, taking a similar approach to the\u00a0[[2011 Census\n    General",
    "Report]{.underline}](https://www.ons.gov.uk/census/2011census/howourcensusworks/howdidwedoin2011/20",
    "/howdidwedoin2011/2011censusgeneralreport?_gl=1*vcpj6w*_ga*NTcwNDgxMDE5LjE2NjIzNzQ5MjE.*_ga_W804VY6Y",
    "zQ5MjE.*_ga_W804VY6YKS*MTY2MzMxMzkzMi4xLjAuMTY2MzMxMzkzMi42MC4wLjA.);",
    "the report will require parliamentary sign off ahead of being\n    published to inform the public on how the Census programme performed",
    "-   major programme assurance -- we report on a quarterly and annual\n    basis to the Infrastructure and Projects Authority (IPA), data is\n    published within each\u00a0[[IPA annual\n    report]{.underline}](https://www.ons.gov.uk/aboutus/transparencyandgovernance/officefornationalstatisticsgovernmentmajorprojectsportfoliodata);\n    regular gateway reviews are undertaken on the programme\n\n**Evaluation Report Publication**",
    "**Evaluation Report Publication**\n\n-   Census General Report: Quarter to be confirmed 2023\n\n-   National Statistician\\'s Recommendation: Quarter 4 2023\n\n-   Assessment of the Benefits of Census 2021: Quarter 4 2024\n\n**Programme**\n\nAmbitious, Radical and Inclusive Economic Statistics Programme (ARIES)",
    "**Programme**\n\nAmbitious, Radical and Inclusive Economic Statistics Programme (ARIES)\n\nThe aim of the ARIES programme is to deliver an ambitious programme that\nwill provide clear, insightful economic, social and environmental\nstatistics and analysis to inform decision making across the UK in an\ninclusive, timely and sustainable way.\n\nIn doing so, the programme scope included within this case has three\nclear spending objectives:",
    "In doing so, the programme scope included within this case has three\nclear spending objectives:\n\n-   improve our core statistical offering, maintaining international\n    standards and comparability, in line with user needs\n\n-   exploit new data sources and innovative methods to inform better\n    quality, more timely and relevant statistics\n\n-   mitigate the risk to existing economic statistics and Environmental\n    Group outputs",
    "-   mitigate the risk to existing economic statistics and Environmental\n    Group outputs\n\nThe impacts identified within the Theory of Change for the programme\ninclude the provision of access to more granular, timely, and accurate\neconomic, social, and environmental statistics, which:\n\n-   inform evidence-based decisions and debate by Government, the\n    public, private and third sectors, and the public\n\n-   provide better policy evaluations for policy makers\n\n-   offer increased user satisfaction",
    "-   provide better policy evaluations for policy makers\n\n-   offer increased user satisfaction\n\n**Evaluation type**\n\nImpact, process\n\nProcess evaluation will measure the progress of activities and outputs\nwithin the Theory of Change against plan implementation.\n\nOutcome/Impact evaluation will provide evidence of the contribution of\nthe programme towards the impacts identified within the Theory of\nChange.\n\nSurveys and administrative data will be used to understand how the\nprogramme meets the needs of users and the level of satisfaction.",
    "**Evaluation Report Publication**\n\n-   The timing of Annual reports (undertaken after the end of each\n    financial year) and the mid-term evaluation of the programme are to\n    be confirmed\n\n-   End-term evaluation: Quarter 3 2025\n\n[[Back to table of\ncontents]{.underline}](https://www.ons.gov.uk/aboutus/transparencyandgovernance/onsevaluationstrategy#toc)\n\n**5.Next steps**",
    "**5.Next steps**\n\nThis strategy has outlined the department\\'s sustained ambition for\nmonitoring and evaluation. It has also highlighted the ONS\\'s pillars of\nactivity designed to address barriers to good evaluation, alongside the\nrole of the Analysis Function and Integrated Data Service.",
    "This strategy has two versions -- an outward facing document and an\ninward facing document linked to guidance and trainings on our intranet.\nWe encourage discussion and feedback from our stakeholders. Please\ndirect any questions or comments\nto\u00a0[[evaluation@ons.gov.uk]{.underline}](mailto:evaluation@ons.gov.uk)\n\nWe will renew this strategy by 2027, and we will update it more\nregularly as evaluation develops at the ONS.",
    "[[Back to table of\ncontents]{.underline}](https://www.ons.gov.uk/aboutus/transparencyandgovernance/onsevaluationstrategy#toc)\n\n**6.ONS organisational theory of change**",
    "**6.ONS organisational theory of change**\n\nThe ONS Organisational Theory of Change is used to ensure new business\ncases, and monitoring and evaluation plans align to the organisation's\ncurrent five-year strategy. The model will be used in organisation-level\nmonitoring and evaluation activities including the annual impact report.\nThe model is updated annually in line with the organisation's strategic\nbusiness plan, and in line with future organisational strategies.",
    "[[ONS organisational theory of change\nmodel]{.underline}](https://www.ons.gov.uk/file?uri=/aboutus/transparencyandgovernance/onsevaluationstrategy/2150onsdepartmentaltheoryofchangediagramportraita.pdf)\n\n[[Back to table of\ncontents]{.underline}](https://www.ons.gov.uk/aboutus/transparencyandgovernance/onsevaluationstrategy#toc)"
]